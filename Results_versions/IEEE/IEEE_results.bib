@INPROCEEDINGS{9757686,
  author={Schlagowski, Ruben and Gupta, Kunal and Mertes, Silvan and Billinghurst, Mark and Metzner, Susanne and André, Elisabeth},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Jamming in MR: Towards Real-Time Music Collaboration in Mixed Reality}, 
  year={2022},
  volume={},
  number={},
  pages={854-855},
  abstract={Recent pandemic-related contact restrictions have made it difficult for musicians to meet in person to make music. As a result, there has been an increased demand for applications that enable remote and real-time music collaboration. One desirable goal here is to give musicians a sense of social presence, to make them feel that they are “on site” with their musical partners. We conducted a focus group study to investigate the impact of remote jamming on users' affect. Further, we gathered user requirements for a Mixed Reality system that enables real-time jamming and developed a prototype based on these findings.},
  keywords={Three-dimensional displays;Conferences;Music;Mixed reality;Collaboration;Prototypes;Virtual reality;Mixed Reality;Remote Collaboration;Music;Human-centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/VRW55335.2022.00278},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108875,
  author={Liang, Hai-Ning and Yu, Lingyun and Liarokapis, Fotis},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Workshop: Mixing Realities: Cross-Reality Visualization, Interaction, and Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={298-300},
  abstract={Cross-reality (CR) systems offer different levels of virtuality/physicality to users and enable them to move back and forth between the reality-virtuality continuums in a seamless way. Immersive augmented, virtual, and mixed reality (AR/VR/MR) head-mounted displays (HMDs) have become the main tools that enable cross-reality interaction. However, working with high-dimensional data is challenging due to complex data structures and their dimensionality. The goal of the workshop is to provide an opportunity for researchers from immersive technologies, HCI, and Visualization fields to share their original ideas and work-in-progress thoughts on the design and evaluation of interactive techniques and systems for effective cross-reality visualization, interaction, and collaboration.},
  keywords={Three-dimensional displays;Head-mounted displays;Conferences;Data visualization;Collaboration;Mixed reality;Virtual reality},
  doi={10.1109/VRW58643.2023.00069},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8446533,
  author={Aschenbrenner, Doris and Li, Meng and Dukalski, Radoslaw and Verlinden, Jouke and Lukosch, Stephan},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Collaborative Production Line Planning with Augmented Fabrication}, 
  year={2018},
  volume={},
  number={},
  pages={509-510},
  abstract={The project “Factory-in-a-day” aims at reducing the installation time of a new hybrid robot-human production line, from weeks or months that current industrial systems now take, down to one day. The ability to rapidly install (and reconfigure) production lines where robots work alongside humans will strongly reduce operating cost and open a range of new opportunities for industry. In this paper, we explore a method of collaborative fabrication planning with the help of Augmented Reality as part of the concept Augmented Fabrication. In order to plan a new production line, two co-located workers at the factory wear a Microsoft Hololens head-mounted display and thus share a common visual context on the planed position of the robots and the production machines. They are assisted by an external remote expert connected via the Internet who is virtually co-located. We developed three different visualizations of the state of the local collaboration and plan to compare them in a user study.},
  keywords={Planning;Fabrication;Robots;Augmented reality;Three-dimensional displays;Task analysis;Human-centered computing [Mixed / augmented reality];[Social and professional topics]: Computer supported cooperative work;Applied computing [Industry and manufacturing];[Computer systems organization]: Robotics},
  doi={10.1109/VR.2018.8446533},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9090675,
  author={Kim, Kangsoo and Erickson, Austin and Norouzi, Nahal},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Developing Embodied Interactive Virtual Characters for Human-Subjects Studies}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  abstract={Embodied interactive virtual characters, such as virtual humans or animals, have been actively used for various Virtual/Augmented / Mixed Reality (VAMR) applications, and researchers have developed different types of embodied virtual characters and studied their effects on the user’s perception and behavior. This tutorial aims to provide the audience with background knowledge on research in embodied interactive virtual characters and how to develop such interactive characters for their specific applications, particularly focusing on human-in-the-loop systems (Wizard of Oz paradigm). The tutorial will first explore the prior interactive virtual character research focusing on the social influence of these entities over the users, e.g., the sense of social presence, trust, collaboration, while discussing the recent trend of the convergence among IVAs, MR, and Internet of Things (IoT) in the scope of virtual characters interacting with the physical surroundings. We will also share our recent research findings and some lessons from our 5+ years of experience in researching interactive virtual characters and user studies at the Synthetic Reality Lab (SREAL), University of Central Florida (UCF). The tutorial will explain how to develop virtual characters in Unity using 3rd party assets and plugins, such as Mixamo and Rogo Digital’s LipSync. The audience will follow the step-by-step instructions with provided materials and eventually have a simple interactive virtual character that they can control through conventional 2D user interfaces, considering human-in-the-loop studies. The tutorial will also explain how to develop a sensing module to understand the current state of the surrounding environment, which can make a realistic connection between the physical and the virtual worlds. For example, an Arduino board with a couple of sensors, e.g., a wind sensor, can be used to detect the wind in the real environment and trigger the coherent events in the virtual environment, such as blowing a virtual ball on a table.},
  keywords={Tutorials;X3D;Sensors;World Wide Web;Standards;Mixed reality;Internet of Things},
  doi={10.1109/VRW50115.2020.00291},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108638,
  author={Maio, Rafael and Marques, Bernardo and Santos, Andre and Ramalho, Pedro and Almeida, Duarte and Dias, Paulo and Santos, Beatriz Sousa},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Real-Time Data Monitoring of an Industry 4.0 Assembly Line using Pervasive Augmented Reality: First Impressions}, 
  year={2023},
  volume={},
  number={},
  pages={414-417},
  abstract={Industry 4.0 is the latest industrial revolution, tend to seamlessly integrate digital and physical worlds into industrial procedures in an effort to better support different stakeholders' needs. This work proposes a Pervasive Augmented Reality (AR) first prototype, developed in collaboration with industrial partners, to support real-time data monitoring and problem detection of an industrial assembly line. A Human-Centered Design (HCD) methodology was used to identify stakeholders' difficulties, needs, and challenges, as well as define requirements to guide the development of the prototype. The first impressions of these individuals are described, after an initial user study on the industrial shop floor, conducted to evaluate the proposed prototype, and collect feedback on how it can be improved to better support such contexts.},
  keywords={Three-dimensional displays;Conferences;Prototypes;Collaboration;User interfaces;Real-time systems;Fourth Industrial Revolution;Human-centered computing;Human computer interaction (HCI);Interaction Paradigms;Mixed/augmented reality;HCI Design and Evalution Methods;User Studies},
  doi={10.1109/VRW58643.2023.00090},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7281081,
  author={Cailhol, Simon and Fillatreau, Philippe and Fourquet, Jean-Yves and Zhao, Yingshen},
  booktitle={2014 International Conference on Virtual Reality and Visualization}, 
  title={A Multi-layer Approach for Interactive Path Planning Control in Virtual Reality Simulation}, 
  year={2014},
  volume={},
  number={},
  pages={296-301},
  abstract={His work considers path-planning processes for manipulation tasks such as assembly, maintenance or disassembly in a Virtual Reality (VR) context. The approach consists in providing a collaborative system associating a user immersed in VR and an automatic path planning process. It is based on an novel environment model containing semantic, topological and geometric information and an original planning process split in two phases: coarse and fine planning. The automatic planner suggests a path to the user and guides him trough a hap tic device. The user can escape from the proposed solution if he wants to explore a possible better way. In this case, the interactive system detects the users intention and computes in real-time a new path taking the users intent into account. Experiments illustrate the different aspects of the proposed approach: multi-representation of the environment, path planning process, users intent prediction and control sharing.},
  keywords={Planning;Path planning;Semantics;Solid modeling;Computational modeling;Robots;Shape;Virtual Reality;Interactive path planning;control sharing;multi-layer architecture},
  doi={10.1109/ICVRV.2014.76},
  ISSN={},
  month={Aug},}

@INPROCEEDINGS{9090404,
  author={Waldow, Kristoffer and Fuhrmann, Arnulph},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Addressing Deaf or Hard-of-Hearing People in Avatar-Based Mixed Reality Collaboration Systems}, 
  year={2020},
  volume={},
  number={},
  pages={594-595},
  abstract={Automatic Speech Recognition (ASR) technologies can be used to address people with auditory disabilities by integrating them in an interpersonal communication via textual visualization of speech. Especially in avatar-based Mixed Reality (MR) remote collaboration systems, speech is an important additional modality and allows natural human interaction. Therefore, we propose an easy to integrate ASR and textual visualization extension for an avatar-based MR remote collaboration system that visualizes speech via spatial floating speech bubbles. In a small pilot study, we achieved word accuracy of our extension of 97% by measuring the widely used word error rate.},
  keywords={Virtual reality;Speech recognition;Visualization;Collaboration;Auditory system;Engines;Microsoft Windows;Human-centered computing—HCI theory;concepts and models; Human-centered computing—Systems and tools for interaction design;Human-centered computing;HCI theory, concepts and models;Human-centered computing;Systems and tools for interaction design},
  doi={10.1109/VRW50115.2020.00148},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9483852,
  author={Dai, Rui and Pan, Zhigeng},
  booktitle={2021 IEEE 7th International Conference on Virtual Reality (ICVR)}, 
  title={A Virtual Companion Empty-Nest Elderly Dining System Based on Virtual Avatars}, 
  year={2021},
  volume={},
  number={},
  pages={446-451},
  abstract={With the further acceleration of my country's urbanization and aging, the number of empty-nest elderly people gradually accounts for a larger proportion of my country's social structure. The pension issue has also become a social issue of urgent concern. Many young people work busy to support their families in other cities, and cannot go home frequently to visit their elderly parents. It is even rarer to eat together. Based on virtual human technology, this paper introduces a remote virtual companion system based on depth camera Kinect, which aims to solve the problem that children cannot accompany empty-nest elderly to dine. This system supports multi-person collaborative interaction in different places, and supports two companion modes. One mode is that when relatives have time to accompany the elderly to eat, this system drives the virtual avatars through the character mapping algorithm and tracking the bone movements of the real person. The other mode is when the relatives do not have time to accompany the elderly to eat, this system uses intelligent virtual agent to accompany the elderly to eat. The interaction method of this system has three ways of multi-modal interaction, including visual interaction, gesture interaction, and voice interaction.},
  keywords={Visualization;Tracking;Avatars;Conferences;Senior citizens;Urban areas;Collaboration;human posture recognition;Kinect;virtual avatar;virtual agent;remote interaction;pension mode},
  doi={10.1109/ICVR51878.2021.9483852},
  ISSN={2331-9569},
  month={May},}

@INPROCEEDINGS{9212860,
  author={Arce-Lopera, Carlos and Gomez, Arturo and Montoya, Camilo},
  booktitle={2019 International Conference on Virtual Reality and Visualization (ICVRV)}, 
  title={User Engagement for Collaborative Learning on a Mobile and Desktop Augmented Reality Application}, 
  year={2019},
  volume={},
  number={},
  pages={193-195},
  abstract={User engagement and information understanding was assessed for a basic electronics tutorial designed using Augmented Reality (AR). The main idea was to compare user performance in collaborative environments between two different versions of the AR application: a desktop version and a mobile version. Preliminary testing of both versions revealed that users were interested in AR technology but did not intuitively understand how to better utilize the system in comparison with a static video. Moreover, in spite there was not a statistically significant difference between information understanding between both conditions, the mobile application was rated significantly lower than the desktop application in user engagement for collaborative environments.},
  keywords={Tutorials;Augmented reality;Collaboration;Visualization;Usability;User interfaces;Human centered computing Collaborative and social computing Empirical studies in collaborative and social computing;Human centered computing;Visualization;Visualization design and evaluation methods},
  doi={10.1109/ICVRV47840.2019.00045},
  ISSN={2375-141X},
  month={Nov},}

@INPROCEEDINGS{9419141,
  author={Yassien, Amal and Emad, Youssef and Abdennadher, Slim},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={CDVVAR: VR/AR Collaborative Data Visualization Tool}, 
  year={2021},
  volume={},
  number={},
  pages={599-600},
  abstract={The emergence of immersive platforms has opened room for designing more effective data visualization tools. Therefore, we developed CDVVAR, a VR/AR collaborative tool that can visualize any dataset using three different techniques. Our prototype enables users to share graphs and highlight (ping) specific data point across VR or mobile AR platforms. We conducted a within-subject study (12 pairs) to evaluate the effectiveness of our prototype. Each pair was shown graphs and asked to point specific data values in both VR and mobile AR setup. The time to collaboratively answer the questions was recorded along with users’ general feedback.},
  keywords={Human computer interaction;Three-dimensional displays;Conferences;Collaborative tools;Data visualization;Prototypes;Virtual reality;Human-centered computing;Visualization;Visualization systems and tools;Human-Computer-Interaction (HCI)},
  doi={10.1109/VRW52623.2021.00182},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9090571,
  author={Kim, Jongyong and Song, Jonghoon and Seo, Woong and Ihm, Insung and Yoon, Seung-Hyun and Park, Sanghun},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={XR Framework for Collaborating Remote Heterogeneous Devices}, 
  year={2020},
  volume={},
  number={},
  pages={586-587},
  abstract={With the advent of high-speed 5G networks, establishing a remote space-sharing XR environment that supports real-time interaction based on the latest VR/AR/MR technology is receiving attention as a promising new research topic. With that aim, we have developed a framework that allows users with different types of devices, in different physical spaces, to effectively build a virtual world where collaborative work can be performed. By using this technology, new types of content can be effectively produced that can interact in real time with various commercial VR/MR head-mounted displays, smart mobile devices, and even beam projectors. In this paper, we demonstrate the effectiveness and potential of this framework through the example of game content development where MR and VR head-mounted display users interact in virtual space.},
  keywords={Virtual reality;Real-time systems;Games;Resists;Conferences;Three-dimensional displays;Performance evaluation;Human-centered computing;Human computer interaction;Interaction paradigms;Mixed/augmented reality;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality},
  doi={10.1109/VRW50115.2020.00144},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10536524,
  author={Nam, Hyeongil},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Multimodal VR/MR Simulation with Empathic Agents for Nursing Education}, 
  year={2024},
  volume={},
  number={},
  pages={1128-1129},
  abstract={In the field of healthcare education, including medical and nursing education, realistic and interactive simulations of various clinical scenes are required to mimic real-life situations, necessitating more than just a procedural experience. There has been a recent surge in demand for virtual/mixed reality (VR/MR) due to its advantages in providing diverse simulation environments. However, several challenges still remain to be addressed to advance to the next level of VR/MR-based clinical simulation. In this paper, I introduce the research that I have conducted on creating immersive clinical nursing scenes in VRIMR - based learning and training simulation systems, focusing on key elements such as multi-modal interaction, virtual agents, and learner performance assessment. Furthermore, I will discuss future research aimed at fostering collaboration in various healthcare fields, including nursing, and developing VRlMR simulation systems that enable learners to experience and learn from personalized clinical situations. These topics and questions will be further discussed at the IEEE VR 2024 Doctoral Consortium.},
  keywords={Training;Solid modeling;Three-dimensional displays;Design methodology;Conferences;Focusing;Medical services;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies;Applied computing—Education—Interactive learning environments},
  doi={10.1109/VRW62533.2024.00358},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8446542,
  author={Phan, Thai and Hönig, Wolfgang and Ayanian, Nora},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Mixed Reality Collaboration Between Human-Agent Teams}, 
  year={2018},
  volume={},
  number={},
  pages={659-660},
  abstract={Collaboration between two or more geographically dispersed teams has applications in research and training. In many cases specialized devices, such as robots, may need to be combined between the collaborating groups. However, it would be expensive or even impossible to collocate them at a single physical location. We describe the design of a mixed reality test bed which allows dispersed humans and physically embodied agents to collaborate within a single virtual environment. We demonstrate our approach using Unity's networking architecture as well as open source robot software and hardware. In our scenario, a total of 3 humans and 6 drones must move through a narrow doorway while avoiding collisions in the physical spaces as well as virtual space.},
  keywords={Drones;Servers;Virtual reality;Tracking;Collaboration;Wireless communication;Robots;Human-centered computing-Virtual reality;Computer systems organization-Robotic autonomy},
  doi={10.1109/VR.2018.8446542},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108732,
  author={Xu, Xuanhui and Puggioni, Antonella and Kilroy, David and Campbell, Abraham},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Collaborative Co-located Mixed Reality in Teaching Veterinary Radiation Safety Rules: a Preliminary Evaluation}, 
  year={2023},
  volume={},
  number={},
  pages={889-890},
  abstract={When learning radiographic techniques on horses, veterinary stu-dents must be familiar with radiation safety rules, which are essential to avoid potentially dangerous exposure to the x-ray beam. We pro-pose a collaborative co-located Mixed Reality (MR) system for training students on radiation safety rules allowing staff to guide the students to practice positioning for radiography safely. The performance of 22 veterinary students was measured before and after they had grouped training sessions using the system. The result showed that the participants performed significantly better in the post-test. This evaluation illustrates the potential of expanding the technique into other subjects.},
  keywords={Training;Radiography;Human computer interaction;Three-dimensional displays;Radiation safety;Conferences;Mixed reality;Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human computer interaction (HCI)-Interaction techniques-Gestural input},
  doi={10.1109/VRW58643.2023.00287},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8797872,
  author={Hamzeheinejad, Negin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={[DC] VR Simulation as a Motivator in Gait Rehabilitation}, 
  year={2019},
  volume={},
  number={},
  pages={1383-1384},
  abstract={Gait rehabilitation is a necessary process for patients suffering from post-stroke motor impairments. The patients are required to perform repetitive practices using a robot-assisted gait device. Repeated exercises can become extremely frustrating and the patients lose their motivation over time. In my PhD research, I focus on Virtual Reality (VR) as a medium to improve gait rehabilitation in terms of enjoyment, motivation, efficiency, and effectiveness. The objective is to systematically investigate different factors, such as the presence of a trainer, interactivity, gamification, and storytelling in a two-step process. First, by evaluating the applicability of different factors for the clinical use with healthy subjects. Second, by evaluating the effectiveness of the VR simulation with patients with gait deficits, especially stroke patients in collaboration with a [country] clinic.},
  keywords={Medical treatment;Virtual reality;Solid modeling;Robots;Mirrors;Neurons;Stroke (medical condition);Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
  doi={10.1109/VR.2019.8797872},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9419106,
  author={Podkosova, Iana and Zibrek, Katja and Pettré, Julien and Hoyet, Ludovic and Olivier, Anne-Hélène},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Exploring behaviour towards avatars and agents in immersive virtual environments with mixed-agency interactions}, 
  year={2021},
  volume={},
  number={},
  pages={140-143},
  abstract={Immersive virtual environments (IVEs) in which multiple users navigate by walking and interact with each other in natural ways are perfectly suited for team applications from training to recreation. At the same time, they can solve scheduling conflicts by employing virtual agents in place of missing team members or additional participants of a scenario. While this idea has been long discussed in IVEs research there are no prior publications on social interactions in systems with multiple embodied users and agents. This paper presents an experiment at a work-in-progress stage that addresses the impact of perceived agency and control of a virtual character in a collaborative scenario with two embodied users and one virtual agent. Our future study will investigate whether users treat avatars and agents differently within a mixed-agency scenario, analysing several behavioural metrics and self-report of participants.},
  keywords={Training;Measurement;Legged locomotion;Three-dimensional displays;Navigation;Conferences;Avatars;Collaborative Virtual Reality;Avatars;Virtual Agents},
  doi={10.1109/VRW52623.2021.00033},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10536496,
  author={Terendii, Olha and Frish, Sam and Prokopiev, Tymur and Hemmerling, Daria and Jadczyk, Tomasz and Janusz, Mateusz},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Touch My Heart: Navigating the Heart Models in MR with Haptic Feedback, 3D Sound, and Interactive Gamification}, 
  year={2024},
  volume={},
  number={},
  pages={1190-1191},
  abstract={Touch My Heart utilizes a mixed reality headset and digital twin technology for an immersive exploration of the heart. The system, integrating hand tracking and haptic feedback, offers a realistic simulation, enhancing spatial audio for a fusion of senses. Users can interact with a 3D heart model, providing a comprehensive understanding of structure, function, and abnormalities including cardiac arrhythmia's and valvular heart diseases. The initiative supports medical education, enabling detailed exploration of heart pathophysiology and collaborative discussions among healthcare professionals. Additionally, it facilitates patient engagement by educating on heart conditions.},
  keywords={Heart;Solid modeling;Three-dimensional displays;Navigation;Spatial audio;Mixed reality;Virtual reality;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VRW62533.2024.00389},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108600,
  author={Mukhaimar, Ayman and Miao, Yuan and Vrcelj, Zora and Gu, Bruce and Yang, Ang and Zhao, Jun and Sandanayake, Malindu and Chan, Melissa},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Multi-person tracking for virtual reality surrounding awareness}, 
  year={2023},
  volume={},
  number={},
  pages={593-594},
  abstract={Virtual reality devices are designed to cover our vision so we are unaware of our surroundings and completely emerged into a different world. This limits VR user's ability to move freely in crowded areas, such as classrooms, which limits the user interactions in VR. We present a framework that enables VR users to see other people around them inside the VR environment with the help of an external 3D depth camera. The proposed framework enables multi-VR users to use one tracking camera and provides a cost-effective solution. The proposed framework can also help in performing collaborative activities.},
  keywords={Visualization;Three-dimensional displays;Conferences;Collaboration;Virtual reality;Dogs;User interfaces;Cameras;Skeleton;Surrounding awareness in virtual reality-Virtual reality people tracking-Virtual reality skeleton tracking-Virtual reality real world interaction-Real world visualization in Virtual reality},
  doi={10.1109/VRW58643.2023.00139},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9419114,
  author={Schlager, Bettina and Stoll, Daniela and Krösl, Katharina and Fuhrmann, Anton},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Tactical and Strategical Analysis in Virtual Geographical Environments}, 
  year={2021},
  volume={},
  number={},
  pages={621-622},
  abstract={Planning tactical and strategical operations on a terrain is a well structured and complex process. It is usually executed by a group of interdisciplinary experts with different objectives. Traditional equipment for tactical analysis like paper maps or sand tables cannot transfer spatial understanding for visibility analysis and height judgment. Immersive virtual geographical environments offer additional perspectives for rapid decision making and reasoning of spatial structures. We propose a collaborative multi-user virtual reality system for the tactical analysis of terrain data to support mission planning.},
  keywords={Three-dimensional displays;Conferences;Decision making;Collaboration;Virtual reality;User interfaces;Cognition;Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism;Virtual Reality},
  doi={10.1109/VRW52623.2021.00193},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108844,
  author={Guarese, Renan and Pretty, Emma and Zambetta, Fabio},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={XR towards tele-guidance: mixing realities in assistive technologies for blind and visually impaired people}, 
  year={2023},
  volume={},
  number={},
  pages={324-329},
  abstract={This work proposes the use of immersive asymmetric collaboration as an assistive technology for people who are blind or visually impaired (BVI). Similar to tele-guidance (TG) systems, it is feasible to transmit in-situ spatial information from the perspective of a BVI person wearing an Augmented Reality (AR) headset to be viewed by a remote sighted person wearing a Virtual Reality (VR) headset. Through this asymmetric collaborative guidance scenario, we expect remote guides may have better spatial understanding of the environment, being immersed in its digital twin, while BVI users may find guidance to be less time-consuming in their current environment, when compared to current TG systems, solely based on video. In the current experiment, TG is done in a Wizard-of-Oz fashion, while users (N = 18) are blindfolded and follow its audio cues in search of targets in a room. Results outlined that different audio interfaces have an impact on the system's usability, with one of our tested methods providing significantly better results in multiple usability metrics. As a proof of concept, we showcase how cross-reality asymmetric collaboration can be used in day-to-day manual tasks, such as finding objects in a pantry or a closet.},
  keywords={Measurement;Headphones;Visualization;Three-dimensional displays;Collaboration;Assistive technologies;User interfaces;Human-centered computing-Accessibility technologies-Empirical studies in accessibility-;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Auditory feedback;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;},
  doi={10.1109/VRW58643.2023.00074},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10536317,
  author={Schwenderling, Lovis and Herbrich, Wilhelm and Joeres, Fabian and Hansen, Christian},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={A Novel Framework for Hand Visualization in Web-Based Collaborative XR}, 
  year={2024},
  volume={},
  number={},
  pages={18-23},
  abstract={Many extended reality (XR) applications are platform-specific, making accessibility and cross-platform collaboration difficult. Web-based collaborative XR can enhance adoption of XR technologies, using the browser as a platform-independent interface. However, challenges arise from the browser environment, such as performance limitations. To this end, we present a WebXR-based framework for hand interaction in cross-platform collaboration in XR. A network structure and methods for collaborative and individual object manipulation complement the integrated hand tracking. Three different fidelity levels to represent the hands of remote users were implemented to accommodate different performance capabilities. Concepts ranged from virtual hands to discrete poses with abstract objects. A sample application was implemented with a puzzle task. Two users collaborated in the browsers of the Microsoft HoloLens 2 and the Meta Quest 2. Qualitative and quantitative data on user performance (n=9), and frame rate recordings (n=1) were collected. All users were able to solve the puzzle together quickly and intu-itively. The Quest environment was preferred, as there were more performance issues with the HoloLens. Hand interaction was well-received and proved to be sufficient as the only form of communication. Simpler representations of the hands lead to a higher frame rate, whereby the effects were device-dependent. The impact on task performance was low. Hand interaction enables an intuitive exchange of objects and basic communication in cross-platform collaboration via browsers. Depending on the XR environment, however, device-specific performance limitations must be taken into account by modulating the amount of data and rendering effort.},
  keywords={Performance evaluation;Three-dimensional displays;Collaboration;User experience;Time measurement;Browsers;Velocity measurement;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Web-based interaction;Human-centered computing—Collaborative and social computing—Collaborative and social computing systems and tools},
  doi={10.1109/VRW62533.2024.00010},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9847779,
  author={Hu, Guosheng and Wang, Yu'ao and Mao, Mengyuan and Zhao, Yi},
  booktitle={2022 8th International Conference on Virtual Reality (ICVR)}, 
  title={Remote Care and Collaboration for Empty Nest Family: Smart Home, Digital Twin and Mixed Reality}, 
  year={2022},
  volume={},
  number={},
  pages={126-134},
  abstract={With the acceleration of aging and the rapid upgrading of smart home devices, the solitary elderly of empty nest inevitably face the whirlpool of smart equipment. Lack of escort and assistance from their younger generations and third-party service, the senior's sense of loneliness and loss of control over smart home has become increasingly prominent. The authors of this article established a remote collaboration system for smart home (RCSSH) based on digital twin (DT) and mixed reality (MR), which achieves the cooperation between remote users (family juniors) and home users (the seniors) to complete auxiliary tasks such as remote company, management of smart devices, etc. In addition to sharing immersive perspectives and collaborative elements, the system also collects home environment data via hardware of internet of things (IoT) and displays them in the DT space to enrich the collaboration experience. Using augmented reality (AR) and remote interaction beyond physical space, the system can reduce the learning burden of smart homes for the in-home seniors through the bilateral remote collaboration, and expand the means that remote family members accompany their seniors, that make up for the loss of self-value for the elderly.},
  keywords={Collaboration;Mixed reality;Smart homes;Hardware;Digital twins;Internet of Things;Older adults;smart home;remote collaboration;immersive company;solitary senior},
  doi={10.1109/ICVR55215.2022.9847779},
  ISSN={2331-9569},
  month={May},}

@INPROCEEDINGS{8797719,
  author={Yoon, Boram and Kim, Hyung-il and Lee, Gun A. and Billinghurst, Mark and Woo, Woontack},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Effect of Avatar Appearance on Social Presence in an Augmented Reality Remote Collaboration}, 
  year={2019},
  volume={},
  number={},
  pages={547-556},
  abstract={This paper investigates the effect of avatar appearance on Social Presence and users' perception in an Augmented Reality (AR) telepresence system. Despite the development of various commercial 3D telepresence systems, there has been little evaluation and discussions about the appearance of the collaborator's avatars. We conducted two user studies comparing the effect of avatar appearances with three levels of body part visibility (head & hands, upper body, and whole body) and two different character styles (realistic and cartoon-like) on Social Presence while performing two different remote collaboration tasks. We found that a realistic whole body avatar was perceived as being the best for remote collaboration, but an upper body or cartoon style could be considered as a substitute depending on the collaboration context. We discuss these results and suggest guidelines for designing future avatar-mediated AR remote collaboration systems.},
  keywords={Avatars;Collaboration;Three-dimensional displays;Telepresence;Augmented reality;Task analysis;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed/augmented reality;Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies},
  doi={10.1109/VR.2019.8797719},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{1667630,
  author={Kuechler, M. and Kunz, A.},
  booktitle={IEEE Virtual Reality Conference (VR 2006)}, 
  title={HoloPort - A Device for Simultaneous Video and Data Conferencing Featuring Gaze Awareness}, 
  year={2006},
  volume={},
  number={},
  pages={81-88},
  abstract={Complex business processes and globally distributed teams require new means to support net-based teamwork. Here, virtual reality technologies offer new ways to a more intuitive collaboration. In this paper, a system setup dubbed HoloPort is presented, which enables intuitive point-to-point video conferencing between two small groups. The device features gaze awareness between local and remote conferees during video and data conferences, and pen-based on-screen interaction. After an introduction to tele-collaboration systems, the working principle of the proposed HoloPort is illustrated, and the device’s individual components are described in detail. Then, different application scenarios using the HoloPort are discussed. Finally, the paper concludes with an outlook on future improvements.},
  keywords={Videoconference;Collaborative work;Cameras;Virtual reality;Computer interfaces;Computer networks;Remote monitoring;Displays;Technological innovation;Teamwork;Video and data conferencing;gaze awareness;computer supported cooperative work;networked collaboration;teleimmersion},
  doi={10.1109/VR.2006.71},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{9756787,
  author={Jing, Allison and Lee, Gun and Billinghurst, Mark},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Speech to Visualise Shared Gaze Cues in MR Remote Collaboration}, 
  year={2022},
  volume={},
  number={},
  pages={250-259},
  abstract={In this paper, we present a 360° panoramic Mixed Reality (MR) sys-tem that visualises shared gaze cues using contextual speech input to improve task coordination. We conducted two studies to evaluate the design of the MR gaze-speech interface exploring the combinations of visualisation style and context control level. Findings from the first study suggest that an explicit visual form that directly connects the collaborators’ shared gaze to the contextual conversation is preferred. The second study indicates that the gaze-speech modality shortens the coordination time to attend to the shared interest, making the communication more natural and the collaboration more effective. Qualitative feedback also suggest that having a constant joint gaze indicator provides a consistent bi-directional view while establishing a sense of co-presence during task collaboration. We discuss the implications for the design of collaborative MR systems and directions for future research.},
  keywords={Manufacturing industries;Visualization;Three-dimensional displays;Collaboration;Mixed reality;Virtual reality;Bidirectional control;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
  doi={10.1109/VR51125.2022.00044},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9756740,
  author={Medeiros, Marina L. and Schlager, Bettina and Krösl, Katharina and Fuhrmann, Anton},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Potential of VR-based Tactical Resource Planning on Spatial Data}, 
  year={2022},
  volume={},
  number={},
  pages={176-185},
  abstract={Planning tactical operations on topographic maps, for rescue or military missions, is a complex process conducted by interdisciplinary experts and involves the time-consuming derivation of 3D information from 2D maps, mostly solely executed by experienced professionals. Previous research repeatedly showed that virtual reality (VR) can convey spatial relationships and complex 3D structures intuitively. In this work, we leverage the benefits of immersive head-mounted displays (HMDs) and present the design, implementation, and evaluation of a collaborative VR application for tactical resource planning on spatial data. We derived system and design requirements from consultations with domain experts and observations of a military on-site staff exercise, a simulation-based training aiming to strengthen rapid decision-making and teamwork during a time of crisis. To evaluate our prototype, we conducted semi-structured interviews with domain experts who organized and observed field tests at different military staff exercises. The interviews support the proposed design of the prototype and show general design implications for planning tools in VR. Our results show that the potential of VR-based tactical resource planning is dependent on the technical features as well as on non-technical environmental aspects, such as user attitude, prior experience, and interoperability.},
  keywords={Training;Three-dimensional displays;Prototypes;Virtual reality;User interfaces;Spatial databases;Planning;Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism;Virtual Reality},
  doi={10.1109/VR51125.2022.00036},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{4480774,
  author={Irawati, Sylvia and Ahn, Sangchul and Kim, Jinwook and Ko, Heedong},
  booktitle={2008 IEEE Virtual Reality Conference}, 
  title={VARU Framework: Enabling Rapid Prototyping of VR, AR and Ubiquitous Applications}, 
  year={2008},
  volume={},
  number={},
  pages={201-208},
  abstract={Recent advanced interface technologies allow the user to interact with different spaces such as Virtual Reality (VR), Augmented Reality (AR) and Ubiquitous Computing (UC) spaces. Previously, human computer interaction (HCI) issues in VR, AR and UC have been largely carried out in separate communities. Here, we combine these three interaction spaces into a single interaction space, called Tangible Space. We propose the VARU framework which is designed for rapid prototyping of a tangible space application. It is designed to provide extensibility, flexibility and scalability. Depending on the available resources, the user could interact with either the virtual, physical or mixed environment. By having the VR, AR and UC spaces in a single platform, it gives us the possibility to explore different types of collaboration across the different spaces. As a result, we present our prototype application which is built using the VARU framework.},
  keywords={Prototypes;Virtual reality;Space technology;Human computer interaction;Collaboration;Application software;Augmented reality;Ubiquitous computing;Computer displays;Switches;C.2.4 [Computer Communication Networks]: Distributed Systems - Client/server;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, Augmented and Virtual Realities;framework;virtual reality;augmented reality;ubiquitous computing;tangible space},
  doi={10.1109/VR.2008.4480774},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{5444794,
  author={Lee, Sangyoon and Hua, Hong},
  booktitle={2010 IEEE Virtual Reality Conference (VR)}, 
  title={Effects of viewing conditions and rotation methods in a collaborative tabletop AR environment}, 
  year={2010},
  volume={},
  number={},
  pages={163-170},
  abstract={In this paper, we investigate the effects of viewing conditions and rotation methods on different types of collaborative tasks in a tabletop AR environment in which two users are co-located. The viewing condition means how the manipulation of a tabletop world by one user is shown in the other users' view and the rotation method means what type of input devices is used to rotate the tabletop world for alternative orientations. Our experiment considered two different viewing conditions-consistent view and inconsistent view and two different rotation methods-direct turn and indirect turn. In the experiment, a 3D display environment called ¿Stereoscopic Collaboration in Augmented and Projective Environments (SCAPE)¿ was utilized as a test environment, and two tasks were considered: Lego-like block building task and text label selection task. The former was designed for synchronous and referring-strong type, and the latter was designed for asynchronous and orientation-strong type. As dependent variables, various objective and subjective measurements including task completion time, quality of task result, turn angle, and questionnaire were measured. According to the results, the viewing conditions had significant effects on several objective and subjective measurements. On task completion time, their effect for the synchronous task was opposite to that for the asynchronous task. On the other hand, the rotation methods had significant effects only on turn angle.},
  keywords={Collaboration;Computer displays;Educational institutions;Time measurement;Virtual reality;Computer interfaces;Physics computing;Visualization;Layout;Three dimensional displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-artificial, augmented, and virtual realities;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces-collaborative computing, evaluation/methodology;H.1.2 [Models and Principles]: User/Machine Systems-human factors},
  doi={10.1109/VR.2010.5444794},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{10536357,
  author={Stacchio, Lorenzo and Vallasciani, Giacomo and Augello, Giulio and Carrador, Silvano and Cascarano, Pasquale and Marfia, Gustavo},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={WiXaRd: Towards a Holistic Distributed Platform for Multi-Party and Cross-Reality WebXR Experiences}, 
  year={2024},
  volume={},
  number={},
  pages={264-272},
  abstract={Extended Reality (XR) paradigms have found applications in different areas, with vast adoption in both education and industry. Those flourished since XR enables spatial 3D object manipulation, and annotations, which have proven to be crucial for enhancing learning and task execution efficacy. In developing such experiences, inde-pendence towards the adopted device, ease of use, integration with external data sources, and real-time collaboration are pivotal factors to fit human needs. Considering these aspects, WebXR technologies could provide a solution, however, we found a lack of open-source systems that provide holistically: (i) a flexible WebXR immersive environment; (ii) a remote-streaming mechanism, to increase its accessibility; (iii) integration with the 3D external database. To this date, we here introduce WiXaRd, a WebXR and cross-reality system designed to provide such features, making it suitable for applications like collaborative remote showcasing, teaching, and training.},
  keywords={Training;Industries;Social computing;Three-dimensional displays;Soft sensors;Collaboration;Rendering (computer graphics);WebXR;eXtended Reality;Cross-Reality;Remote Rendering;H.5.2 [Human-centered computing]-Collaborative and social computing systems and tools (GUI);H.5.m [Human-centered computing]: Visualization systems and tools},
  doi={10.1109/VRW62533.2024.00052},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108714,
  author={Numan, Nels and Lu, Ziwen and Congdon, Benjamin and Giunchi, Daniele and Rotsidis, Alexandros and Lernis, Andreas and Larmos, Kyriakos and Kourra, Tereza and Charalambous, Panayiotis and Chrysanthou, Yiorgos and Julier, Simon and Steed, Anthony},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Towards Outdoor Collaborative Mixed Reality: Lessons Learnt from a Prototype System}, 
  year={2023},
  volume={},
  number={},
  pages={113-118},
  abstract={Most research on collaborative mixed reality (CMR) has focused on indoor spaces. In this paper, we present our ongoing work aimed at investigating the potential of CMR in outdoor spaces. These spaces present unique challenges due to their larger and more com-plex nature, particularly in terms of reconstruction, tracking, and interaction. Our prototype system utilises a photorealistic model to facilitate collaboration between remote virtual reality (VR) users and a local augmented reality (AR) user. We discuss our design considerations, lessons learnt, and areas for future work.},
  keywords={Solid modeling;Social computing;Three-dimensional displays;Source coding;Prototypes;Collaboration;Mixed reality;mixed reality;augmented reality;virtual reality;collab-oration;outdoor;city-scale;reconstruction;registration;Human-centered computing-Collaborative and social computing systems and tools;Human-centered computing-Ubiquitous and mobile computing systems and tools},
  doi={10.1109/VRW58643.2023.00029},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8797682,
  author={Wisotzky, Eric L. and Rosenthal, Jean-Claude and Eisert, Peter and Hilsmann, Anna and Schmid, Falko and Bauer, Michael and Schneider, Armin and Uecker, Florian C.},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Interactive and Multimodal-based Augmented Reality for Remote Assistance using a Digital Surgical Microscope}, 
  year={2019},
  volume={},
  number={},
  pages={1477-1484},
  abstract={We present an interactive and multimodal-based augmented reality system for computer-assisted surgery in the context of ear, nose and throat (ENT) treatment. The proposed processing pipeline uses fully digital stereoscopic imaging devices, which support multispectral and white light imaging to generate high resolution image data, and consists of five modules. Input/output data handling, a hybrid multimodal image analysis and a bi-directional interactive augmented reality (AR) and mixed reality (MR) interface for local and remote surgical assistance are of high relevance for the complete framework. The hybrid multimodal 3D scene analysis module uses different wavelengths to classify tissue structures and combines this spectral data with metric 3D information. Additionally, we propose a zoom-independent intraoperative tool for virtual ossicular prosthesis insertion (e.g. stapedectomy) guaranteeing very high metric accuracy in sub-millimeter range (1/10 mm). A bi-directional interactive AR/MR communication module guarantees low latency, while consisting surgical information and avoiding informational overload. Display agnostic AR/MR visualization can show our analyzed data synchronized inside the digital binocular, the 3D display or any connected head-mounted-display (HMD). In addition, the analyzed data can be enriched with annotations by involving external clinical experts using AR/MR and furthermore an accurate registration of preoperative data. The benefits of such a collaborative surgical system are manifold and will lead to a highly improved patient outcome through an easier tissue classification and reduced surgery risk.},
  keywords={Surgery;Three-dimensional displays;Calibration;Microscopy;Optical imaging;Biomedical optical imaging;Optical microscopy;H.5.1 [Information Interfaces And Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;I.2.1 [Artificial Intelligence] Applications and Expert Systems—Medicine and science;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding—3D/stereo scene analysis;I.4.8 [Image Processing And Computer Vision]: Scene Analysis—Stereo;J.3 [Computer Applications]: Life And Medical Sciences—Medical information systems},
  doi={10.1109/VR.2019.8797682},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10108776,
  author={Zhang, Shuhao and Li, Yue and Man, Ka Lok and Yue, Yong and Smith, Jeremy},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Towards Cross-Reality Interaction and Collaboration: A Comparative Study of Object Selection and Manipulation in Reality and Virtuality}, 
  year={2023},
  volume={},
  number={},
  pages={330-337},
  abstract={Cross-Reality (CR) is an important topic for the research of multiuser collaborative systems. It allows users to participate in the reality-virtuality continuum and select appropriate interactive systems to work with, such as Virtual Reality Head-Mounted Displays (VR HMDs). However, there is limited work showing how interaction in VR differs from the more commonly used Personal Computers (PCs) and tablet devices in terms of object selection and manipulation. In this paper, we present a comparative study that investigated how users perform and perceive workload on 3D object selection and manipulation tasks using different devices (e.g. PC, tablet, and VR). We recorded the time and accuracy as objective task performance measures, and users' self-reported workload as a subjective measure. Our results revealed that unlike the biased performances of PC and tablet, VR has a balanced performance and great potentials in complex tasks.},
  keywords={Performance evaluation;Three-dimensional displays;Head-mounted displays;Interactive systems;Collaboration;Virtual reality;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
  doi={10.1109/VRW58643.2023.00075},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8797733,
  author={Cavallo, Marco and Dholakia, Mishal and Havlena, Matous and Ocheltree, Kenneth and Podlaseck, Mark},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative Information Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={145-153},
  abstract={Immersive environments have gradually become standard for visualizing and analyzing large or complex datasets that would otherwise be cumbersome, if not impossible, to explore through smaller scale computing devices. However, this type of workspace often proves to possess limitations in terms of interaction, flexibility, cost and scalability. In this paper we introduce a novel immersive environment called Dataspace, which features a new combination of heterogeneous technologies and methods of interaction towards creating a better team workspace. Dataspace provides 15 high-resolution displays that can be dynamically reconfigured in space through robotic arms, a central table where information can be projected, and a unique integration with augmented reality (AR) and virtual reality (VR) headsets and other mobile devices. In particular, we contribute novel interaction methodologies to couple the physical environment with AR and VR technologies, enabling visualization of complex types of data and mitigating the scalability issues of existing immersive environments. We demonstrate through four use cases how this environment can be effectively used across different domains and reconfigured based on user requirements. Finally, we compare Dataspace with existing technologies, summarizing the trade-offs that should be considered when attempting to build better collaborative workspaces for the future.},
  keywords={Data visualization;Three-dimensional displays;Collaboration;Headphones;Two dimensional displays;Augmented reality;Human-centered computing—Visualization—Visualization Systems and Tools;Human-centered computing—Human computer interaction (HCI)—Interactive systems and tools},
  doi={10.1109/VR.2019.8797733},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{10108830,
  author={Stacchio, Lorenzo and Armandi, Vincenzo and Donatiello, Lorenzo and Marfia, Gustavo},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={AnnHoloTator: A Mixed Reality Collaborative Platform for Manufacturing Work Instruction Interaction}, 
  year={2023},
  volume={},
  number={},
  pages={418-424},
  abstract={Admittedly, eXtend Reality (XR) technologies may play a key role in the digital transformation sparked by the Industry 4.0 initiative. Within the virtuality continuum, Augmented and Mixed Reality (AR, MR) emerge as disruptive technologies as they can improve industrial processes while maximizing worker efficiency and mobility by providing virtual elements in real-world scenarios where and when necessary. In particular, such technologies can foster the adoption of Lean Manufacturing (LM) paradigms supporting on-site assembly processes to improve productivity. We here describe how such an approach has been implemented in a real use case, with the development of AnnHoloTator, a collaborative mixed-reality platform for Microsoft Hololens 2 that allows users to visualize and manipulate digital documents directly in AR. Unlike existing systems, AnnHoloTator lets its users explore and modify the content of the digital documents stored in the cloud by interacting with their holograms. The system provides a data-flexible approach, considering that documents reside on a remote server, where also the majority of computation is performed. This provides an advantage: on-site professionals can directly annotate a virtual document and share it with other workers.},
  keywords={Productivity;Visualization;Social computing;Three-dimensional displays;Collaboration;Mixed reality;User interfaces;Human-centered computing-Visualization-Visualization systems and tools;Human-centered computing-Collaborative and social computing-Collaborative and social computing systems and tools;Applied computing-Document management and text processing-Document management},
  doi={10.1109/VRW58643.2023.00091},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10536594,
  author={Moser, Thomas and Schlager, Alexander},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Training and Assistance in Industrial Environments Using Extended Reality}, 
  year={2024},
  volume={},
  number={},
  pages={97-103},
  abstract={This study primarily focuses on developing a training and assistance framework that utilizes Extended Reality (XR) technologies, encompassing Augmented Reality (AR), Mixed Reality (MR), and Virtual Reality (VR). The objective is to facilitate intuitive learning of new work methodologies and enhance technical skills through task-oriented strategies in planning, execution, and production. Central to this paper are five VR-based scenarios, which are a subset of 22 diverse use cases explored in a collaborative effort with four Austrian universities. The research methodology entailed a thorough assessment of the existing training practices within participating companies, leading to the creation of tailored, interactive prototypes through a cyclic development process. These prototypes underwent rigorous on-site testing and evaluation by the employees of the industrial partners, employing a comprehensive multi-criteria evaluation model. The feedback gathered was instrumental in gauging the efficacy of these novel training approaches in comparison to conventional methods, and in pinpointing elements of the use cases that could be standardized for future development of training applications. This paper makes a significant contribution to the domain by shedding light on the practical implementation of XR technologies in industrial training environments, and by discussing the potential advantages and challenges inherent in their adoption.},
  keywords={Industrial training;Technological innovation;Three-dimensional displays;Extended reality;Scalability;Prototypes;User interfaces;Human-centered computing—Interaction Paradigms—Mixed/Augmented reality;Human-centered computing—Interaction Paradigms—Virtual reality;Applied computing—Education—Computer-assisted instruction;Applied computing—Education—Interactive learning environments},
  doi={10.1109/VRW62533.2024.00023},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9756783,
  author={Kavakli, Manolya and Cremona, Cinzia},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={The Virtual Production Studio Concept – An Emerging Game Changer in Filmmaking}, 
  year={2022},
  volume={},
  number={},
  pages={29-37},
  abstract={This paper analyses methodological and conceptual shifts in filmmaking brought about by the integrated use of VR technology, game engines, and LED walls in a Virtual Production Studio (VPS). The paper focuses not only on the components of an integrated VPS system, but also on what makes it qualitatively different from traditional filmmaking technologies. The VPS concept transforms the relationships among display technology, live action, image production, and postproduction (VFX) by causing a qualitative and performative shift in filmmaking. The paper analyses the current state of VP research from both conceptual and technological perspectives, global VP facilities, and significant trends in Australia by presenting two business case studies. We have observed three major trends: Globalisation, Collaboration and Industrialisation. The paper concludes by stating that there is a need for an integrated VPS network through shared infrastructure and resources to foster high quality research and to solve industry-identified problems through industry-led collaborative research partnerships.},
  keywords={Tracking;Collaboration;Globalization;Entertainment industry;Production;Games;Cameras;Cinematography;Virtual reality;Virtual production studio;LED walls;filmmaking;in camera VFX;Film;K.6.1 [Management of Computing and Information Systems];Project and People Management—Life Cycle; K.7.m [The Computing Profession];Miscellaneous—Ethics},
  doi={10.1109/VR51125.2022.00020},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9419324,
  author={Williams, Adam S. and Ortega, Francisco R.},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Using a 6 Degrees of Freedom Virtual Reality Input Device With An Augmented Reality Headset In A Collaborative Environment}, 
  year={2021},
  volume={},
  number={},
  pages={205-209},
  abstract={Augmented reality headsets have become increasingly consumer-available. Often gesture and speech are the main input modalities provided by these headsets. For some tasks, users may need a more precise input method. Tracked controllers can be added by using image tracking; however, this is not always the most accurate solution. This work outlines how to use off-the-shelf products to create a collaborative cross-device mixed reality experience. In that experience, the positionally tracked inputs from one headset can be used by another headset that may not natively support them.},
  keywords={Headphones;Three-dimensional displays;Conferences;Collaboration;Mixed reality;Input devices;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;Pointing devices;Interaction paradigms;Collaborative interaction;Mixed / augmented reality;Virtual reality},
  doi={10.1109/VRW52623.2021.00045},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9090619,
  author={Pan, Ye and Mitchell, Kenny},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Group-Based Expert Walkthroughs: How Immersive Technologies Can Facilitate the Collaborative Authoring of Character Animation}, 
  year={2020},
  volume={},
  number={},
  pages={188-195},
  abstract={Immersive technologies have increasingly attracted the attention of the computer animation community in search of more intuitive and effective alternatives to the current sophisticated 2D interfaces. The higher affordances offered by 3D interaction, as well as the enhanced spatial understanding have the potential to improve the animators’ task, which is tremendously skill intensive and time-consuming. We explore the capabilities provided by our PoseMMR, multiple users posing and animating characters in a mixed reality (MR) environment, animation via group-based expert walkthroughs. We demonstrated our system can facilitate immersive posing, animation editing, version control and collaboration. We provide a set of guidelines and discussed the benefits and potential of immersive technologies for our future animation toolsets.},
  keywords={Animation;Three-dimensional displays;Collaboration;Virtual reality;Tools;Kinematics;Two dimensional displays;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);Interactive systems and tools;User interface toolkits},
  doi={10.1109/VRW50115.2020.00041},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9419273,
  author={CONGES, Aurélie and YANG, Peipei and Benaben, Frédérick and GRAHAM, Col. Jacob},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Using Virtual Reality to Facilitate Common Operational Pictures’ Representation}, 
  year={2021},
  volume={},
  number={},
  pages={336-341},
  abstract={During crisis, different organizations are involved, each with their jargon and communication devices. Information about the situation is needed by all and it is vital to be able to share the data. To centralize that data, Common Operational Pictures are implemented, but they do not remove the risk of information overload. We propose to use virtual reality to create a virtual environment improving COP representations, thus improving situational awareness and collaborative decision making.},
  keywords={Three-dimensional displays;Conferences;Decision making;Virtual environments;Collaboration;Organizations;User interfaces;Virtual reality;visualization;common operational picture;cooperation},
  doi={10.1109/VRW52623.2021.00067},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9756732,
  author={Liu, Kuan-Yu and Wong, Sai-Keung and Volonte, Matias and Ebrahimi, Elham and Babu, Sabarish V.},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Investigating the Effects of Leading and Following Behaviors of Virtual Humans in Collaborative Fine Motor Tasks in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  pages={330-339},
  abstract={We examined the effects on users during collaboration with two types of virtual human (VH) agents in object transportation in an immersive virtual environment or virtual reality (VR). The two types of virtual humans we examined are leader and follower agents. The goal of the users is to interact with the agents using natural language and carry objects from initial locations to destinations. In each trial, the follower agent follows a user’s instructions to perform actions to manipulate the object. The leader agent determines the appropriate actions that the agent and the user should perform. We developed a system which enabled users and virtual agents to carry objects in an intuitive manner. We conducted a within-subjects study to evaluate the user behaviors under two conditions: (LVH) interaction with a leader virtual human and (FVH) interaction with a follower virtual human. We found that the participants in LVH required a higher workload than that in FVH. However, the users’ experiences, game experiences, and users’ impressions between the two conditions were not significantly different.},
  keywords={Solid modeling;Three-dimensional displays;Multimedia systems;Natural languages;Collaboration;Virtual environments;Transportation;Virtual Humans;Virtual Reality;Animation;Behavior Modeling;Human Computer Interaction},
  doi={10.1109/VR51125.2022.00052},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9974175,
  author={Chan, Alvin and Lee, Jeannie S.A. and Chen, Renjie},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Message from the ISMAR 2022 Demos Chairs}, 
  year={2022},
  volume={},
  number={},
  pages={xix-xix},
  abstract={It is our great pleasure to present the Demos program for the 21st IEEE International Symposium on Mixed and Augmented Reality. In this fast-evolving field of XR, it is particularly important for researchers to share their first-hand experiences of new methods, and system development approaches. The Demos session is an open forum where researchers from different backgrounds bridge the boundaries between art and technology and form new collaborations.},
  keywords={},
  doi={10.1109/ISMAR-Adjunct57072.2022.00007},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{5336747,
  author={Rolland, Jannick P. and Thompson, Brian J. and Stapleton, Christopher},
  booktitle={2009 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media and Humanities}, 
  title={Messages from the symposium general chairs}, 
  year={2009},
  volume={},
  number={},
  pages={x-x},
  abstract={Mixed and Augmented Reality melts the boundaries between visceral physical reality and a dynamic virtual reality to enhance human performance and heighten experiences. It extends the power of our imagination to create and extends the effcacy of communication, and collaboration. Our symposium has expanded to represent and serve a large and diverse community. It is designed to address research challenges in science, business, professional trades, the arts, media and the humanities. ISMAR 2009 marks a transformational year; the beginning of an expanded program to include the Art, Media and Humanities. We have instituted the frst Art, Media, and Humanities program to interpret and assess the impact of this technology on the world in which we live. For those who are experiencing this feld for the frst time, we have created a comprehensive tutorial program on Mixed and Augmented Reality. Our industry workshops represent diverse professional sectors that are now applying Mixed and Augmented Reality in a rich collection of experiential environments. Within the last year, the chasm between the early adopters and the main stream has been breached with mobile devices; we are seeing how the magic of Mixed and Augmented Reality is enhancing our ability to experience content and communicate information. Mixed and Augmented Reality is no longer defned by its technology alone, it has become the leading edge of an emerging generation. The innovations in technology are creating exciting opportunities that challenge the way we live our lives. ISMAR 2009 provides a forum for us to assimilate, refect, and accelerate as we are presented with how mixing realities is and will change how we live, work, play, and learn. As an international, interdisciplinary symposium, we are not only introducing technology solutions to modalities of communication thereby breaking down the boundaries between reality and our imagination, we are also in the process of melting boundaries between nations, cultures, disciplines, sectors, and markets. With this transformational year, ISMAR can emerge as a platform, established during this 1 week symposium, to represent and serve a year-round growing global community and professional network.},
  keywords={},
  doi={10.1109/ISMAR-AMH.2009.5336747},
  ISSN={2381-8360},
  month={Oct},}

@INPROCEEDINGS{5643301,
  author={Debackere, Boris},
  booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality - Arts, Media, and Humanities}, 
  title={Augmented dreams}, 
  year={2010},
  volume={},
  number={},
  pages={xv-xv},
  abstract={Many forms of artistic expression might be considered as an augmentation of reality. What are the challenges for 'Augmented Reality - technologies that enhance physical reality by layering interactive computer-generated content to it - when used as an artistic medium? While the underlying technologies (tracking, displaying and rendering) that drive augmented reality focus on a transparent creation, distribution and access of information the artist should critically examine their implementation. V2_Lab considers the artistic Research and Development (aRt&D) as an important element that can introduce specific qualities in the field of technological innovation and realization. Especially in the interdisciplinary collaboration with engineers and computer scientists, the artistic approach is unconventional in connecting different research fields, leaving behind discipline-specific paradigms. A key question in the creation of an art project is: what should the work communicate and what should the audience experience? Cinema could be an interesting case study, it has stimulated our imagination for more than a century and numerous successive media strive towards achieving a resembling experience in their audience: a cinematic impact. Nowadays cinema is everywhere, especially outside the confines of the movie theatre: it exists in all manner of altered forms and has become moreover an essential aspect of contemporary art. The interest that artists have in cinema is nothing new: it can be trace back to the early twentieth-century avant-gardes who explored the possibilities of film and initiated the continuing interaction between art and cinema. What is this particular experience we describe as 'cinematic' that attracts us to the movie theatre? In his book from 1916, The Photoplay A Psychological Study, Hugo Münsterberg states: "Everybody knows from his own experience that there is a sharp and specific analogy between the film forms and the mental mechanism by which consciousness functions on all its levels." In order to emerge, the cinema illusion asks for imagination. Film functions as a trigger for the mental processes that generate the true inner illusion. Film fused the magical way of creating movement introduced by the optical illusion toys with the qualities of photography to capture and represent reality. This merger shifted the attention of watching movement depicted to an expressive way of creating illusions by framing the world, structuring time and linking one experience to the next with sensations of images and sounds in space and time. Only later did the world in front of the camera became a constructed one, based on adopted elements of narration, stage play and music from theatre, opera and Vaudeville. A fabricated mise en scene to appeal the human imagination chimerically: unreal, imaginary and visionary. The present accelerated progress of information technologies are inevitably defining new directions of how moving images will be experienced in the future, going beyond the viewing constellation of today and changing the relation between the creator, his tools and the viewer. Digital media do not represent, they generate. They are software rather than hardware and unlike any other medium we have ever known, ephemeral: transforming and growing systems in itself. The virtual tool becomes a (re-)active actor in the creation of dynamic processes and demands for different approaches to those from the era of mimicking media.},
  keywords={},
  doi={10.1109/ISMAR-AMH.2010.5643301},
  ISSN={2381-8360},
  month={Oct},}

@INPROCEEDINGS{1544641,
  author={},
  booktitle={Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'05)}, 
  title={Proceedings. International Symposium on Mixed and Augmented Reality}, 
  year={2005},
  volume={},
  number={},
  pages={},
  abstract={The following topics are dealt with: image rendering; head-mounted displays; vision-based tracking; collaborative mixed reality; interaction techniques; augmented reality; calibration; authoring systems.},
  keywords={},
  doi={10.1109/ISMAR.2005.34},
  ISSN={},
  month={Oct},}



@INPROCEEDINGS{8699172,
  author={García-Pereira, Inma and Gimeno, Jesus and Pérez, Manolo and Portalés, Cristina and Casas, Sergio},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={MIME: A Mixed-Space Collaborative System with Three Immersion Levels and Multiple Users}, 
  year={2018},
  volume={},
  number={},
  pages={179-183},
  abstract={Shared spaces for remote collaboration are nowadays possible by considering a variety of users, devices, immersion systems, interaction capabilities, navigation paradigms, etc. There is a substantial amount of research done in this line, proposing different solutions. However, still a more general solution that considers the heterogeneity of the involved actors/items is lacking. In this paper, we present MIME, a mixed-space tri-collaborative system. Differently from other mixed-space systems, MIME considers three different types of users (in different locations) according to the level of immersion in the system, who can interact simultaneously – what we call a tri-collaboration. For the three types, we provide a solution to navigate, point at objects/locations and make annotations, while users are able to see a virtual representation of the rest of users. Additionally, the total number of users that can simultaneously interact with the system is only restricted by the available hardware, i.e., various users of the same type can be simultaneously connected to the system. We have conducted a preliminary study at the laboratory level, showing that MIME is a promising tool that can be used in many real cases for different purposes.},
  keywords={Augmented reality;Augmented Reality;Virtual Reality;shared spaces;mixed-space;remote collaboration;K.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
  doi={10.1109/ISMAR-Adjunct.2018.00062},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{6948517,
  author={Lukosch, Stephan and Billinghurst, Mark and Kiyokawa, Kiyoshi and Alem, Leila},
  booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Collaboration in mediated and augmented reality}, 
  year={2014},
  volume={},
  number={},
  pages={1-1},
  abstract={In this half-day workshop we will explore how Augmented Reality (AR) and Mediated Reality (MR) can be used to develop radically new types of collaborative experiences that overcome some of the limitations of current conferencing systems. In combination, AR and MR technologies could be used to merge the shared perceived realities of different users as well as enriching their own individual experience in a collaborative task. The goal of the workshop is to bring together researchers who are interested developing collaborative systems using AR and MR technologies. They will build a picture of current and prior research on collaboration in AR and MR as well as set up a common research agenda for work going forward. Topics of the workshop will address open research issues and include but are not restricted to the following: •Case studies on using MR/AR for collaboration •Tools for building collaborative MR/AR systems •Effects of MR/AR on trust, presence, and coordination •Interaction models for collaboration in MR/AR •Tools for collaboration in MR/AR •Collaboration awareness in MR/AR.},
  keywords={Collaboration;Educational institutions;Conferences;Augmented reality;Information systems;Computers},
  doi={10.1109/ISMAR.2014.6948517},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8951978,
  author={Teo, Theophilus and Lee, Gun A. and Billinghurst, Mark and Adcock, Matt},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Merging Live and Static 360 Panoramas Inside a 3D Scene for Mixed Reality Remote Collaboration}, 
  year={2019},
  volume={},
  number={},
  pages={22-25},
  abstract={We propose a new technique for Mixed Reality (MR) remote collaboration that uses live and static 360 panoramas inside a 3D scene. Prior work on MR remote collaboration demonstrated benefits of using a 360 panorama, or using a 3D scene. However, collaboration using 360 panorama offers limited spatial information. Our prototype system allows viewing and manipulating of 360 panoramas and visual cues inside a 3D scene, adding spatial information for enhanced contextual understanding between the remote collaborators. We designed a system that combines Augmented Reality (AR) and Virtual Reality (VR) to bridge the visual cues and information relative to the 3D space to help enhance the user experience and interaction in a larger scale MR remote collaboration. We describe our implementation in detail with a discussion of the design implications and potential benefits that could help improve MR remote collaboration.},
  keywords={Three-dimensional displays;Collaboration;Visualization;Prototypes;Avatars;Cameras;Mixed Reality;Remote Collaboration;360 Panorama;3D Reconstruction},
  doi={10.1109/ISMAR-Adjunct.2019.00021},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8951977,
  author={Sasikumar, Prasanth and Gao, Lei and Bai, Huidong and Billinghurst, Mark},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Wearable RemoteFusion: A Mixed Reality Remote Collaboration System with Local Eye Gaze and Remote Hand Gesture Sharing}, 
  year={2019},
  volume={},
  number={},
  pages={393-394},
  abstract={We present a wearable Mixed Reality (MR) remote collaboration system called Wearable RemoteFusion. The system supports spatial annotation and view frustum sharing, and enables natural non-verbal communication cues (eye gaze and hand gesture) for visual assistance in a stitched live dense scene. We describe the design and implementation details of the prototype system, and report on a pilot user study investigating how sharing natural gaze and gesture cues affects collaborative performance and the user experience. We found that by sharing augmented natural cues like the local eye gaze and remote hand gesture, participants had a stronger feeling of Co-presence, and the remote user could guide the local user to complete tasks with less physical workload. We discuss implications for collaborative interface design and directions for future research.},
  keywords={Collaboration;Virtual reality;Task analysis;Three-dimensional displays;Annotations;Biomedical engineering;Mixed Reality, Augmented Reality, remote collaboration, eye gaze, hand gesture},
  doi={10.1109/ISMAR-Adjunct.2019.000-3},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8699227,
  author={Wang, Peng and Zhang, Shusheng and Bai, Xiaoliang and Billinghurst, Mark and He, Weiping and Zhang, Li and Du, Jiaxiang and Wang, Shuxia},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Do You Know What I Mean? An MR-Based Collaborative Platform}, 
  year={2018},
  volume={},
  number={},
  pages={77-78},
  abstract={The Mixed Reality (MR) technology can be used to create unique collaborative experiences. In this paper, we propose a new remote collaboration platform using MR and eye-tracking that enables a remote helper to assist a local worker in an assembly task. We present results from research exploring the effect of sharing virtual gaze and annotations cues in an MR-based projector interface for remote collaboration. The key advantage compared to other remote collaborative MR interfaces is that it projects the remote expert's eye gaze into the real worksite to improve co-presence. The prototype system was evaluated with a pilot study comparing two conditions: POINTER and ET (eye-tracker cues). We observed that the task completion performance was better in the ET condition. And that sharing gaze significantly improved the awareness of each other's focus and co-presence.},
  keywords={Augmented reality;Remote collaboration;Eye gaze;Augmented Reality;Mixed Reality;H.5.1 [User Experience Design]: Collaborative interfaces—Collaboration, augmented and virtual realities},
  doi={10.1109/ISMAR-Adjunct.2018.00038},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10322184,
  author={Li, Ke and Rolff, Tim and Bacher, Reinhard and Steinicke, Frank},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={RealityGit: Cross Reality Version Control of R&D Optical Workbench}, 
  year={2023},
  volume={},
  number={},
  pages={807-808},
  abstract={Multi-user collaboration at research and development (R&D) workbenches in advanced optics laboratories involves updating and documenting the status of hundreds of components. Although there are abundant mixed reality (MR) systems to support the collaboration of spatially co-located users, limited methods are available to facilitate cooperation in the spatial-temporal domain. Inspired by the version control workflow used in software development, we propose RealityGit, a novel cross-reality (CR) system design that leverages the recent advancements of neural radiance fields (NeRF). We illustrate how the NeRF model can be used to channel users with different extended reality (XR) experiences by providing an accurate immersive visual documentation of the historical states of a complex R&D workbench. We demonstrate the feasibility of such a system through an implementation where users could contribute to the version control workflows by integrating historical NeRF scans into their MR devices or providing status reviews by annotating or editing a NeRF model in virtual reality (VR).},
  keywords={Solid modeling;Visualization;Collaboration;Mixed reality;Optics;Software;X reality;Mixed Reality;Version Control;Neural Radiance Fields;Human-centered computing},
  doi={10.1109/ISMAR-Adjunct60411.2023.00178},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{9585889,
  author={Jing, Allison},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={The Impact of Gaze Cues in Mixed Reality Collaborations}, 
  year={2021},
  volume={},
  number={},
  pages={473-475},
  abstract={Gaze is one of the most important communication cues in performing physical tasks in both face-to-face and remote collaboration. Dynamic gaze information can indicate the user’s intention, focus, and current attention while visualising this information can often compensate for other communication channels that are not always readily available. Previous studies have shown that sharing and understanding another person’s gaze cues can benefit mutual awareness and task coordination in traditional 2D displays. However, researchers have not fully explored the impact of the virtual representations of gaze cues using Mixed Reality technologies. In this doctoral consortium presentation, I will present eyemR-Vis, a 360 panoramic Mixed Reality (MR) remote collaboration system that shares gaze behavioural visualisations between a local worker and a remote collaborator. In the paper I discuss the PhD research motivation, background material, recently published study results, and plans for future work.},
  keywords={Visualization;Two dimensional displays;Mixed reality;Collaboration;Medical services;Communication channels;Task analysis;Gaze Visualisation;Mixed Reality Remote Collaboration},
  doi={10.1109/ISMAR-Adjunct54149.2021.00111},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10322284,
  author={Cho, Hyunwoo and Yuan, Bowen and Hart, Jonathon Derek and Chang, Eunhee and Chang, Zhuang and Cao, Jiashuo and Lee, Gun A. and Piumsomboon, Thammathip and Billinghurst, Mark},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={An Asynchronous Hybrid Cross Reality Collaborative System}, 
  year={2023},
  volume={},
  number={},
  pages={70-73},
  abstract={This work presents a Mixed Reality (MR)-based asynchronous hybrid cross reality collaborative system which supports recording and playback of user actions in three-dimensional task space at different periods in time. Using this system, an expert user can record a task process such as virtual object placement or assembly, which can then be viewed by other users in either Augmented Reality (AR) or Virtual Reality (VR) views at later points in time to complete the task. In VR, the pre-scanned 3D workspace can be experienced to enhance the understanding of spatial information. Alternatively, AR can provide real-scale information to help the workers manipulate real world objects, and complete the task assignment. Users can also seamlessly move between AR and VR views as desired. In this way the system can contribute to improving task performance and co-presence during asynchronous collaboration.},
  keywords={Training;Visualization;Three-dimensional displays;Avatars;Collaboration;Prototypes;Mixed reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Mixed / augmented reality},
  doi={10.1109/ISMAR-Adjunct60411.2023.00022},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{10322134,
  author={Grønbæk, Jens Emil},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Transitional Blended Realities: Linking between Video and Mixed Reality for Distributed Team Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={220-222},
  abstract={This paper envisions Transitional Blended Realities (TBR)—systems that integrate interfaces for Video Conferencing and Collaborative Mixed Reality. By utilizing a visual overlap in the reference space between remote physical spaces, TBR supports fluid transitions between video conferencing and mixed reality collaboration. This approach has the potential to enhance both collaboration flexibility and accessibility, fostering more dynamic distributed team meetings.},
  keywords={Human computer interaction;Visualization;Social computing;Fluids;Mixed reality;Collaboration;Switches;[Human-centered computing Mixed / augmented reality]: Human-centered computing Collaborative and social computing systems and tools},
  doi={10.1109/ISMAR-Adjunct60411.2023.00051},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{10322266,
  author={Friedl-Knirsch, Judith and Stach, Christian and Anthes, Christoph},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Exploring Collaboration for Data Analysis in Augmented Reality for Multiple Devices}, 
  year={2023},
  volume={},
  number={},
  pages={65-69},
  abstract={Collaboration is a key aspect of Cross-Virtuality Analytics. When collaborating in Augmented Reality, there are different types of display technologies available. In this work we elaborate on differences between video see-through, optical see-through and handheld Augmented Reality technologies with currently available hardware. We then present the concept of a prototype for collaborative data analysis combining these different technologies with laptops and pen and paper. Finally, we report on the initial findings of a pilot study on the impact of these technologies on collaboration in data analysis.},
  keywords={Visualization;Data analysis;Portable computers;Collaboration;Prototypes;Hardware;Augmented reality;Human-centered computing;Mixed / augmented reality;User studies},
  doi={10.1109/ISMAR-Adjunct60411.2023.00021},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{8088489,
  author={Piumsomboon, Thammathip and Dey, Arindam and Ens, Barrett and Lee, Gun and Billinghurst, Mark},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, 
  title={[POSTER] CoVAR: Mixed-Platform Remote Collaborative Augmented and Virtual Realities System with Shared Collaboration Cues}, 
  year={2017},
  volume={},
  number={},
  pages={218-219},
  abstract={We present CoVAR, a novel Virtual Reality (VR) and Augmented Reality (AR) system for remote collaboration. It supports collaboration between AR and VR users by sharing a 3D reconstruction of the AR user's environment. To enhance this mixed platform collaboration, it provides natural inputs such as eye-gaze and hand gestures, remote embodiment through avatar's head and hands, and awareness cues of field-of-view and gaze cue. In this paper, we describe the system architecture, setup and calibration procedures, input methods and interaction, and collaboration enhancement features.},
  keywords={Collaboration;Augmented reality;Calibration;Image reconstruction;Three-dimensional displays;Tracking;mixed reality;remote collaboration;eye tracking},
  doi={10.1109/ISMAR-Adjunct.2017.72},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10322265,
  author={Cho, Hyunwoo and Chang, Eunhee and Chang, Zhuang and Cao, Jiashuo and Yuan, Bowen and Hart, Jonathon Derek and Lee, Gun A. and Piumsomboon, Thammathip and Billinghurst, Mark},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Hybrid Cross-Reality Collaborative System}, 
  year={2023},
  volume={},
  number={},
  pages={775-776},
  abstract={This work introduces a Mixed Reality (MR)-based hybrid cross-reality collaborative system that enables recording and playback of user actions in a large task space over time. An expert records their actions, such as virtual object placement, that others can view in Augmented Reality (AR) or Virtual Reality (VR) later to complete a task. VR offers a pre-scanned 3D workspace for spatial understanding, while AR gives real-scale information for manipulating real objects. Users can easily switch between AR and VR views, enhancing task performance and co-presence during asynchronous collaboration. The system is showcased in an object assembly scenario with parts retrieved from a storehouse. A pilot study showed that this cross-reality system improved task completion speed with lower workload.},
  keywords={Training;Visualization;Three-dimensional displays;Collaboration;Prototypes;Mixed reality;Switches;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Mixed/augmented reality},
  doi={10.1109/ISMAR-Adjunct60411.2023.00170},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{7836537,
  author={Salazar, Mikel and Laorden, Carlos},
  booktitle={2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, 
  title={Collaborative content creation with the OpenUIX Framework}, 
  year={2016},
  volume={},
  number={},
  pages={350-350},
  abstract={The OpenUIX framework aims to provide end-users -and developers- with a level playing field where they can not only access 3D user interfaces adapted to their context (location, time of day, hardware platform, disabilities, etc.), but also create and share them with others. To achieve this goal, the framework has a built-in toolset that allows the collaborative edition of the different components of a interaction space; from the threedimensional representations of the different entities (either virtual or real in nature), to the lighting conditions or the logic behaviors that govern the user experience. With this demo, the ISMAR 2016 attendees will be able to test several of the aforementioned collaborative edition tools in different interaction spaces across the entire conference venue (and even outside of it, if the circumstances allow it). Initially, the attendees will have the opportunity to test the different tools at the table provided by the organization of the event (and with mobile platforms provided by the presenters, to ensure the proper execution of the demo). In this way, the presenters will be able to explain and guide them through a series of basic use cases, ranging from virtual painting on 2D surfaces or 3D modeling to how to employ the UI description language the framework employs to modify the lighting conditions of the interaction space (thus allowing a more seamless integration of virtual objects into the physical world). Once the attendees go through the basic use cases and prove their understanding of the interaction techniques, they will receive a code to download a development version of an AR browser created with this framework (available for Android and iOS platforms). With this browser installed in their own devices, the attendees will be able to access other interaction spaces across the venue (that the presenters will prepare beforehand).},
  keywords={Collaboration;Mobile communication;Painting;Browsers;Human computer interaction;Hardware;Three-dimensional displays;3DUI;collaborative edition;interaction spaces},
  doi={10.1109/ISMAR-Adjunct.2016.0116},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{6948493,
  author={Schattel, David and Tönnis, Marcus and Klinker, Gudrun and Schubert, Gerhard and Petzold, Frank},
  booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={[Demo] On-site augmented collaborative architecture visualization}, 
  year={2014},
  volume={},
  number={},
  pages={369-370},
  abstract={The early design phase for a new building is a crucial stage in the design process of architects. It has to be ensured that the building fits into the future environment. The Collaborative Design Platform targets this issue by integrating modern digital means with well known traditional concepts. Well-used styrofoam blocks are still cut by hand but are now tracked, placed and visualized in 3D by use of a tabletop platform and a TV screen showing an arbitrary view of the scenery. With this demonstration, we get one step further and provide an interactive visualization at the proposed building site, further enhancing collaboration between different audiences. Mobile phones and tablet devices are used to visualize marker-less registered virtual building structures and immediately show changes made to the models in the Collaborative Design laboratory. This way, architects can get a direct impression about how a building will integrate within the environment and residents can get an early impression about future plans.},
  keywords={Buildings;Three-dimensional displays;Solid modeling;Augmented reality;Collaboration;Mobile handsets;Visualization;H.5.m [Information interfaces and presentation];Miscellaneous},
  doi={10.1109/ISMAR.2014.6948493},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{6402584,
  author={Morishima, Shigeki and Mashita, Tomohiro and Kiyokawa, Kiyoshi and Takemura, Haruo},
  booktitle={2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A waist-mounted ProCam system for remote collaboration}, 
  year={2012},
  volume={},
  number={},
  pages={301-302},
  abstract={We propose a waist-mounted projector-camera (ProCam) system for asymmetric remote collaboration. A wearable camera is often used to transmit a worker's situation to a remote instructor, however 3D structure of the worker's environment is not always available and the instructor has a minimal flexibility in changing the camera's viewpoint. A stationary 3D measurement system is also commonly used for remote collaboration, however a narrow measurement area and occlusion from a worker's body can be a severe problem. Our waist-mounted ProCam system reconstructs worker's environment in real-time without occlusion from the worker's body. The remote instructor can give instructions simply by drawing annotations on the reconstructed environment on screen, and they are properly projected in front of the worker. Structured-light based reconstruction, vision-based localization, and visual annotation projection, are processed synchronously with a camera and modified shutter glasses so that both the camera and the worker observe only the information they need. Experimental results show that users prefer our system to a stationary ProCam system.},
  keywords={Cameras;Collaboration;Glass;Educational institutions;Solid modeling;Electronic mail;Area measurement;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial;augmented;virtual realities H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Computer-Supported Cooperative Work},
  doi={10.1109/ISMAR.2012.6402584},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{8951888,
  author={Zhang, YanXiang and Tao, Li and Lu, Yaping and Li, Ying},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Design of Paper Book Oriented Augmented Reality Collaborative Annotation System for Science Education}, 
  year={2019},
  volume={},
  number={},
  pages={417-421},
  abstract={The authors designed a kind of augmented reality annotation system based on network knowledge collaboration for primary science education to expand the cognitive effect. Various types of annotations such as text, images, videos, links, 3D models, etc. can be added to the corresponding position of the paper book by multi-user and multi-device through the system. And the annotation contents could be retrieved in AR mode by other users. The connotation and dimension of scientific knowledge could be expanded through concentrating diversified annotations. The system only records the relative position of the annotation on the page by a hand-aided registration, and uploads the location information to the server without infringing the copyright of the book. The system allows users to add links as annotations, through which users could interact with social media and knowledge communities. By using this system, users' ideas could be connected thus promote the flow of knowledge between different types of readers (such as students, parents, and teachers), readers and authors and it is conducive to the exchange and inspiration of ideas, promoting the integration of knowledge and the generation of group wisdom.},
  keywords={Annotations;Collaboration;Databases;Augmented reality;Videos;Mobile handsets;Education;Collaborative-system,-knowledge-collaboration,-augmented-reality,-annotation,-science-education},
  doi={10.1109/ISMAR-Adjunct.2019.00045},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9974241,
  author={Qiu, Dongyu and Ow, Clemen and Xia, Xinxing and Yeo, Jin Qi and Xia, Jiazhi and Hean Low, Malcolm Yoke and Wang, Zhengkui and Shoon Chan, Alvin Toong and Guan, Frank Yunqing},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={ViCollAR: A Novel System for 3D Data Visualization using Collaborative Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={907-908},
  abstract={Data visualization refers to the presentation of data in a pictorial or graphical format and it can significantly help users, e.g., decision makers, to see the analytics presented visually, such that they can grasp key concepts or identify new patterns rapidly. Various tools and systems have been developed towards data visualization, but they all suffer from different limitations towards immersive and interactive data visualization. In this paper, we presented our developed system, ViCollAR, which aims to achieve immersive, interactive and collaborative 3D data visualization leveraging Augmented Reality (AR) technologies. ViCollAR allows multiple users to simultaneously and collaboratively view and interact with the visualized data using their bare hands in 3D space, and it allows the users to freely navigate through the real spatial environment during interactions. A prototype has been successfully developed for the targeted system and tested by users.},
  keywords={Productivity;Three-dimensional displays;Navigation;Data visualization;Collaboration;Prototypes;Augmented reality;Augmented Reality;Data Visualization;User Interaction;Immersion},
  doi={10.1109/ISMAR-Adjunct57072.2022.00199},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{8699245,
  author={Hart, Jonathon D. and Piumsomboon, Thammathip and Lee, Gun and Billinghurst, Mark},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Sharing and Augmenting Emotion in Collaborative Mixed Reality}, 
  year={2018},
  volume={},
  number={},
  pages={212-213},
  abstract={We present a concept of emotion sharing and augmentation for collaborative mixed-reality. To depict the ideal use case of such system, we give two example scenarios. We describe our prototype system for capturing and augmenting emotion through facial expression, eye-gaze, voice, physiological data and share them through their virtual representation, and discuss on future research directions with potential applications.},
  keywords={Augmented reality;Mixed reality;avatars;emotion sharing;emotion augmentation;facial expression;remote collaboration;H.5.1 Information interfaces and presentation (e.g., HCI): Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/ISMAR-Adjunct.2018.00069},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8951982,
  author={Wang, Peng and Bai, Xiaoliang and Billinghurst, Mark and Zhang, Shusheng and Han, Dechuan and Lv, Hao and He, Weiping and Yan, Yuxiang and Zhang, Xiangyu and Min, Haitao},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={An MR Remote Collaborative Platform Based on 3D CAD Models for Training in Industry}, 
  year={2019},
  volume={},
  number={},
  pages={91-92},
  abstract={In this paper, we describe a new Mixed Reality (MR) remote collaborative platform making use of 3D CAD models for training in the manufacturing industry. It enables a remote expert in Virtual Reality (VR) to train a local worker in a physical assembly task. For the local site, we use Spatial Augmented Reality (SAR) to enable the local worker see virtual cues without wearing any AR devices, leaving their user hands free to easily manipulate the physical parts. For the remote expert, we construct a 3D virtual environment using virtual replicas of the physical parts. We also report on the results of a usability study of the prototype.},
  keywords={Three-dimensional displays;Training;Collaboration;Solid modeling;Task analysis;Virtual reality;Usability;Remote collaboration;3D CAD models;training in the industry;Augmented Reality;Mixed reality},
  doi={10.1109/ISMAR-Adjunct.2019.00038},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9974297,
  author={Butaslac, Isidro III and Luchetti, Alessandro and Ino, Junya and Fujimoto, Yuichiro and Sawabe, Taishi and Kanbara, Masayuki and Kato, Hirokazu and Uemura, Keisuke and Otake, Yoshito and Sato, Yoshinobu and Takao, Masaki and Sugano, Nobuhiko},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Application of Participatory Design Methodology in AR: Developing Prototypes for Two Context Scenarios}, 
  year={2022},
  volume={},
  number={},
  pages={244-248},
  abstract={Recent technological developments and projects are now becoming more multidisciplinary. However, in designing augmented reality training and support systems (ARTSS), researchers in this field may already hold biases that lean towards the de facto standard of what constitutes good ARTSS. These biases are not inherently harmful, but there exists the risk that these may not lead to answering the demands and goals of the intended end-user that are of completely different specializations. To address this risk, we suggest using Par-ticipatory Design (PD) as a methodology for creating and designing ARTSS. By putting the collaborators as co-designers in the authoring of augmented reality prototypes, we can verify that the final ARTSS meets the end-user's requirements.},
  keywords={Training;Prototypes;Surgery;Collaboration;Stakeholders;Iterative methods;Augmented reality;Augmented reality;participatory design;iterative pro-totyping;surgery training;occupational therapy;Human-centered computing;Interaction design;Interaction design process and methods;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality},
  doi={10.1109/ISMAR-Adjunct57072.2022.00054},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{9585826,
  author={Kim, Hyung-Il and Kim, Taehei and Song, Eunhwa and Oh, Seo Young and Kim, Dooyoung and Woo, Woontack},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Multi-scale Mixed Reality Collaboration for Digital Twin}, 
  year={2021},
  volume={},
  number={},
  pages={435-436},
  abstract={In this poster, we present a digital twin-based mixed reality system for remote collaboration with the size-scaling of the user and the space. The proposed system supports collaboration between an AR host user and a VR remote user by sharing a 3D digital twin of the AR host user. To enhance the coarse authoring of a shared digital twin environment, we provide a size scaling of the digital twin environment with the world-in-miniature view. Also, we enable scaling the size of the VR user’s avatar to enhance both coarse (size-up) and fine-grained (size-down) authoring of the digital twin environment. We describe the system setup, input methods, and interaction methods for scaling space and user.},
  keywords={Three-dimensional displays;Digital twin;Avatars;Collaboration;Mixed reality;Systems support;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Collaborative interaction},
  doi={10.1109/ISMAR-Adjunct54149.2021.00098},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{6948459,
  author={Pucihar, Klen Čopič and Coulton, Paul},
  booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={[Poster] Utilizing contact-view as an augmented reality authoring method for printed document annotation}, 
  year={2014},
  volume={},
  number={},
  pages={299-300},
  abstract={In Augmented Reality (AR) the real world is enhanced by superimposed digital information commonly visualized through augmented annotations. The visualized data comes from many different data sources. One increasingly important source of data is user generated content. Unfortunately, AR tools that support user generated content are not common hence the majority of augmented data within AR applications is not generated utilizing AR technology. In this paper we discuss the main reasons for this and evaluate how the contact-view paradigm could enhance annotation authoring process within the class of tabletop size AR workspaces. This evaluation is based on a prototype that allows musicians to annotate a music score manuscript utilizing freehand drawing on top of device screen. Experimentation showed the potential of contact-view paradigm as an annotation authoring method that performs well in single and collaborative multi-user situations.},
  keywords={Augmented reality;Context;Prototypes;Collaboration;Rendering (computer graphics);User-generated content;Cameras;AR;magic-lens;authoring;annotation;collaboration;contact-view;mobile;handheld;tbaletop},
  doi={10.1109/ISMAR.2014.6948459},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{6948461,
  author={Reichherzer, Carolin and Nassani, Alaeddin and Billinghurst, Mark},
  booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={[Poster] Social panoramas using wearable computers}, 
  year={2014},
  volume={},
  number={},
  pages={303-304},
  abstract={In this paper we describe the concept of Social Panoramas that combine panorama images, Mixed Reality, and wearable computers to support remote collaboration. We have developed a prototype that allows panorama images to be explored in real time between a Google Glass user and a remote tablet user. This uses a variety of cues for supporting awareness, and enabling pointing and drawing. We conducted a study to explore if these cues can increase Social Presence. The results suggest that increased interaction does not increase Social Presence, but tools with a higher perceived usability show an improved sense of presence.},
  keywords={Glass;Usability;Collaboration;Cameras;Head;Prototypes;Wearable computers;Head-mounted Display;Panorama;Remote Collaboration;Mixed Reality;Mobile Computing},
  doi={10.1109/ISMAR.2014.6948461},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{8088514,
  author={Saito, Hideo and Mori, Shohei and Ikeda, Sei},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)}, 
  title={Workshop on highly diverse cameras and displays for mixed and augmented reality (HDCD4MAR)}, 
  year={2017},
  volume={},
  number={},
  pages={326-326},
  abstract={Summary form only given, as follows. A complete record of the workshop was not made available for publication as part of the conference proceedings. Computer vision technology has evolved AR/MR to a point and given AR/MR (augmented reality/mixed reality) communities fruits to solve unique problems in AR/MR. To bring AR/MR to the next level, we need to investigate a way to fully utilize these fruits. From this point of view, in this workshop, HDCD4MAR, we focus on bringing opportunities to researchers to discuss experiences and findings on vision-based approaches especially regarding the diversity of input and output devices that would exist in the AR/MR environments or be brought there by the AR/MR users. We expect participants from various types of research fields such as multi-view stereo, computational photography, vSLAM, light field displays, etc. and workshop will give chances them for future collaborations.},
  keywords={Augmented reality;Cameras;Computer vision;Photography;Collaboration},
  doi={10.1109/ISMAR-Adjunct.2017.84},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9974227,
  author={McDade, Jeremy and Drogemuller, Adam and Jing, Allison and Ireland, Nick and Walsh, James and Thomas, Bruce and Mayer, Wolfgang and Cunningham, Andrew},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={CADET: A Collaborative Agile Data Exploration Tool for Mixed Reality}, 
  year={2022},
  volume={},
  number={},
  pages={899-900},
  abstract={The need to understand and communicate the nuances of complex situational information is an ever-present requirement in Command and Control (C2). It is often difficult for remote users of a system to clearly understand what a user is trying to relay. Mixed Reality (MR) technology presents a significant opportunity for exploring and communicating C2 data. In this paper, we present our system, CADET, as step towards enriching the collaborative C2 user ex-perience by allowing users to remotely and locally perform adhoc analysis through information displays created by hand interactions and speech in MR.},
  keywords={Command and control systems;Mixed reality;Data visualization;Collaboration;Relays;Augmented reality;Human-centered computing- Visualization-Visualization application domains-Information visualization;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality},
  doi={10.1109/ISMAR-Adjunct57072.2022.00195},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{9585868,
  author={Gonsher, Ian and Lei, Zhenhong},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Prototype of Force Feedback Tool for Mixed Reality Applications}, 
  year={2021},
  volume={},
  number={},
  pages={508-509},
  abstract={This prototype demonstrates the viability of manipulating both physical and virtual objects with the same tool in order to maintain object permanence across both modes of interaction. Using oppositional force feedback, provided by a servo, and an augmented visual interface, provided by the user’s smartphone, this tool simulates the look and feel of a physical object within an augmented environment. Additionally, the tool is also able to manipulate physical objects that are not part of the augmented reality, such as a physical nut. By integrating both modes of interaction into the same tool, users can fluidly move between these different modes of interaction, manipulating both physical and virtual objects as the need arises. By overlaying this kind of visual and haptic augmentation onto a common tool such as a pair of pliers, we hope to further explore scenarios for collaborative telepresence in future work.},
  keywords={Visualization;Telepresence;Force feedback;Prototypes;Mixed reality;Tools;Fasteners;Mixed Reality;Haptic Feedback;Virtual Reality;Augmented Reality},
  doi={10.1109/ISMAR-Adjunct54149.2021.00123},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9974357,
  author={Santhosh, Sandhya},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Developing a Methodology for Co-creation using Extended Reality Technologies}, 
  year={2022},
  volume={},
  number={},
  pages={927-930},
  abstract={In the past decades there is a lot of heed towards Human Centered Design (HCD) Approach in which users are involved for needs collection or validation sessions of the prototypes for problem identification. A successful design relies also on how well different stakeholders are proactively involved since the beginning or even in the creative phase of product development. Therefore, in the recent times, innovative organizations are shifting their ideation strategies towards the concept of “co-creation” in order to improve their creative businesses. Co-creation is known to draw in final users or customers in the design process to collaborate and interact in jointly generating solutions and creating value. Thus, the platform where co-creators engage plays an influential role in improving the creative participation and thus user experience. Traditional co-creation platforms limit a user in various forms. The immense growth of innovative technologies such as Extended Reality (XR) tools have expanded to interactive and collaborative environment developments regulating new ways of working. My PhD research proposes XR tools as potential digital co-creation platforms and focuses on establishing the key factors that influence “co-creation” process in XR by laying out a methodology and developing number of applications fulfilling these factors during product development process promoting the idea. This paper describes the principles of co-creation, the importance of cocreation platforms, elements of co-creation and outlines few application scenarios developed in the context of product development process striving to demonstrate XR environment as a co-creation space. The research will also provide interesting insights into emerging design practices and expects to study the factors that affect a user in this innovative co-creative XR world and collect practical recommendations. Through ISMAR'22 Doctoral Consortium, I would like to gain expert inputs on validation methods for the applications developed that can strengthen the concept proposal.},
  keywords={Extended reality;Prototypes;Collaboration;Organizations;Medical services;Product development;Stakeholders;Co-Creation;Extended Reality;Product Development;virtual;Collaboration;Interaction;User Experience},
  doi={10.1109/ISMAR-Adjunct57072.2022.00207},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{6671795,
  author={Kim, Seungwon and Lee, Gun A. and Sakata, Nobuchika and Dünser, Andreas and Vartiainen, Elina and Billinghurst, Mark},
  booktitle={2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Study of augmented gesture communication cues and view sharing in remote collaboration}, 
  year={2013},
  volume={},
  number={},
  pages={261-262},
  abstract={In this research, we explore how different types of augmented gesture communication cues can be used under different view sharing techniques in a remote collaboration system. In a pilot study, we compared four conditions: (1) Pointers on Still Image, (2) Pointers on Live Video, (3) Annotation on Still Image, and (4) Annotation on Live Video. Through this study, we found three results. First, users collaborate more efficiently using annotation cues than pointer cues for communicating object position and orientation information. Second, live video becomes more important when quick feedback is needed. Third, the type of gesture cue has more influence on performance and user preference than the type of view sharing method.},
  keywords={Collaboration;Portable computers;Streaming media;Visualization;Augmented reality;Speech;Video Conferencing;Augmented Reality},
  doi={10.1109/ISMAR.2013.6671795},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{7344745,
  author={Lukosch, Stephan and Billinghurst, Mark and Kiyokawa, Kiyoshi and Feiner, Steven and Alem, Leila},
  booktitle={2015 IEEE International Symposium on Mixed and Augmented Reality Workshops}, 
  title={Collaboration in Mediated and Augmented Reality (CiMAR) Summary}, 
  year={2015},
  volume={},
  number={},
  pages={1-2},
  abstract={Summary form only given. The world is becoming more complex everyday, so problem solving often requires global teams of experts working together. To do this effectively there is a need for collaborative tools, and so a variety of teleconferencing and telepresence technologies have been developed. However, most of them involve some variation of traditional video conferencing, which has limitations, such as not being able to effectively convey spatial cues. This workshop focuses on how Augmented Reality (AR) [1] and Mediated Reality (MR) [2] technology can be used to overcome these limitations and develop radically new types of collaborative experiences. In combination, AR and MR technologies could be used to merge the shared perceived realities of different users, as well as enrich each user's own individual experience in a collaborative task. Several studies have explored the effectiveness of using AR and MR for complex tasks. For example, AR has been shown to improve the effectiveness of individual manual assembly tasks [3], while MR systems have been shown to improve the performance time and mental effort in collaborative design tasks [4]. Recent research on using MR for collaboration among crime scene investigators indicates that using shared visual feedback promotes mutual understanding, leads to consensus, and supports hypothesis testing [5]. There are many examples of collaborative AR applications. The Studierstube system [6] targets face-to-face presentations and allows users to walk around virtual 3D scientific data. WearCom [7] enables users to see remote users as virtual avatars in real space. Höllerer et al. [8] allow indoor AR users to visualize the locations and paths of outdoor AR users, and all users to create shared annotations. Poelman et al. [5] present an AR system that allows remote experts to collaborate with local investigators on a crime scene investigation in order to secure evidence. Datcu et al. [9] present an AR system that supports the distributed situational awareness of cross-organisational teams in the security domain. Gauglitz et al. [10] introduce a tablet-based system that incorporates a touchscreen interface through which a remote user can navigate a physical environment and create world-aligned annotations for supporting remote maintenance. In summary, MR and AR technology is becoming mature enough to support a variety of collaboration scenarios. However, there are still a number of open issues that need further research. One major issue concerns the presence of remote users and how they can interact with the system and each other. Partial answers can be found in the areas of human-computer interaction, social interaction, affective computing, and task domain analysis. This workshop brings together researchers who are developing or interested in creating collaborative systems using AR and MR technologies. Starting with a keynote from Prof. Tetsuro Ogi from Keio University, Japan, workshop participants will discuss open research issues in relation to: Case studies using MR/AR for collaboration; Tools for building collaborative MR/AR systems; Effects of MR/AR on trust, presence, and coordination; Interaction models for collaboration in MR/AR; Tools for collaboration in MR/AR; Collaboration awareness in MR/AR. The goal is to build a picture of current and prior research on collaboration in AR and MR, as well as set up a common research agenda for work going forward. This, in turn, can be used to grow the research community.},
  keywords={Collaboration;Augmented reality;Conferences;Australia;Security;Manuals},
  doi={10.1109/ISMARW.2015.26},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7328080,
  author={Fleck, Philipp and Arth, Clemens and Pirchheim, Christian and Schmalstieg, Dieter},
  booktitle={2015 IEEE International Symposium on Mixed and Augmented Reality}, 
  title={[POSTER] Tracking and Mapping with a Swarm of Heterogeneous Clients}, 
  year={2015},
  volume={},
  number={},
  pages={136-139},
  abstract={In this work, we propose a multi-user system for tracking and mapping, which accommodates mobile clients with different capabilities, mediated by a server capable of providing real-time structure from motion. Clients share their observations of the scene according to their individual capabilities. This can involve only keyframe tracking, but also mapping and map densification, if more computational resources are available. Our contribution is a system architecture that lets heterogeneous clients contribute to a collaborative mapping effort, without prescribing fixed capabilities for the client devices. We investigate the implications that the clients' capabilities have on the collaborative reconstruction effort and its use for AR applications.},
  keywords={Servers;Simultaneous localization and mapping;Three-dimensional displays;Collaboration;Cameras;Mobile handsets;Image reconstruction},
  doi={10.1109/ISMAR.2015.40},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{7328062,
  author={Yang, Peng and Kitahara, Itaru and Ohta, Yuichi},
  booktitle={2015 IEEE International Symposium on Mixed and Augmented Reality}, 
  title={[POSTER] Remote Mixed Reality System Supporting Interactions with Virtualized Objects}, 
  year={2015},
  volume={},
  number={},
  pages={64-67},
  abstract={Mixed Reality (MR) can merge real and virtual worlds seamlessly. This paper proposes a method to realize smooth collaboration using a remote MR, which makes it possible for geographically distributed users to share the same objects and communicate in real time as if they are at the same place. In this paper, we consider a situation where the users at local and remote sites perform a collaborative work, and real objects to be operated exist only at the local site. It is necessary to share the real objects between the two sites. However, prior studies have shown sharing real objects by duplication is either too costly or unrealistic. Therefore, we propose a method to share the objects by virtualizing the real objects using Computer Vision (CV) and then rendering the virtualized objects using MR. We have proposed a remote collaborative work system to create a smoother user experience for collaborative work with virtualized objects for remote users. Through experiments, we confirmed the effectiveness of our approach.},
  keywords={Collaboration;Virtual reality;Collaborative work;Rendering (computer graphics);Three-dimensional displays;Cameras;Real-time systems;Mixed reality;virtualized object;3D interaction;remote collaboration;RGB-D camera},
  doi={10.1109/ISMAR.2015.22},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{5643606,
  author={Wei, Dong and Zhou, Steven Zhiying and Xie, Du},
  booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, 
  title={MTMR: A conceptual interior design framework integrating Mixed Reality with the Multi-Touch tabletop interface}, 
  year={2010},
  volume={},
  number={},
  pages={279-280},
  abstract={This paper introduces a conceptual interior design framework - Multi-Touch Mixed Reality (MTMR), which integrates mixed reality with the multi-touch tabletop interface, to provide an intuitive and efficient interface for collaborative design and an augmented 3D view to users at the same time. Under this framework, multiple designers can carry out design work simultaneously on the top view displayed on the tabletop, while live video of the ongoing design work is captured and augmented by overlaying virtual 3D furniture models to their 2D virtual counterparts, and shown on a vertical screen in front of the tabletop. Meanwhile, the remote client's camera view of the physical room is augmented with the interior design layout in real time, that is, as the designers place, move, and modify the virtual furniture models on the tabletop, the client sees the corresponding life-size 3D virtual furniture models residing, moving, and changing in the physical room through the camera view on his/her screen. By adopting MTMR, which we argue may also apply to other kinds of collaborative work, the designers can expect a good working experience in terms of naturalness and intuitiveness, while the client can be involved in the design process and view the design result without moving around heavy furniture. By presenting MTMR, we hope to provide reliable and precise freehand interactions to mixed reality systems, with multi-touch inputs on tabletop interfaces.},
  keywords={Three dimensional displays;Virtual reality;Cameras;Solid modeling;Streaming media;Collaborative work;Layout;Mixed reality;multi-touch tabletop interfaces},
  doi={10.1109/ISMAR.2010.5643606},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{5336478,
  author={Ai, Zhuming and Livingston, Mark A.},
  booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, 
  title={Integration of georegistered information on a virtual globe}, 
  year={2009},
  volume={},
  number={},
  pages={169-170},
  abstract={In collaborative augmented reality (AR) missions, much georegistered information is collected and sent to a command and control center. This paper describes the concept and prototypical implementation of a mixed reality (MR) based system that integrates georegistered information from AR systems and other sources on a virtual globe. The application can be used for a command and control center to monitor the field operation where multiple AR users are engaging in a collaborative mission. Google Earth is used to demonstrate the system, which integrates georegistered icons, live video streams from field operators or surveillance cameras, 3D models, and satellite or aerial photos into one MR environment.},
  keywords={Virtual reality;Collaboration;Command and control systems;Augmented reality;Virtual prototyping;Earth;Streaming media;Surveillance;Cameras;Satellites;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, augmented, and virtual realities; H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces;Computer-supported cooperative work},
  doi={10.1109/ISMAR.2009.5336478},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{4538821,
  author={Sareika, Markus and Schmalstieg, Dieter},
  booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, 
  title={Urban Sketcher: Mixed Reality on Site for Urban Planning and Architecture}, 
  year={2007},
  volume={},
  number={},
  pages={27-30},
  abstract={Urban sketcher, a mixed reality application, is designed to encourage and improve communication on urban design among stakeholders. A mix of multimodal input devices enhances collaborative interaction in real-time, while visual feedback is given to all participants on a projected live video augmentation from urban sketcher. Sketching, modifying the scene on site, in the space of the video augmentation supports the exchange of information with interactive visual support. Urban Sketcher is instrumental for developing visions of future urban spaces by augmenting the real environment with sketches, facades, buildings, green spaces or skylines.},
  keywords={Virtual reality;Urban planning;Mixed reality;augmented reality;urban planning;architecture;natural multimodal interaction;collaboration},
  doi={10.1109/ISMAR.2007.4538821},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{5643588,
  author={Kamei, Ginga and Matsuyama, Takeshi and Okada, Ken-ichi},
  booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, 
  title={Augmentation of check in/out model for remote collaboration with Mixed Reality}, 
  year={2010},
  volume={},
  number={},
  pages={243-244},
  abstract={This paper proposes augmentation of check in/out model for remote collaboration with Mixed Reality (MR). We add a 3D shared and private space into the real workspace by MR technology, and augment the check in/out model to remote collaboration. By our proposal, user can intuitively receive remote partner's work via virtual objects in the shared space and stop sharing information about an object by just moving the object into the private space if the user doesn't want to share it. We implement a system which achieves our proposal and evaluate it.},
  keywords={Collaboration;Three dimensional displays;Space technology;Solid modeling;Virtual reality;Aerospace electronics;Brushes;Remote Collaboration;Mixed Reality;Check In/Out Model},
  doi={10.1109/ISMAR.2010.5643588},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{8951899,
  author={Rasmussen, Troels Ammitsbøl and Huang, Weidong},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={SceneCam: Improving Multi-camera Remote Collaboration using Augmented Reality}, 
  year={2019},
  volume={},
  number={},
  pages={28-33},
  abstract={Systems for remote collaboration on physical tasks generally use AR/VR technologies to create a shared visual space for collaborators to perform tasks together. The shared space often comes from a single camera view. Prior research has not reported on the benefits of using multiple cameras for remote collaboration. On the contrary, there seems to be some usability issues, which must be addressed, when designing remote collaboration systems that use multiple cameras to capture different areas and perspectives of a task space. To be usable, a multi-camera remote collaboration system must indicate to the local user which camera the remote user is looking at and vice versa, the system must make it fast and easy for the remote user to obtain the right camera view for a given collaborative task. We present SceneCam, an AR prototype with which we explore different techniques for improving the usability of multi-camera remote collaboration by making camera selection easier and faster. Specifically, SceneCam implements two camera selection techniques. The first technique nudges the remote user to manually select an optimal camera view of the local user's actions. The second technique automatically selects an optimal camera view of the local user and shows it to the remote user. Additionally, SceneCam implements two focus-in-context views (exocentric and egocentric views) that provide the remote user with a spatial overview of the local user's whereabouts in relation to the multiple task space areas and direct visual access to the camera views of said areas. Camera selection techniques (manual point-and-click, nudging, automatic), and focus-in-context views (no focus-in-context view, exocentric, egocentric) make up the two dimensions in a design space for multi-camera remote collaboration. We describe how SceneCam spans this design space. Lastly, as part of future work we discuss some hypotheses regarding the effects of the proposed camera selection techniques, focus-in-context views and combinations hereof on the usability of multi-camera remote collaboration.},
  keywords={Cameras;Task analysis;Collaboration;Two dimensional displays;Visualization;Usability;Three-dimensional displays;Remote collaboration;Augmented Reality;Multiple cameras;Usability},
  doi={10.1109/ISMAR-Adjunct.2019.00023},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{10316494,
  author={Ghamandi, Ryan K. and Hmaiti, Yahya and Nguyen, Tam T. and Ghasemaghaei, Amirpouya and Kattoju, Ravi Kiran and Taranta, Eugene M. and LaViola, Joseph J.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={What And How Together: A Taxonomy On 30 Years Of Collaborative Human-Centered XR Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={322-335},
  abstract={We present a taxonomy of human-centered collaborative XR tasks. XR technologies have extended into the realm of collaboration, improving the quality and accessibility of teamwork. However, after a comprehensive assessment of the literature on the interaction between XR technologies and collaboration, no comprehensive method that emphasizes task actions and properties exists to classify collaborative tasks. Thus, our suggested taxonomy represents a classification system for collaborative tasks. After conducting a thorough literature review across different research venues, we conducted several exhaustive classification and review cycles for over 800 papers collected, which resulted in 148 papers retained to create the taxonomy. We dissected the actions and properties that the collaborative endeavors and tasks of these papers encompass as well as the types of categorizations and relations these papers illustrate. We expand on the design choices and usage of our taxonomy, followed by its limitations and future work. We built this taxonomy in order to reduce ambiguities and confusion regarding the design and comprehension of human-based collaborative tasks that use XR technology, which could prove useful in aiding the development and understanding of these tasks. Our taxonomy reveals a framework for understanding how collaborative tasks are designed and a systematic way of classifying different methods by which people can collaborate and interact in environments that involve XR, while still promoting efficient communication, teamwork, goal achievement and productivity.},
  keywords={Productivity;Social computing;Systematics;Design methodology;Bibliographies;Taxonomy;Collaboration;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Collaborative and social computing design and evaluation methods;HCI design and evaluation methods},
  doi={10.1109/ISMAR59233.2023.00047},
  ISSN={2473-0726},
  month={Oct},}

@INPROCEEDINGS{8943760,
  author={Mahmood, Tahir and Fulmer, Willis and Mungoli, Neelesh and Huang, Jian and Lu, Aidong},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Improving Information Sharing and Collaborative Analysis for Remote GeoSpatial Visualization Using Mixed Reality}, 
  year={2019},
  volume={},
  number={},
  pages={236-247},
  abstract={Remote collaboration systems allow users at different sites to perform joint tasks, which are required by many real-life applications. For example, environmental pollution is a complex problem requiring many kinds of expertise to fully understand, as pollutants disperse not only locally but also regionally or even globally. This paper presents a remote collaborative visualization system through providing co-presence, information sharing, and collaborative analysis functions based on mixed reality techniques. We start with developing an immersive visualization approach for analyzing multi-attribute and geo-spatial data with intuitive multi-model interactions, simulating co-located collaboration effects. We then go beyond by designing a set of information sharing and collaborative analysis functions to support different users to share and analyze their sensemaking processes collaboratively. We provide example results and usage scenario to demonstrate that our system enables users to perform a variety of immersive and collaborative analytics tasks effectively. Through two small user studies focusing on evaluating our design of information sharing and system usability, the evaluation results confirm the effectiveness of comprehensive sharing among user, data, physical, and interaction spaces for improving remote collaborative analysis experience.},
  keywords={Collaboration;Data visualization;Task analysis;Visualization;Information management;Three-dimensional displays;Avatars;Remote collaboration;immersive analytics;geospatial visualization;mixed reality},
  doi={10.1109/ISMAR.2019.00021},
  ISSN={1554-7868},
  month={Oct},}

@INPROCEEDINGS{10322281,
  author={Tahmid, Ibrahim A. and Rodrigues, Francielly and Giovannelli, Alexander and Lisle, Lee and Thomas, Jerald and Bowman, Doug A.},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={CoLT: Enhancing Collaborative Literature Review Tasks with Synchronous and Asynchronous Awareness Across the Reality-Virtuality Continuum}, 
  year={2023},
  volume={},
  number={},
  pages={831-836},
  abstract={Collaboration plays a vital role in both academia and industry whenever we need to browse through a big amount of data to extract meaningful insights. These collaborations often involve people living far from each other, with different levels of access to technology. Effective cross-border collaborations require reliable telepresence systems that provide support for communication, cooperation, and understanding of contextual cues. In the context of collaborative academic writing, while immersive technologies offer novel ways to enhance collaboration and enable efficient information exchange in a shared workspace, traditional devices such as laptops still offer better readability for longer articles. We propose the design of a hybrid cross-reality cross-device networked system that allows the users to harness the advantages of both worlds. Our system allows users to import documents from their personal computers (PC) to an immersive headset, facilitating document sharing and simultaneous collaboration with both colocated colleagues and remote colleagues. Our system also enables a user to seamlessly transition between Virtual Reality, Augmented Reality, and the traditional PC environment, all within a shared workspace. We present the real-world scenario of a global academic team conducting a comprehensive literature review, demonstrating its potential for enhancing cross-reality hybrid collaboration and productivity.},
  keywords={Productivity;Technological innovation;Three-dimensional displays;Telepresence;Portable computers;Bibliographies;Collaboration;Human-centered computing;Virtual Reality;Augmented Reality;Reality-Virtuality Continuum;Synchronous Collaboration;Asynchronous Collaboration;Remote Collaboration;Colocated Collaboration},
  doi={10.1109/ISMAR-Adjunct60411.2023.00183},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{9974426,
  author={Mohan, Pallavi and Alam, Sameer and Nadirsha, T.N. Mohammed and Lilith, Nimrod and Svensson, Åsa},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={A Shared Interactive Space in Mixed Reality for Collaborative Digital Tower Operations}, 
  year={2022},
  volume={},
  number={},
  pages={615-621},
  abstract={Air transportation is undergoing a metamorphic transformation with digital towers (DTs) replacing physical air traffic control (ATC) towers by reproducing live camera feeds of out-of-tower (OOT) views using massive immersive spatial displays. With the recent pandemic-induced decline in air traffic, the quest to introduce cost-effective alternatives for DTs has gained importance. Recent developments in mixed reality (MR)-based visualizations allow air traffic systems to be better represented stereoscopically in an interactive virtual space. Moreover, MR offers customizable immersion, allowing air traffic control officers (ATCOs) to maintain their view of both environment and OOT view, as in a physical tower. Air traffic control, is, however, fundamentally a collaborative activity that requires clear and concise coordination between controllers and pilots to ensure an orderly flow of traffic. This paper presents a collaborative DT prototype system visualized in MR, that allows multiple controllers wearing MR headsets, such as Microsoft HoloLens 2, to view and interact with a shared visualization of the aerodrome, even if they are remotely located. We introduce collaborative routing capabilities to the system, whereby two controllers can collaboratively decide on taxiway routing for an arriving aircraft by interactively selecting and approving or rejecting routes in a shared MR space. We evaluate the prototype system through a preliminary assessment to investigate controllers' perceptions of situation awareness and workload while using the system. This paper presents initial results of our evaluation. Key findings include a faster Time-To-Decision and higher situation awareness when controllers collaborated by using the proposed system. The results demonstrate promising potential for MR-based collaboration for ATC operations, and for the proposed prototype to mature into a fully self-contained, interactive and portable air traffic control system.},
  keywords={Visualization;Poles and towers;Collaboration;Mixed reality;Prototypes;Virtual reality;Aerospace electronics;Human-centered computing;Ubiquitous and mobile computing systems and tools;Applied computing;Aerospace},
  doi={10.1109/ISMAR-Adjunct57072.2022.00128},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{10316498,
  author={Zaman, Faisal and Anslow, Craig and Chalmers, Andrew and Rhee, Taehyun},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={MRMAC: Mixed Reality Multi-user Asymmetric Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={591-600},
  abstract={We present MRMAC, a Mixed Reality Multi-user Asymmetric Collaboration system that allows remote users to teleport virtually into a real-world collaboration space to communicate and collaborate with local users. Our system enables telepresence for remote users by live-streaming the physical environment of local users using a 360° camera while blending 3D virtual assets into the mixed-reality collaboration space. Our novel client-server architecture enables asymmetric collaboration for multiple AR and VR users and incorporates avatars, view controls, as well as synchronized low-latency audio, video, and asset streaming. We evaluated our implementation with two baseline conditions: conventional 2D and standard 360° videoconferencing. Results show that MRMAC outperformed both baselines in inducing a sense of presence, improving task performance, usability, and overall user preference, demonstrating its potential for immersive multi-user telecollaboration.},
  keywords={Telepresence;Three-dimensional displays;Collaboration;Mixed reality;Teleportation;Video conferencing;User experience;Mixed Reality;Telecollaboration;Telepresence},
  doi={10.1109/ISMAR59233.2023.00074},
  ISSN={2473-0726},
  month={Oct},}

@INPROCEEDINGS{8613761,
  author={Lee, Gun A. and Teo, Theophilus and Kim, Seungwon and Billinghurst, Mark},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A User Study on MR Remote Collaboration Using Live 360 Video}, 
  year={2018},
  volume={},
  number={},
  pages={153-164},
  abstract={Sharing and watching live 360 panorama video is available on modern social networking platforms, yet the communication is often a passive one-directional experience. This research investigates how to further improve live 360 panorama based remote collaborative experiences by adding Mixed Reality (MR) cues. SharedSphere is a wearable MR remote collaboration system that enriches a live captured immersive panorama based collaboration through MR visualisation of non-verbal communication cues (e.g., view awareness and gestures cues). We describe the design and implementation details of the prototype system, and report on a user study investigating how MR live panorama sharing affects the user's collaborative experience. The results showed that providing view independence through sharing live panorama enhances co-presence in collaboration, and the MR cues help users understanding each other. Based on the study results we discuss design implications and future research direction.},
  keywords={Collaboration;Streaming media;Virtual reality;Visualization;Cameras;Three-dimensional displays;Two dimensional displays;Mixed Reality;Augmented Reality;remote collaboration;live panorama sharing;view independence},
  doi={10.1109/ISMAR.2018.00051},
  ISSN={1554-7868},
  month={Oct},}

@INPROCEEDINGS{9995367,
  author={Jing, Allison and Gupta, Kunal and McDade, Jeremy and Lee, Gun A. and Billinghurst, Mark},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Comparing Gaze-Supported Modalities with Empathic Mixed Reality Interfaces in Remote Collaboration}, 
  year={2022},
  volume={},
  number={},
  pages={837-846},
  abstract={In this paper, we share real-time collaborative gaze behaviours, hand pointing, gesturing, and heart rate visualisations between remote collaborators using a live 360 ° panoramic-video based Mixed Reality (MR) system. We first ran a pilot study to explore visual designs to combine communication cues with biofeedback (heart rate), aiming to understand user perceptions of empathic collaboration. We then conducted a formal study to investigate the effect of modality (Gaze+Hand, Hand-only) and interface (Near-Gaze, Embodied). The results show that the Gaze+Hand modality in a Near-Gaze interface is significantly better at reducing task load, improving co-presence, enhancing understanding and tightening collaborative behaviours compared to the conventional Embodied hand-only experience. Ranked as the most preferred condition, the Gaze+Hand in Near-Gaze condition is perceived to reduce the need for dividing attention to the collaborator’s physical location, although it feels slightly less natural compared to the embodied visualisations. In addition, the Gaze+Hand conditions also led to more joint attention and less hand pointing to align mutual understanding. Lastly, we provide a design guideline to summarize what we have learned from the studies on the representation between modality, interface, and biofeedback.},
  keywords={Heart rate;Visualization;Design methodology;Collaboration;Mixed reality;Real-time systems;Biological control systems;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR55827.2022.00102},
  ISSN={1554-7868},
  month={Oct},}

@INPROCEEDINGS{10316489,
  author={Garrido, Daniel and Jacob, João and Silva, Daniel Castro},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Performance Impact of Immersion and Collaboration in Visual Data Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={780-789},
  abstract={Immersive Analytics is a recent field of study that focuses on utilizing emerging extended reality technologies to bring visual data analysis from the 2D screen to the real/virtual world. The effectiveness of Immersive Analytics, when compared to traditional systems, has been widely studied in this field’s corpus, usually concluding that the immersive solution is superior. However, when it comes to comparing collaborative to single-user immersive analytics, the literature is lacking in user studies. As such, we developed a comprehensive experimental study with the objective of quantifying and analysing the impact that both immersion and collaboration have on the visual data analysis process. A two-variable (immersion: desktop/virtual reality; number of users: solo/pair) full factorial study was conceived with a mixed design (within-subject for immersion and between subject for number of users). Each of the 24 solo and 24 pairs of participants solved five visual data analysis tasks in both a head-mounted display-based virtual world and a desktop computer environment. The results show that, in terms of task time to completion, there were no significant differences between desktop and virtual reality, or between the solo and pair conditions. However, it was possible to conclude that collaboration is more beneficial the more complex the task is in both desktop and virtual reality, and that for less complex tasks, collaboration can be a hindrance. System Usability Scale scores were significantly better in the virtual reality condition than the desktop one, especially when working in pairs. As for user preference, the virtual reality system was significantly more favoured both as a visual data analysis platform and a collaborative data analysis platform over the desktop system. All supplemental materials are available at https://osf.io/k94u5/.},
  keywords={Visualization;Data analysis;Extended reality;Collaboration;Data visualization;Task analysis;Usability;Human-centered computing;Visualization;Empirical studies in visualization;Interaction paradigms;Virtual reality;Collaborative interaction},
  doi={10.1109/ISMAR59233.2023.00093},
  ISSN={2473-0726},
  month={Oct},}

@INPROCEEDINGS{10322151,
  author={Cho, Hyunwoo and Yuan, Bowen and Hart, Jonathon Derek and Chang, Zhuang and Cao, Jiashuo and Chang, Eunhee and Billinghurst, Mark},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Time Travellers: An Asynchronous Cross Reality Collaborative System}, 
  year={2023},
  volume={},
  number={},
  pages={848-853},
  abstract={This work presents a Mixed Reality (MR)-based asynchronous hybrid cross reality collaborative system which supports recording and playback of user actions in a large task space at different periods in time. Using this system, an expert can record a task process such as virtual object placement or assembly, which can then be viewed by other users in either Augmented Reality (AR) or Virtual Reality (VR) at later points in time to complete the task. In VR, the pre-scanned 3D workspace can be experienced to enhance the understanding of spatial information. Alternatively, AR can provide real-scale information to help the workers manipulate real-world objects, and complete the assignment. Users can seamlessly switch between AR and VR views as desired. In this way, the system can contribute to improving task performance and co-presence during asynchronous collaboration. The system is demonstrated in a use-case scenario of object assembly using parts that must be retrieved from a storehouse location. A pilot user study found that cross reality asynchronous collaboration system was helpful in providing information about work environments, inducing faster task completion with a lower task load. We provide lessons learned and suggestions for future research.},
  keywords={Training;Visualization;Three-dimensional displays;Annotations;Collaboration;Switches;Recording;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Mixed / augmented reality},
  doi={10.1109/ISMAR-Adjunct60411.2023.00186},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{9583841,
  author={Pereira, Nuno and Rowe, Anthony and Farb, Michael W and Liang, Ivan and Lu, Edward and Riebling, Eric},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={ARENA: The Augmented Reality Edge Networking Architecture}, 
  year={2021},
  volume={},
  number={},
  pages={479-488},
  abstract={Many have predicted the future of the Web to be the integration of Web content with the real-world through technologies such as Augmented Reality (AR). This has led to the rise of Extended Reality (XR) Web Browsers used to shorten the long AR application development and deployment cycle of native applications especially across different platforms. As XR Browsers mature, we face new challenges related to collaborative and multi-user applications that span users, devices, and machines. These collaborative XR applications require: (1) networking support for scaling to many users, (2) mechanisms for content access control and application isolation, and (3) the ability to host application logic near clients or data sources to reduce application latency. In this paper, we present the design and evaluation of the AR Edge Networking Architecture (ARENA) which is a platform that simplifies building and hosting collaborative XR applications on WebXR capable browsers. ARENA provides a number of critical components including: a hierarchical geospatial directory service that connects users to nearby servers and content, a token-based authentication system for controlling user access to content, and an application/service runtime supervisor that can dispatch programs across any network connected device. All of the content within ARENA exists as endpoints in a PubSub scene graph model that is synchronized across all users. We evaluate ARENA in terms of client performance as well as benchmark end-to-end response-time as load on the system scales. We show the ability to horizontally scale the system to Internet-scale with scenes containing hundreds of users and latencies on the order of tens of milliseconds. Finally, we highlight projects built using ARENA and showcase how our approach dramatically simplifies collaborative multi-user XR development compared to monolithic approaches.},
  keywords={Performance evaluation;Runtime;Collaboration;Computer architecture;Browsers;Geospatial analysis;Synchronization;Computer systems organization;Architectures;Distributed architectures;Computing methodologies;Computer graphics;Graphics systems and interfaces Mixed / augmented reality},
  doi={10.1109/ISMAR52148.2021.00065},
  ISSN={1554-7868},
  month={Oct},}

@INPROCEEDINGS{9974341,
  author={Niedermayr, Daniel and Wolfartsberger, Josef and Borac, Marijo and Brandl, Robert and Huber, Marcel and Josipovic, Petar},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Analyzing the Potential of Remote Collaboration in Industrial Mixed and Virtual Reality Environments}, 
  year={2022},
  volume={},
  number={},
  pages={66-73},
  abstract={Remote assistance tools in customer support allow for fast response times and reduced service expenses. Video and screen-based applications are widely used in various areas but lack spatial interactivity due to the limitation to a two-dimensional screen. Mixed reality-supported remote collaboration is frequently addressed in research as a way to extend the interaction space. The presented systems often require complicated device setups, calibration, or precomputation for reconstruction of the physical environment. This work investigates a novel approach for location-independent remote assistance and collaboration utilizing state-of-the-art mixed reality devices without any additional hardware modifications. In our approach a local user scans the environment using a HoloLens' standard depth camera. The data is transferred to a remote user equipped with a Virtual Reality headset, where the environment is reconstructed in real-time. The users can interact in the same shared environment. In a user study, the perceived workload was measured for both sides to examine the practical applicability of our approach. The results show that for local users the tasks set resulted in a low mental load, but also indicate that the coarse representation of the environment was mentally demanding for the remote users to work with. With our work we want to make remote collaboration easier and viable for a broader range of applications and facilitate interactions between globally distributed users.},
  keywords={Headphones;Collaboration;Mixed reality;Cameras;Real-time systems;Hardware;Time factors;mixed reality;virtual reality;industry 4.0;remote collaboration;remote assistance;I.4.5 [Image Processing and Computer Vision];Reconstruction;J.6 [Computer Applications];Computer-aided Engineering},
  doi={10.1109/ISMAR-Adjunct57072.2022.00023},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{10322228,
  author={Schröder, Jan-Henrik and Jetter, Hans-Christian},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Towards a Model for Space and Time in Transitional Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={223-227},
  abstract={Transitional collaboration is a unique form of cross-reality collaboration within transitional interfaces. Despite being at different locations of the reality-virtuality continuum, users can closely collaborate and overcome traditional boundaries of space and time. Despite the renewed interest in transitional interfaces as a specific type of hybrid user interfaces, this form of collaboration remains relatively under-explored, primarily since suitable analytical tools and frameworks have been introduced only recently and traditional models from the CSCW community may not fully capture the true nature of transitional collaboration. Therefore, in this position paper, we present the current state of our work on formulating a new model that integrates classic CSCW models with more recent research findings from cross-reality and mixed-reality research on transitional interfaces. We present examples based on existing systems and theoretical literature to demonstrate its practical application, including the visualization of different types of transitional collaboration.},
  keywords={Analytical models;Visualization;Social computing;Computational modeling;Collaboration;User interfaces;Augmented reality;Human-centered computing—Collaborative and social computing—Collaborative and social computing theory;concepts and paradigms—Computer supported cooperative work;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Collaborative interaction;Mixed / augmented reality;Virtual reality},
  doi={10.1109/ISMAR-Adjunct60411.2023.00052},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{10322165,
  author={Guarese, Renan and Polson, Deb and Zambetta, Fabio},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Immersive tele-guidance towards evoking empathy with people who are vision impaired}, 
  year={2023},
  volume={},
  number={},
  pages={809-814},
  abstract={We propose a novel approach to promote empathy with people that have disabilities through the development of an asymmetric collaborative experience. Our research aims to enable visually impaired (VI) users to relay their current surroundings to a remote sighted collaborator using virtual and augmented reality (VR/AR). The proposed system allows for the transmission of in-situ spatial information from the perspective of a VI user wearing an AR headset to a remote sighted guide wearing a VR headset, for the purpose of tele-guidance (TG). We aim to enhance the state empathy between VI and sighted users sharing the same environment by enabling them to exchange ideas and collaborate on tasks. Based on similar research in cross-reality collaboration, we expect the proposed system to allow remote guides to gain a better spatial understanding of the environment by immersing them in its digital twin. Additionally, VI users may find our system to be less time-consuming in their current environment when compared to current TG systems, which are solely based on video and voice chat.},
  keywords={Headphones;Collaboration;Digital twins;Task analysis;Relays;Augmented reality;virtual and augmented reality;tele guidance;assistive technology;digital empathy},
  doi={10.1109/ISMAR-Adjunct60411.2023.00179},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{9974443,
  author={Wang, Yi and Liu, Xiao and Cheng, Ben and Arora, Chetan and Hoang, Thuong},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={VR4HcRE: Virtual Reality Platform for Human-centric Requirements Elicitation}, 
  year={2022},
  volume={},
  number={},
  pages={788-793},
  abstract={Virtual reality (VR) is an important interactive technology for collaboration and communication. However, VR has not been widely used for eliciting human-centric requirements in software engineering contexts, which is an important gap in both the application of VR technology and the research of software engineering. To fill this gap, in this paper, we develop the VR4HcRE platform as a remote multiplayer collaboration and communication system to explore its benefits in human-centric requirement engineering. We consult with software developers and requirements engineers, as stakeholders of the system, to define the features of the system, including remote multiplayer collaboration, audio transcription, simulation of human and environmental factors, and UI design based on mobile phone prototype. We conduct a pilot experiment with 21 participants. Preliminary results indicate that using VR to capture user or special user needs has the potential value, including improving collaboration interaction and requirements expression, and bringing a different sense of participation compared with the physical world. As an ongoing work, we hope this paper generates a series of new research ideas in virtual reality and requirements elicitation.},
  keywords={Solid modeling;Communication systems;Collaboration;Prototypes;Software;Mobile handsets;Environmental factors;Virtual Reality;Human-centric Requirements Elicitation;Remote Multiplayer Collaboration;Human Factor;Software and Its Engineering-Software Creation and Management-Design Software-Requirements Analysis},
  doi={10.1109/ISMAR-Adjunct57072.2022.00168},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{6948412,
  author={Kim, Seungwon and Lee, Gun and Sakata, Nobuchika and Billinghurst, Mark},
  booktitle={2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Improving co-presence with augmented visual communication cues for sharing experience through video conference}, 
  year={2014},
  volume={},
  number={},
  pages={83-92},
  abstract={Video conferencing is becoming more widely used in areas other than face-to-face conversation, such as sharing real world experience with remote friends or family. In this paper we explore how adding augmented visual communication cues can improve the experience of sharing remote task space and collaborating together. We developed a prototype system that allows users to share live video view of their task space taken on a Head Mounted Display (HMD) or Handheld Display (HHD), and communicate through not only voice but also using augmented pointer or annotations drawn on the shared view. To explore the effect of having such an interface for remote collaboration, we conducted a user study comparing three video-conferencing conditions with different combination of communication cues: (1) voice only, (2) voice + pointer, and (3) voice + annotation. The participants used our remote collaboration system to share a parallel experience of puzzle solving in the user study, and we found that adding augmented visual cues significantly improved the sense of being together. The pointer was the most preferred additional cue by users for parallel experience, and there were different states of the users' behavior found in remote collaboration.},
  keywords={Prototypes;Collaboration;Visual communication;Streaming media;Cameras;Visualization;Tracking;Video Conferencing;Augmented Reality},
  doi={10.1109/ISMAR.2014.6948412},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{9974396,
  author={Seraji, Mohammad Rajabi and Stuerzlinger, Wolfgang},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={XVCollab: An Immersive Analytics Tool for Asymmetric Collaboration across the Virtuality Spectrum}, 
  year={2022},
  volume={},
  number={},
  pages={146-154},
  abstract={Research has shown that when a group of people collaborate in decision-making scenarios, they can be more effective than when they work alone. Studies also show that in a data analytics context, using immersive technologies could make users perform better in data understanding, pattern recognition, and finding connections. In this work, we are leveraging previous knowledge in Collaborative Immersive Analytics (CIA) and Cross-virtuality Analytics (XVA) to develop an asymmetric system that enables two groups from different places on the Virtuality-Reality spectrum to simultaneously work on analyzing data. We divide users into two groups: the nonimmersive desktop group and the immersive AR group. These two groups can both author and modify visualizations in their virtuality and share it with the other group when they see fit. For this, we designed a seamless interface for both groups to transform a visualization from non-immersive 2D to immersive AR and vice-versa. We also provide multiple awareness cues in the system that keep either group aware of the other and their actions. We designed these features to boost user performance and ease of use in a collaborative setting and incentivize them to rely on the other group for visualization tasks that are difficult to perform on their end of the virtuality spectrum. Our limited pilot study shows that users find the system engaging, easy to use, and helpful in their data-understanding journey within a collaborative context. Going forward, we plan to conduct more rigorous studies to verify our claims and explore other research questions on this topic.},
  keywords={Data analysis;Decision making;Collaboration;Data visualization;Transforms;Pattern recognition;Task analysis;Human-centered computing;Visualization;Visualization application domains;Visual analytics;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
  doi={10.1109/ISMAR-Adjunct57072.2022.00035},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{9585883,
  author={Healey, Jennifer and Wang, Duotun and Wigington, Curtis and Sun, Tong and Peng, Huaishu},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={A Mixed-Reality System to Promote Child Engagement in Remote Intergenerational Storytelling}, 
  year={2021},
  volume={},
  number={},
  pages={274-279},
  abstract={We present a mixed reality (MR) storytelling system designed specifically for multi-generational collaboration with child engagement as a key focus. Our "Let’s Make a Story" system comprises a two-sided experience that brings together a remote adult and child to tell a story collaboratively. The child has a mixed reality phone-based application with an augmented manipulative that controls the story’s main character. The remote adult participates through a web-based interface. The adult reads the story to the child and helps the child play the story game by providing them with items they need to clear the scenes.In this paper, we detail the implementation of our system and the results of a user study. Eight remote adult-child pairs experienced both the MR and a traditional paper-based storytelling system. To measure engagement, we used questionnaire analysis, engagement time with the story activity, and the word count of the child’s description of how the story should end. We found that children uniformly preferred the MR system, spent more time engaged with the MR system, and used more words to describe how the story should end incorporating details from the game.},
  keywords={Mixed reality;Collaboration;Virtual reality;Games;Time measurement;Augmented reality;Human-centered computing;Augmented Reality;Story Telling;Human-centered Interface;Human Factors;Family Communications},
  doi={10.1109/ISMAR-Adjunct54149.2021.00063},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9284659,
  author={He, Zhenyi and Du, Ruofei and Perlin, Ken},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={CollaboVR: A Reconfigurable Framework for Creative Collaboration in Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={542-554},
  abstract={Writing or sketching on whiteboards is an essential part of collaborative discussions in business meetings, reading groups, design sessions, and interviews. However, prior work in collaborative virtual reality (VR) systems has rarely explored the design space of multi-user layouts and interaction modes with virtual whiteboards. In this paper, we present CollaboVR, a reconfigurable framework for both co-located and geographically dispersed multi-user communication in VR. Our system unleashes users’ creativity by sharing freehand drawings, converting 2D sketches into 3D models, and generating procedural animations in real-time. To minimize the computational expense for VR clients, we leverage a cloud architecture in which the computational expensive application (Chalktalk) is hosted directly on the servers, with results being simultaneously streamed to clients. We have explored three custom layouts – integrated, mirrored, and projective – to reduce visual clutter, increase eye contact, or adapt different use cases. To evaluate CollaboVR, we conducted a within-subject user study with 12 participants. Our findings reveal that users appreciate the custom configurations and real-time interactions provided by CollaboVR. We have open sourced CollaboVR at https://github.com/snowymo/CollaboVR to facilitate future research and development of natural user interfaces and real-time collaborative systems in virtual and augmented reality.},
  keywords={Visualization;Layout;Collaboration;Writing;Real-time systems;Space exploration;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Collaborative and social computing;Collaborative and social computing systems and tools},
  doi={10.1109/ISMAR50242.2020.00082},
  ISSN={1554-7868},
  month={Nov},}

@INPROCEEDINGS{9974210,
  author={Olthof, Anne Marleen and Verlinden, Jouke and Allouch, Somava Ben},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Exploration of design methods and tools for virtual, augmented and mixed reality}, 
  year={2022},
  volume={},
  number={},
  pages={233-237},
  abstract={This paper shows an overview of design practices of the XR-lab at the Amsterdam University of Applied Sciences, The Netherlands. Over the course of six years, interdisciplinary teams of students have delivered 55+ prototypes in virtual, augmented, and mixed reality for a variety of 40+ clients. As human-computer interaction is entering a new evolutionary phase towards human-computer integration, new opportunities in extended reality (XR) have the potential to funda-mentally alter human characteristics and abilities. Therefore, this paper begins with taking a philosophical stance on ‘being human’ and the anthropological concept of ‘liminality’ in XR-experiences. A further exploration of the concept of 'emotional rehearsal spaces' uses know-how from performance art, dance, architecture, and dra-maturgy. Insights from tangible practices at the XR-lab show the cultural journey in XR-collaborations. This is made visible through a quick and dirty experiment on artistic thinking, design thinking, and system thinking, which shows how interdisciplinary collaborations are able to ignite new combinations of thought in design teams and individual professionals. Finally, we show an overview of specific design methods and tools that have been explored at the XR-lab over the years.},
  keywords={Art;Shape;Extended reality;Mixed reality;Prototypes;Computer architecture;Teamwork;Exploration;design methods;design tools;human enhancement;extended reality;mixed reality;augmented reality;virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Interaction design-Interaction design theory;concepts and paradigms},
  doi={10.1109/ISMAR-Adjunct57072.2022.00052},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{8115409,
  author={Barba, Evan and Marroquin, Ramon Zamora},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Primer on Spatial Scale and Its Application to Mixed Reality}, 
  year={2017},
  volume={},
  number={},
  pages={100-110},
  abstract={As mixed reality grows in popularity, the concepts and language we use to describe it will need to evolve as well. Having such concepts will allow for better interdisciplinary collaboration in both the arts and sciences, help to inform the creation of new software tools that enable the further evolution of the field, and will enable mixed reality research to advance scientific understanding in other disciplines. We provide an explication of the concept of spatial scale, including its relevant history in the fields of psychology and geography, and demonstrate its relevance to mixed reality. Through two case studies we show that spatial scale can operate effectively as a system of classification and analysis for mixed reality, and identify two concepts-scale transitions and the scale/complexity tradeoff-as critical to using this concept in future discussions of mixed reality.},
  keywords={Virtual reality;Psychology;Cognition;Geography;Collaboration;Media;mixed reality;augmented reality;virtual reality;spatial scale;multiscale analysis;scale transitions},
  doi={10.1109/ISMAR.2017.27},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9995144,
  author={Bonis, Michele De and Nguyen, Huyen and Bourdot, Patrick},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Literature Review of User Studies in Extended Reality Applications for Archaeology}, 
  year={2022},
  volume={},
  number={},
  pages={92-101},
  abstract={In the present study we conducted a systematic review on user studies for Archaeology in eXtended Reality of the last 10 years. After a screening and selection process, 52 articles were selected for an in-depth analysis. Their classification follows different axes: devices, location dependency, type of users, interaction and collaboration. We also organised the existing user studies according to tasks, evaluation measurements, number of participants, and how the study was conducted (pre-test and/or post-test, formative and summative evaluation, quantitative and qualitative data). We found an intertwined relation between Archaeology and Cultural Heritage, which is reflected in the vast presence of applications for museum exhibitions and tours on archaeological sites. Similarities between systems developed for archaeologists and for general public were also investigated. Our purpose was to find a common ground between different user studies that could help designers of the next systems have a base on which they can build their system. We also highlighted which would be the preferred and most suitable evaluation techniques, when they are needed, with the type of users to address. The results show a heterogeneity of measurable variables and possible choices, but some guidelines could be derived.},
  keywords={Archeology;Systematics;Extended reality;Bibliographies;Collaboration;Particle measurements;Museums;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality/ Mixed or augmented reality;Applied computing;Physical sciences and engineering;Archaeology},
  doi={10.1109/ISMAR55827.2022.00023},
  ISSN={1554-7868},
  month={Oct},}

@INPROCEEDINGS{9585858,
  author={Emerson, Leah and Lipinski, Riley and Shirey, Heather and Malloy, Theresa and Marrinan, Thomas},
  booktitle={2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Enabling Collaborative Interaction with 360° Panoramas between Large-scale Displays and Immersive Headsets}, 
  year={2021},
  volume={},
  number={},
  pages={183-188},
  abstract={Head mounted displays (HMDs) can provide users with an immersive virtual reality (VR) experience, but often are limited to viewing a single environment or data set at a time. In this paper, we describe a system of networked applications whereby co-located users in the real world can use a large-scale display wall to collaborate and share data with immersed users wearing HMDs. Our work focuses on the sharing of 360° surround-view panoramic images and contextual annotations. The large-scale display wall affords non-immersed users the ability to view a multitude of contextual information and the HMDs afford the ability for users to immerse themselves in a virtual scene. The asymmetric virtual reality collaboration between immersed and non-immersed individuals can lead to deeper under-standing and the feeling of a shared experience. We will highlight a series of use cases – two digital humanities projects that capture real locations using a 360° camera, and one scientific discovery project that uses computer generated 360° surround-view panoramas. In all cases, groups can benefit from both the immersive capabilities of HMDs and the collaborative affordances of large-scale display walls, and a unified experience is created for all users.},
  keywords={Social computing;Annotations;Stereo image processing;Collaboration;Virtual environments;Virtual reality;Immersive experience;Interviews;Standards;Videos;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Human-centered computing;Collaborative and social computing;Collaborative and social computing systems and tools},
  doi={10.1109/ISMAR-Adjunct54149.2021.00045},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9288475,
  author={Luchetti, Alessandro and Parolin, Edoardo and Butaslac, Isidro and Fujimoto, Yuichiro and Kanbara, Masayuki and Bosetti, Paolo and Cecco, Mariolino De and Kato, Hirokazu},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Stepping over Obstacles with Augmented Reality based on Visual Exproprioception}, 
  year={2020},
  volume={},
  number={},
  pages={96-101},
  abstract={The purpose of this study is to analyze the different kinds of ex- proprioceptive visual cues on an Augmented Reality (AR) system during gait exercise, on top of understanding which cues provide the best visualizations in stepping over obstacles. The main problem for users is to understand the position of virtual objects relative to themselves. Since visual exproprioception provides information about the body position in relation to the environment, it has the possibility to yield positive effects with regards to position control and gait biomechanics in the AR system. This research was born as part of the collaboration with the staff of Takanohara Central Hospital in Japan. Twenty-seven individuals were invited to take part in the user study to test three visual interfaces. The task of the mentioned user study involves making the subjects cross and avoid virtual obstacles of different heights that come from different directions. The AR application was implemented in the experiment by using the Head-Mounted Display (HMD) Microsoft HoloLens. Data obtained from the experiment revealed that the interface projected in front of the user from a third-person point of view resulted to improvements in terms of posture, visual stimuli, and safety.},
  keywords={Visualization;Three-dimensional displays;Avatars;Two dimensional displays;Resists;Task analysis;Augmented reality;Augmented Reality;Exproprioception;HoloLens;Rehabilitation;Obstacle crossing},
  doi={10.1109/ISMAR-Adjunct51615.2020.00039},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9288455,
  author={Cidota, Marina A. and Datcu, Dragoş},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Remote Assistance System in Augmented Reality for Early School Dropout Prevention}, 
  year={2020},
  volume={},
  number={},
  pages={321-325},
  abstract={The educational system suffers from early school dropout (i.e. graduation of the 8th grade at most), which is critical in Romania by its magnitude (16.4% in 2018). Unequal access to resources in education, the gap between rural and urban areas and the integration issue of the Roma population are challenges that result in deeper inequalities in society as a whole. Although many reforms have been applied and the budget for education kept increasing lately, Romania did not reach (or even get close to) any of the EU educational targets set for 2020 (i.e. 10% for early school dropout). The current situation of the education demands that more modern solutions should be discussed. In this paper, we propose an innovative technical system which facilitates remote learning experiences, especially for pupils with learning difficulties. We aim to explore the use of Augmented Reality (AR) to promote virtual co-location in education. That means, remote teachers can reach rather isolated pupils more easily and can smoothly engage in collaborative learning sessions. It is our belief that such an approach has the potential to reduce the risk of early school dropout.},
  keywords={Computers;Education;Urban areas;Tools;Writing;Pupils;Augmented reality;School dropout prevention;pupil-teacher remote collaboration;augmented reality;situational awareness},
  doi={10.1109/ISMAR-Adjunct51615.2020.00091},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{9995407,
  author={Hopkins, Torin and Weng, Suibi Che-Chuan and Vanukuru, Rishi and Wenzel, Emma and Banic, Amy and Gross, Mark D and Do, Ellen Yi-Luen},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Studying the Effects of Network Latency on Audio-Visual Perception During an AR Musical Task}, 
  year={2022},
  volume={},
  number={},
  pages={26-34},
  abstract={Augmented Reality (AR), with its ability to make people feel like they are in the same space as friends from across the world, is an ideal medium for the purpose of Networked Musical Collaboration. Most conventional systems that enable networked musical collaboration minimize network latency by focusing on the transfer of auditory information at the expense of visual feedback. Studies into human perception have shown that sensory integration of audio and visual stimuli can take place even when there is a slight delay between the two signals. We studied the way changes in network latency effect participants’ auditory and visual perception in latency detection, latency tolerance and attention focus; in this paper, we explore the trade-off between the presence of AR visuals and the minimization of latency. Twenty-four participants were asked to play a hand drum and collaborate with a prerecorded remote musician rendered as an avatar in AR. Multiple trials involving different levels of audio-visual latency were conducted. We then analyzed the subjective responses of the participants together with the recorded musical information from each session. Results indicate a minimum noticeable delay value–defined as the highest amount of delay that can be experienced before two stimulated senses are perceived as separate events–between 160 milliseconds (ms) and 320 ms. Players also reported that a delay between sound and an accompanying avatar animation became less tolerable at 320 ms of delay, but was never completely intolerable, even up to 1200 ms of delay. We conclude that players begin to notice delay at about 320 ms and most players can tolerate large delays between sound and animation.},
  keywords={Visualization;Avatars;Music;Collaboration;Streaming media;Animation;Real-time systems;Human-centered computing;Interaction paradigms;Mixed / augmented reality;Collaborative Interaction;Applied computing;Sound and music computing},
  doi={10.1109/ISMAR55827.2022.00016},
  ISSN={1554-7868},
  month={Oct},}

@INPROCEEDINGS{10316516,
  author={Yu, Kevin and Roth, Daniel and Strak, Robin and Pankratz, Frieder and Reichling, Julia and Kraetsch, Clemens and Weidert, Simon and Lazarovici, Marc and Navab, Nassir and Eck, Ulrich},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Mixed Reality 3D Teleconsultation for Emergency Decompressive Craniotomy: An Evaluation with Medical Residents}, 
  year={2023},
  volume={},
  number={},
  pages={662-671},
  abstract={Enabling collaborative telepresence in healthcare, especially surgical procedures, presents a critical challenge. The decompressive craniotomy procedure stands out as particularly complex and time-sensitive. The current teleconsultation approach relies on 2D color cameras, often offering only a fixed view and limited visual capabilities between experts and surgeons. However, teleconsultation can be addressed with Mixed Reality and immersive technology to potentially enable a better consultation of the procedure. We conducted an extensive user study focusing on decompressive craniotomy to investigate the advantages and challenges of our 3D teleconsultation system compared to a 2D video-based consultation system. Our 3D teleconsultation system leverages real-time 3D reconstruction of the patient and environment to empower experts to provide guidance and create virtual 3D annotations. The study utilized 3D-printed head models to perform a lifelike surgical intervention. It involved 14 medical residents and demonstrated an in-vitro 17% improvement in accurately describing the incision size on the patient’s head, contributing to potentially improved patient outcomes.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Telepresence;Annotations;Scalp;Mixed reality;3D Telepresence;Medical Consultation;Evaluation},
  doi={10.1109/ISMAR59233.2023.00081},
  ISSN={2473-0726},
  month={Oct},}

@INPROCEEDINGS{9974459,
  author={Eckhoff, Daniel and Ng, Royce and Cassinelli, Alvaro},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Virtual Reality Therapy for the Psychological Well-being of Palliative Care Patients in Hong Kong}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={In this paper we introduce novel Virtual Reality (VR) and Aug-mented Reality (AR) treatments to improve the psychological well being of patients in palliative care, based on interviews with a clin-ical psychologist who has successfully implemented VR assisted interventions on palliative care patients in the Hong Kong hospital system. Our VR and AR assisted interventions are adaptations of traditional palliative care therapies which simultaneously facilitate patients communication with family and friends while isolated in hospital due to physical weakness and COVID-19 related restrictions. The first system we propose is a networked, metaverse platform for palliative care patients to create customized virtual environments with therapists, family and friends which function as immersive and collaborative versions of ‘life review’ and ‘reminiscence therapy’. The second proposed system will investigate the use of Mixed Real-ity telepresence and haptic touch in an AR environment, which will allow palliative care patients to physically feel friends and family in a virtual space, adding to the sense of presence and immersion in that environment.},
  keywords={COVID-19;Telepresence;Hospitals;Metaverse;Bibliographies;Psychology;Collaboration;Applied computing-Health Informatics;Human-centered computing-Virtual reality},
  doi={10.1109/ISMAR-Adjunct57072.2022.00010},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{9974500,
  author={Ge, Saixi and Luo, Siyu and Yan, Shuo and Shen, Xukun},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Effects of Augmenting Real-Time Biofeedback in An Immersive VR Performance}, 
  year={2022},
  volume={},
  number={},
  pages={751-756},
  abstract={Currently, the most common form of Virtual Reality (VR) performance is that the performer wearing motion capture devices while the audience are immersed in a virtual environment remotely. Augmenting real-time biosignals provide the possibility of enhancing audience's perceptual experience in a collaborative virtual environment. In this research, we focus on exploring two different augmentation methods based on real-time biofeedback from the performer and the audience during a live VR performance. We propose a support system to investigate how performer's heart rate (HR) and audience's electroencephalogram (EEG) visualizations, and the corresponding auditory feedback, influence audience's sense of perception, immersion, and the narrative understanding. The results show that the performer's heart rate visual biofeedback significantly produced more concentration of the participants and the audience's EEG visual biofeedback significantly increased the participants' ability of narrative understanding.},
  keywords={Heart rate;Performance evaluation;Human computer interaction;Visualization;Virtual environments;Real-time systems;Electroencephalography;Audience perception;biosignals;visualization and sonification;VR performance;Human-centered computing-Human computer interaction (HCI) - Interaction paradigms-Virtual reality;Human-centered computing-Interaction design;Applied computing-Arts and humanities-Performing arts},
  doi={10.1109/ISMAR-Adjunct57072.2022.00159},
  ISSN={2771-1110},
  month={Oct},}

@INPROCEEDINGS{10316426,
  author={Vanukuru, Rishi and Weng, Suibi Che-Chuan and Ranjan, Krithik and Hopkins, Torin and Banic, Amy and Gross, Mark D. and Do, Ellen Yi-Luen},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={DualStream: Spatially Sharing Selves and Surroundings using Mobile Devices and Augmented Reality}, 
  year={2023},
  volume={},
  number={},
  pages={138-147},
  abstract={In-person human interaction relies on our spatial perception of each other and our surroundings. Current remote communication tools partially address each of these aspects. Video calls convey real user representations but without spatial interactions. Augmented and Virtual Reality (AR/VR) experiences are immersive and spatial but often use virtual environments and characters instead of real-life representations. Bridging these gaps, we introduce DualStream, a system for synchronous mobile AR remote communication that captures, streams, and displays spatial representations of users and their surroundings. DualStream supports transitions between user and environment representations with different levels of visuospatial fidelity, as well as the creation of persistent shared spaces using environment snapshots. We demonstrate how DualStream can enable spatial communication in real-world contexts, and support the creation of blended spaces for collaboration. A formative evaluation of DualStream revealed that users valued the ability to interact spatially and move between representations, and could see DualStream fitting into their own remote communication practices in the near future. Drawing from these findings, we discuss new opportunities for designing more widely accessible spatial communication tools, centered around the mobile phone.},
  keywords={Headphones;Fitting;Collaboration;Virtual environments;Cameras;Mobile handsets;Sensors;Human-centered computing;Mixed/augmented reality;Collaborative Interaction;Mobile computing},
  doi={10.1109/ISMAR59233.2023.00028},
  ISSN={2473-0726},
  month={Oct},}

@INPROCEEDINGS{5643559,
  author={Prytz, Erik and Nilsson, Susanna and Jönsson, Arne},
  booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, 
  title={The importance of eye-contact for collaboration in AR systems}, 
  year={2010},
  volume={},
  number={},
  pages={119-126},
  abstract={Eye contact is believed to be an important factor in normal human communication and as a result of this a head mounted display (HMD) is often seen as something intrusive and limiting. This can be especially problematic when AR is used in a collaborative setting. The study presented in this paper aims to investigate the effects an HMD-based AR system can have on eye contact behaviour between participants in a collaborative task and thus, in extension, the effects of the HMD on collaboration itself. The focus of the study is on task-oriented collaboration between professionals. The participants worked through three different scenarios alternating between HMDs and regular paper maps with the purpose of managing the crisis response to a simulated major forest fire. Correlations between eye contact between participants and questionnaire items concerning team- and taskwork were analysed, indicating that, for the paper map condition, a high amount of eye contact is associated with low confidence and trust in the artefacts used (i.e. paper map and symbols). The amount of eye-contact in both conditions was very low. It was significantly higher for conditions without HMDs. However, the confidence and trust in the artefacts was generally rated significantly higher with HMDs than without. In conclusion, the decrease in eye contact with HMDs does not seem to have a direct effect on the collaboration in a professional, task-oriented context. This is contrary to popular assumptions and the results are relevant for future design choices for AR systems using HMDs.},
  keywords={Correlation;Cameras;Command and control systems;Biological system modeling;Teamwork;Collaborative work},
  doi={10.1109/ISMAR.2010.5643559},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{1115077,
  author={Fiorentino, M. and de Amicis, R. and Monno, G. and Stork, A.},
  booktitle={Proceedings. International Symposium on Mixed and Augmented Reality}, 
  title={Spacedesign: a mixed reality workspace for aesthetic industrial design}, 
  year={2002},
  volume={},
  number={},
  pages={86-318},
  abstract={Spacedesign is an innovative mixed reality (MR) application addressed to aesthetic design of free form curves and surfaces. It is a unique and comprehensive approach which uses task-specific configurations to support the design workflow from concept to mock-up evaluation and review. The first-phase conceptual design benefits from a workbench-like 3-D display for free hand sketching, surfacing and engineering visualization. Semitransparent stereo glasses augment the pre-production physical prototype by additional shapes, textures and annotations. Both workspaces share a common interface and allow collaboration and cooperation between different experts, who can configure the system for the specific task. A faster design workflow and CAD data consistency can be thus naturally achieved. Tests and collaborations with designers, mainly from automotive industry, are providing systematic feedback for this ongoing research. As far as the authors are concerned, there is no known similar approach that integrates the creation and editing phase of 3D curves and surfaces in virtual and augmented reality (VR/AR). Herein we see the major contribution of our new application.},
  keywords={Virtual reality;Aerospace industry;Collaborative work;Three dimensional displays;Design engineering;Data visualization;Glass;Prototypes;Shape;Design automation},
  doi={10.1109/ISMAR.2002.1115077},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{5336522,
  author={Nilsson, Susanna and Johansson, Bjorn and Jonsson, Arne},
  booktitle={2009 8th IEEE International Symposium on Mixed and Augmented Reality}, 
  title={Using AR to support cross-organisational collaboration in dynamic tasks}, 
  year={2009},
  volume={},
  number={},
  pages={3-12},
  abstract={This paper presents a study where Augmented Reality (AR) technology has been used as a tool for supporting collaboration between the rescue services, the police and military personnel in a crisis management scenario. There are few studies on how AR systems should be designed to improve cooperation between actors from different organizations while at the same time support individual needs. In the present study an AR system was utilized for supporting joint planning tasks by providing organisation-specific views of a shared working. The study involved a simulated emergency event conducted in close to real settings with representatives from the organisations for which the system is developed. As a baseline, a series of trials without the AR system was carried out. Results show that the users were positive towards the AR system, and would like to use it in real work. They also experience some performance benefits of using the AR system compared to their traditional tools. Finally, the problem of designing for collaborative work as well as the benefits of using an iterative design processes is discussed.},
  keywords={Collaboration;Collaborative work;Collaborative tools;Crisis management;Command and control systems;Augmented reality;Personnel;Terminology;Paper technology;Information science},
  doi={10.1109/ISMAR.2009.5336522},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{1240695,
  author={MacWilliams, A. and Sandor, C. and Wagner, M. and Bauer, M. and Klinker, G. and Bruegge, B.},
  booktitle={The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.}, 
  title={Herding sheep: live system for distributed augmented reality}, 
  year={2003},
  volume={},
  number={},
  pages={123-132},
  abstract={In the past, architectures of augmented reality systems have been widely different and tailored to specific tasks. In this paper, we use the example of the SHEEP game to show how the structural flexibility of DWARF, our component-based distributed wearable augmented reality framework, facilitates a rapid prototyping and online development process for building, debugging and altering a complex, distributed, highly interactive AR system. The SHEEP system was designed to test and demonstrate the potential of tangible user interfaces which dynamically visualize, manipulate and control complex operations of many inter-dependent processes. SHEEP allows the users more freedom of action and forms of interaction and collaboration, following the tool metaphor that bundles software with hardware in units that are easily understandable to the user. We describe how we developed SHEEP, showing the combined evolution of framework and application, as well as the progress from rapid prototype to final demonstration system. The dynamic aspects of DWARF facilitated testing and allowed us to rapidly evaluate new technologies. SHEEP has been shown successfully at various occasions. We describe our experiences with these demos.},
  keywords={Augmented reality;Prototypes;Collaborative software;Buildings;Debugging;System testing;User interfaces;Visualization;Control systems;Collaborative tools},
  doi={10.1109/ISMAR.2003.1240695},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{1383050,
  author={Benko, H. and Ishak, E.W. and Feiner, S.},
  booktitle={Third IEEE and ACM International Symposium on Mixed and Augmented Reality}, 
  title={Collaborative mixed reality visualization of an archaeological excavation}, 
  year={2004},
  volume={},
  number={},
  pages={132-140},
  abstract={We present VITA (visual interaction tool for archaeology), an experimental collaborative mixed reality system for offsite visualization of an archaeological dig. Our system allows multiple users to visualize the dig site in a mixed reality environment in which tracked, see-through, head-worn displays are combined with a multi-user, multi-touch, projected table surface, a large screen display, and tracked hand-held displays. We focus on augmenting existing archaeological analysis methods with new ways to organize, visualize, and combine the standard 2D information available from an excavation (drawings, pictures, and notes) with textured, laser range-scanned 3D models of objects and the site itself. Users can combine speech, touch, and 3D hand gestures to interact multimodally with the environment. Preliminary user tests were conducted with archaeology researchers and students, and their feedback is presented here.},
  keywords={Collaboration;Virtual reality;Visualization;Large screen displays;Laser feedback;Collaborative tools;Information analysis;Laser modes;Speech;Testing},
  doi={10.1109/ISMAR.2004.23},
  ISSN={},
  month={Nov},}

@INPROCEEDINGS{5643564,
  author={Gruber, Lukas and Gauglitz, Steffen and Ventura, Jonathan and Zollmann, Stefanie and Huber, Manuel and Schlegel, Michael and Klinker, Gudrun and Schmalstieg, Dieter and Höllerer, Tobias},
  booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, 
  title={The City of Sights: Design, construction, and measurement of an Augmented Reality stage set}, 
  year={2010},
  volume={},
  number={},
  pages={157-163},
  abstract={We describe the design and implementation of a physical and virtual model of an imaginary urban scene-the “City of Sights”- that can serve as a backdrop or “stage” for a variety of Augmented Reality (AR) research. We argue that the AR research community would benefit from such a standard model dataset which can be used for evaluation of such AR topics as tracking systems, modeling, spatial AR, rendering tests, collaborative AR and user interface design. By openly sharing the digital blueprints and assembly instructions for our models, we allow the proposed set to be physically replicable by anyone and permit customization and experimental changes to the stage design which enable comprehensive exploration of algorithms and methods. Furthermore we provide an accompanying rich dataset consisting of video sequences under varying conditions with ground truth camera pose. We employed three different ground truth acquisition methods to support a broad range of use cases. The goal of our design is to enable and improve the replicability and evaluation of future augmented reality research.},
  keywords={Solid modeling;Cameras;Three dimensional displays;Calibration;Subspace constraints;Accuracy;Computational modeling},
  doi={10.1109/ISMAR.2010.5643564},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{7328055,
  author={Oskiper, Taragay and Sizintsev, Mikhail and Branzoi, Vlad and Samarasekera, Supun and Kumar, Rakesh},
  booktitle={2015 IEEE International Symposium on Mixed and Augmented Reality}, 
  title={Augmented Reality Scout: Joint Unaided-Eye and Telescopic-Zoom System for Immersive Team Training}, 
  year={2015},
  volume={},
  number={},
  pages={25-30},
  abstract={In this paper we present a dual, wide area, collaborative augmented reality (AR) system that consists of standard live view augmentation, e.g., from helmet, and zoomed-in view augmentation, e.g., from binoculars. The proposed advanced scouting capability allows long range high precision augmentation of live unaided and zoomed-in imagery with aerial and terrain based synthetic objects, vehicles, people and effects. The inserted objects must appear stable in the display and not jitter or drift as the user moves around and examines the scene. The AR insertions for the binocs must work instantly when they are picked up anywhere as the user moves around. The design of both AR modules is based on using two different cameras with wide and narrow field of view (FoV) lenses. The wide FoV gives context and enables the recovery of location and orientation of the prop in 6 degrees of freedom (DoF) much more robustly, whereas the narrow FoV is used for the actual augmentation and increased precision in tracking. Furthermore, narrow camera in unaided eye and wide camera on the binoculars are jointly used for global yaw (heading) correction. We present our navigation algorithms using monocular cameras in combination with IMU and GPS in an Extended Kalman Filter (EKF) framework to obtain robust and real-time pose estimation for precise augmentation and cooperative tracking.},
  keywords={Cameras;Databases;Global Positioning System;Visualization;Robustness;Three-dimensional displays;Buildings;visual-inertial navigation;monocular wide and narrow field of view camera;GPS;sensor fusion;EKF},
  doi={10.1109/ISMAR.2015.11},
  ISSN={},
  month={Sep.},}

@INPROCEEDINGS{10108671,
  author={Numan, Nels},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={[DC] Towards Understanding, Alleviating, and Exploiting the Effects of Asymmetry in Collaborative Mixed Reality}, 
  year={2023},
  volume={},
  number={},
  pages={991-992},
  abstract={Mixed reality (MR) has the potential to transform the way we communicate and collaborate. However, there is a lack of knowledge about the collaborative use of different MR devices, such as collaboration between local augmented reality (AR) and remote virtual reality (VR) users. This position paper outlines research focused on deepening our understanding of the impact of asymmetry on collaboration and to use this knowledge to mitigate its negative effects and leverage its positive potential, leading to improved collaboration in MR environments. The findings of this research are expected to be valuable for the design of systems that support a diverse range of collaborative scenarios.},
  keywords={Social computing;Three-dimensional displays;Conferences;Collaboration;Mixed reality;Transforms;User interfaces;Human-centered computing—Collaborative and social computing systems and tools},
  doi={10.1109/VRW58643.2023.00338},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9419334,
  author={Kim, Hyejin and Young, Jacob and Medeiros, Daniel and Thompson, Stephen and Rhee, Taehyun},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={TeleGate: Immersive Multi-User Collaboration for Mixed Reality 360°Video}, 
  year={2021},
  volume={},
  number={},
  pages={532-533},
  abstract={When collaborating on virtual content within 360° mixed reality environments it is often desirable for collaborators to fully immerse themselves within the task space, usually by means of a head-mounted display. However, these socially isolate any co-located collaborators, removing the ability to communicate through important gestural, facial, and body language cues. We present TeleGate, a system that instead utilises a shared immersive display to allow collaboration within remote environments between an arbitrary number of users, keeping collaborators visible while allowing immersive and interactive collaboration within remote environments.},
  keywords={Teleconferencing;Three-dimensional displays;Conferences;Collaboration;Mixed reality;Lighting;Virtual reality;Human-centered computing;Mixed / augmented reality;Collaborative interaction},
  doi={10.1109/VRW52623.2021.00148},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10536247,
  author={Kang, Seoyoung},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Investigating Avatar Facial Expressions and Collaboration Dynamics for Social Presence in Avatar-Mediated XR Remote Communication}, 
  year={2024},
  volume={},
  number={},
  pages={1114-1115},
  abstract={In avatar-mediated remote communication, reflecting one's facial expressions in avatars remains challenging but essential in facilitating effective communication and fostering social presence. From our prior studies, we underscored the significance of emotion-based facial expressions in avatar-mediated remote communications within eXtended Reality (XR). Our findings indicated that during emotional conversations as well as informative speeches, activating the emotion-based blendshapes resulted in a level of social presence and communication quality comparable to activating all facial blend-shapes. As we move forward, our research aims to investigate avatar facial expression considering XR contexts, with an emphasis on the challenges stemming from XR device constraints and the dynamic contexts of remote collaboration. We aspire to devise adaptive guidelines for avatar facial expression, leveraging emotional cues using XR devices. Additionally, we plan to systematically analyze the implications of remote collaboration's varying scenarios, formulating guidelines for adaptive avatar representation that account for different collaborative environments and scenarios. To enhance social presence and communication experiences among users, we are preparing for a series of user studies, aiming to validate the effectiveness of our approaches and prototype systems.},
  keywords={Technological innovation;Social computing;Three-dimensional displays;Avatars;Refining;Collaboration;Psychology;Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality;Human-centered computing—Collaborative and social computing—Empirical studies in collaborative and social computing},
  doi={10.1109/VRW62533.2024.00351},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108758,
  author={Marques, Bernardo and Santos, Andre and Martins, Nuno and Silva, Samuel and Dias, Paulo and Santos, Beatriz Sousa},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Exploring Situated Instructions for Mixed Reality (MR) Remote Collaboration: Comparing Embedded and Non-Embedded Annotations}, 
  year={2023},
  volume={},
  number={},
  pages={589-590},
  abstract={Remote collaboration using Mixed Reality (MR) enable off-site experts to assist on-site collaborators needing guidance. Different types of visualizations have been proposed for sharing situated information, e.g., embedded, non-embedded, others. However, the effectiveness of these visualizations have not been compared. This work describes a user study with 16 participants, aimed at comparing two conditions: C1 - Embedded and C2 - Non-Embedded annotations during a real-life scenario of remote maintenance. Two distinct devices were used: Hand-Held Device (HHD) and a Head-Mounted Display (HMD). This last using Embedded annotations was considered the better alternative, while HHD using Non-Embedded annotations were rated lower from all conditions analyzed.},
  keywords={Visualization;Three-dimensional displays;Head-mounted displays;Annotations;Mixed reality;Collaboration;Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction Paradigms-Mixed/augmented reality;},
  doi={10.1109/VRW58643.2023.00137},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8798128,
  author={Teo, Theophilus and Lee, Gun A and Billinghurst, Mark and Adcock, Matt},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Supporting Visual Annotation Cues in a Live 360 Panorama-based Mixed Reality Remote Collaboration}, 
  year={2019},
  volume={},
  number={},
  pages={1187-1188},
  abstract={We propose enhancing live 360 panorama-based Mixed Reality (MR) remote collaboration through supporting visual annotation cues. Prior work on live 360 panorama-based collaboration used MR visualization to overlay visual cues, such as view frames and virtual hands, yet they were not registered onto the shared physical workspace, hence had limitations in accuracy for pointing or marking objects. Our prototype system uses spatial mapping and tracking feature of an Augmented Reality head-mounted display to show visual annotation cues accurately registered onto the physical environment. We describe the design and implementation details of our prototype system, and discuss on how such feature could help improve MR remote collaboration.},
  keywords={Visualization;Collaboration;Resists;Three-dimensional displays;Prototypes;Augmented reality;Mixed Reality;remote collaboration;360 panorama;annotation;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Collaborative computing;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798128},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{9757671,
  author={Zaman, Faisal},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={[DC] Improving Multi-User Interaction for Mixed Reality Telecollaboration}, 
  year={2022},
  volume={},
  number={},
  pages={940-941},
  abstract={Mixed reality (MR) approaches offer merging of real and virtual worlds to create new environments and visualizations for real-time interaction. Existing MR systems, however, do not utilise user real environment, lack detail in dynamic environments, and often lack multi-user capabilities. This research focuses on exploring multiuser aspects of immersive collaboration, where an arbitrary number of co-located and remotely located users can collaborate in a single or merged collaborative MR space. The aim is to enable users to experience VR/AR together, irrespective of the type of HMD, and facilitate users with their collaborative tasks. The main goal is to develop an immersive collaboration platform in which users can utilize the space around them and at the same time collaborate and switch between different perspectives of other co-located and remote users.},
  keywords={Visualization;Three-dimensional displays;Conferences;Collaboration;Mixed reality;Virtual reality;Switches;Human-centered computing;Human computer interaction (HCI);Interaction paradigms-Mixed/augmented reality},
  doi={10.1109/VRW55335.2022.00321},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10536279,
  author={Wang, Cheng Yao and Saffo, David and Moriarty, Bill and Maclntyre, Blair},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={CollabXR: Bridging Realities in Collaborative Workspaces with Dynamic Plugin and Collaborative Tools Integration}, 
  year={2024},
  volume={},
  number={},
  pages={454-457},
  abstract={Hybrid meetings, often constrained by the limitations of videocon-ferencing, typically lack immersive and cohesive collaborative experiences, leaving participants feeling disconnected. Extended Reality (XR) technology counters this by enabling immersive spaces for both remote and co-located participants, thereby enhancing social presence, non-verbal communication, and educational experiences. As Mixed Reality (MR) platforms become more accessible, there is a shift towards cross-reality, platform-agnostic collaboration. However, this shift introduces unique communication challenges due to interaction asymmetries across different modalities. Addressing these issues, this paper presents CollabXR, an open-source frame-work developed using Babylon.js, WebXR, and Colyseus, specifically designed for the research community. CollabXR features a unique runtime plugin system for custom code and data integration and is seamlessly integrated with widely-used collaborative and communication platforms. It serves as a research platform for developing novel interaction techniques to facilitate cross-reality collaboration, overcoming communication asymmetries in scenarios where complete symmetry is unachievable.},
  keywords={Training;Runtime;Codes;Extended reality;Collaboration;Mixed reality;Virtual environments;Human-centered computing-Collaborative interaction-;-Human-centered computing-Virtual reality-},
  doi={10.1109/VRW62533.2024.00089},
  ISSN={},
  month={March},}

@INPROCEEDINGS{8798024,
  author={Wang, Peng and Zhang, Shusheng and Bai, Xiaoliang and Billinghurst, Mark and He, Weiping and Wang, Shuxia and Zhang, Xiaokun and Du, Jiaxiang and Chen, Yongxing},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Head Pointer or Eye Gaze: Which Helps More in MR Remote Collaboration?}, 
  year={2019},
  volume={},
  number={},
  pages={1219-1220},
  abstract={This paper investigates how two different unique gaze visualizations (the head pointer(HP), eye gaze(EG)) affect table-size physical tasks in Mixed Reality (MR) remote collaboration. We developed a remote collaborative MR Platform which supports sharing of the remote expert's HP and EG. The prototype was evaluated with a user study comparing two conditions: sharing HP and EG with respect to their effectiveness in the performance and quality of cooperation. There was a statistically significant difference between two conditions on the performance time, and HP is a good proxy for EG in remote collaboration.},
  keywords={Collaboration;Task analysis;Virtual reality;Prototypes;Cameras;Visualization;Human factors;Remote collaboration;Augmented Reality;Mixed Reality;Eye gaze;Head Pointer;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces—Computer-supported cooperative work},
  doi={10.1109/VR.2019.8798024},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{4480753,
  author={Yamamoto, Shun and Tamaki, Hidekazu and Okajima, Yuta and Okada, Kenichi and Bannai, Yuichi},
  booktitle={2008 IEEE Virtual Reality Conference}, 
  title={Symmetric Model of Remote Collaborative MR Using Tangible Replicas}, 
  year={2008},
  volume={},
  number={},
  pages={71-74},
  abstract={Research into collaborative mixed reality (MR) or augmented reality has recently been active. Previous studies showed that MR was preferred for collocated collaboration while immersive virtual reality was preferred for remote collaboration. The main reason for this preference is that the physical object in remote space cannot be handled directly. However, MR using tangible objects is still attractive for remote collaborative systems, because MR enables seamless interaction with real objects enhanced by virtual information with the sense of touch. Here we introduce "tangible replicas"(dual objects that have the same shape, size, and surface), and propose a symmetrical model for remote collaborative MR. The result of experiments shows that pointing and drawing functions on the tangible replica work well despite limited shared information.},
  keywords={Collaboration;Virtual reality;Collaborative work;Feedback;Virtual environment;Haptic interfaces;Biological system modeling;Environmental factors;Augmented reality;Displays;Mixed Reality;remote collaboration;collaborative interaction;usability},
  doi={10.1109/VR.2008.4480753},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{8446508,
  author={Huang, Wen},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Evaluating the Effectiveness of Head-Mounted Display Virtual Reality (HMD VR) Environment on Students' Learning for a Virtual Collaborative Engineering Assembly Task}, 
  year={2018},
  volume={},
  number={},
  pages={827-829},
  abstract={The emerging VR social networks (e.g., Facebook Spaces, Rec Room) provide opportunities for engineering faculties to design collaborative virtual engineering tasks in their classroom instruction with HMD VR system. However, we do not how this capacity will affect students' learning and their professional skills (e.g., communication and collaboration). The proposed study is expected to fill this research gap and will use a mixed-methods design to explore students' performance and learning outcomes in a virtual collaborative automotive assembly task. The quantitative data will be collected from the pre-and-post task survey and the task itself. This data will be used to analyze the differences among experiment and control groups. Students' responses to the open questions in the post-task survey will serve as triangulation and provide deep insight for the quantitative results. The study is expected to not only contribute to the research field but also benefit different stakeholders in the engineering education systems.},
  keywords={Task analysis;Collaboration;Resists;Virtual environments;Collaborative work;Three-dimensional displays;head-mounted display;virtual reality;engineering education;collaborative learning},
  doi={10.1109/VR.2018.8446508},
  ISSN={},
  month={March},}

@INPROCEEDINGS{5759458,
  author={Roulland, Frederic and Castellani, Stefania and Valobra, Pascal and Ciriza, Victor and O'Neill, Jacki and Deng, Ye},
  booktitle={2011 IEEE Virtual Reality Conference}, 
  title={Mixed reality for supporting office devices troubleshooting}, 
  year={2011},
  volume={},
  number={},
  pages={175-178},
  abstract={In this paper we describe the Mixed Reality system that we are developing to facilitate a real-world application, that of collaborative remote troubleshooting of broken office devices. The architecture of the system is centered on a 3D virtual representation of the device augmented with status data of the actual device coming from its internal sensors. The purpose of this paper is to illustrate how this approach supports the interactions required by the remote collaborative troubleshooting activity whilst taking into account technical constraints that come from a real world application. We believe it constitutes an interesting opportunity for using Mixed Reality in this domain.},
  keywords={Three dimensional displays;Collaboration;Solid modeling;Semantics;Collaborative work;Performance evaluation;Maintenance engineering;device troubleshooting;Mixed Reality;3D modeling;collaborative systems},
  doi={10.1109/VR.2011.5759458},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{5444777,
  author={Back, Maribeth and Childs, Timothy and Dunnigan, Anthony and Foote, Jonathan and Gattepally, Sagar and Liew, Bee and Shingu, Jun and Vaughan, Jim},
  booktitle={2010 IEEE Virtual Reality Conference (VR)}, 
  title={The virtual factory: Exploring 3D worlds as industrial collaboration and control environments}, 
  year={2010},
  volume={},
  number={},
  pages={257-258},
  abstract={This project investigates practical uses of virtual, mobile, and mixed reality systems in industrial settings, in particular control and collaboration applications for factories. In collaboration with TCHO, a chocolate maker start-up in San Francisco, we have built virtual mirror-world representations of a real-world chocolate factory and are importing its data and modeling its processes. The system integrates mobile devices such as cell phones and tablet computers. The resulting "virtual factory" is a cross-reality environment designed for simulation, visualization, and collaboration, using a set of interlinked, real-time 3D and 2D layers of information about the factory and its processes.},
  keywords={Virtual manufacturing;Industrial control;Collaboration;Production facilities;Electrical equipment industry;Virtual reality;Control systems;Application software;Mobile computing;Cellular phones;3D applications;virtual worlds;x-reality;mixed reality;remote collaboration;remote monitoring;remote control;data visualization;collaborative tools;cross-reality;sensors;pervasive computing;simulation;mobile computing},
  doi={10.1109/VR.2010.5444777},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{5759480,
  author={Martin, Pierre and Bourdot, Patrick},
  booktitle={2011 IEEE Virtual Reality Conference}, 
  title={Designing a reconfigurable multimodal and collaborative supervisor for Virtual Environment}, 
  year={2011},
  volume={},
  number={},
  pages={225-226},
  abstract={Virtual Reality (VR) systems cannot be promoted for complex applications (involving the interpretation of massive and intricate databases) without creating natural and ”transparent” user interfaces: intuitive interfaces are required to bring non-expert users to use VR technologies. Many studies have been carried out on multimodal and collaborative systems in VR. Although these two aspects are usually studied separately, they share interesting similarities. Our work focuses on the way to manage multimodal and collaborative interactions in a same process. We present here the similarities between these two processes and the main features of a reconfigurable multimodal and collaborative supervisor for Virtual Environments (VEs). The aim of such a system is to ensure the merge of pieces of information coming from VR devices (tracking, gestures, speech, haptics, etc.), to control immersive multi-user applications using the main communication and sensorimotor channels of humans. The framework's architecture of this supervisor wants to be generic, modular and reconfigurable (via an XML configuration file), in order to be applied to many different contexts.},
  keywords={Collaboration;Virtual environment;Multimedia communication;User interfaces;Context;Electronic mail},
  doi={10.1109/VR.2011.5759480},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{8798166,
  author={Hamilton, Rob},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Collaborative and Competitive Futures for Virtual Reality Music and Sound}, 
  year={2019},
  volume={},
  number={},
  pages={1510-1512},
  abstract={The histories of virtual reality systems draw heavily from foundational work in telepresence and robotics, cinema and gaming. Across each of these disciplines the roles of “player” and “audience” member vary significantly, as do the rules, affordances and experiential goals put forth by the systems themselves. As such, designers and developers of interactive virtual (here defined as including mixed and augmented) reality systems are faced with a fundamental choice: to create experiences that are inherently collaborative or competitive. While on the surface such a choice might seem a simple articulation of the core design principles for any one given project, the directions in which virtual reality systems are guided and the impacts these choices will have on societal acceptance of VR as a principal component of our technological futures should not be ignored. This paper discusses models of collaboration and competition as put forth within a series of interactive virtual experiences and proposes an ideal future for virtual reality in which interactivity and telepresence leverage collaboration as a core mechanic.},
  keywords={Music;Virtual reality;Games;Collaboration;Solid modeling;Instruments;Art;J.5 [Computer Applications]: Arts and Humanities—fine arts;H.5.5 [Sound and Music Computing];H.5.1 [Information Interfaces and presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities},
  doi={10.1109/VR.2019.8798166},
  ISSN={2642-5254},
  month={March},}

@INPROCEEDINGS{8446184,
  author={Leber, Isabel and Merk, Matthias and Tullius, Gabriela and Hertkorn, Peter},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Using Pico Projectors with Spatial Contextual Awareness to Create Augmented Knowledge Spaces for Interdisciplinary Engineering Teams}, 
  year={2018},
  volume={},
  number={},
  pages={617-618},
  abstract={Engineers of the research project “Digital Product Life-Cycle” are using a graph-based design language to model all aspects of the product they are working on. This abstract model is the base for all further investigations, developments and implementations. In particular at early stages of development, collaborative decision making is very important. We propose a semantic augmented knowledge space by means of mixed reality technology, to support engineering teams. Therefore we present an interaction prototype consisting of a pico projector and a camera. In our usage scenario engineers are augmenting different artefacts in a virtual working environment. The concept of our prototype contains both an interaction and a technical concept. To realise implicit and natural interactions, we conducted two prototype tests: (1) A test with a low-fidelity prototype and (2) a test by using the method Wizard of Oz. As a result, we present a prototype with interaction selection using augmentation spotlighting and an interaction zoom as a semantic zoom.},
  keywords={Prototypes;Knowledge engineering;Collaboration;Semantics;Cameras;Testing;Creativity;Human-centered computing―Ubiquitous computing―;Human-centered computing―Mixed/augmented reality},
  doi={10.1109/VR.2018.8446184},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9757433,
  author={Dey, Arindam and Cao, Yufei and Dobbins, Chelsea},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Effects of Heart Rate Feedback on an Asymmetric Platform using Augmented Reality and Laptop}, 
  year={2022},
  volume={},
  number={},
  pages={209-216},
  abstract={Asymmetric systems enable remote collaboration using multiple different devices. In this paper, we have designed an asymmetric system that enables collaboration between Augmented Reality (AR) and Laptop users. Remote collaboration systems often lack mutual emotional understanding between the collaborators. Researchers in virtual reality have used physiological data sharing as a way to increase social awareness between collaborators. No such effort was made in AR and in asymmetric domains. A key contribution of this work is that we have enabled physiological (heart rate) feedback sharing between the collaborators to generate more awareness about one another in an asymmetric system. We executed a user study with 20 participants where they performed a collaborative and a competitive task. Our results indicated that providing heart rate feedback is beneficial for AR devices. We noticed the experimental tasks had an effect on galvanic skin response although no such effect was found on heart rate signal.},
  keywords={Heart rate;Training;Portable computers;Three-dimensional displays;Conferences;Collaboration;User interfaces;Human-centered computing—Visualization—Visualization techniques—Treemaps;Human-centered computing—Visualization—Visualization design and evaluation methods},
  doi={10.1109/VRW55335.2022.00051},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108747,
  author={Yu, Mengmeng and Bi, Chongke and Han, Dong},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={CRVideo: Cross-reality 360° Video Social Systems Exploration}, 
  year={2023},
  volume={},
  number={},
  pages={319-323},
  abstract={Cross-reality systems provide visual representations at different levels of virtuality/physicality and offer smooth transitions between them to users. These systems also support multiple modes of interaction, and coordinate collaboration between users with multiple realities. In this paper, we explore a cross-reality 360° video social system, aiming to enable users with different personalities to get a social experience consistent with their psychological changes and behavioral habits when watching 360° video. We designed the prototype system CRVideo, conducted a preliminary user survey and a user study to research the social requirements of users and verify the effectiveness of our cross-reality system. The results show that the social habits of extroverted and introverted users in video platforms differ greatly, and the system across realistic levels can meet the psychological change curve of users in different periods.},
  keywords={Visualization;Three-dimensional displays;Conferences;Psychology;Prototypes;Collaboration;Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)—HCI design and evaluation methods-User studies},
  doi={10.1109/VRW58643.2023.00073},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9419284,
  author={Dhanjan, Jaspreet Singh and Steed, Anthony},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Revisiting the Scene-Graph-as-Bus Concept: Inter-networking Heterogeneous Applications Using glTF Fragments}, 
  year={2021},
  volume={},
  number={},
  pages={342-346},
  abstract={While there are now many examples of successful collaborative mixed reality applications, each application uses its own custom networking framework and applications rarely inter-operate. To enable much larger-scale distributed systems, we will need inter-networking protocols that allow heterogeneous applications to exchange data. We demonstrate a proof of concept implementation that revisits the concept of using a scene-graph as a bus. That is, sharing low-level geometry and rendering information, rather than high-level semantic events. Our networking protocol uses glTF fragments and edits to express scene changes. We use the proof of concept to explore the potential to inter-network very different applications that are based on different underlying graphics engine technology.},
  keywords={Geometry;Protocols;Three-dimensional displays;Conferences;Semantics;Mixed reality;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Networks;Network protocols;Application layer protocols},
  doi={10.1109/VRW52623.2021.00068},
  ISSN={},
  month={March},}

@INPROCEEDINGS{4811051,
  author={Bott, Jared N. and Crowley, James G. and LaViola, Joseph J.},
  booktitle={2009 IEEE Virtual Reality Conference}, 
  title={One Man Band: A 3D Gestural Interface for Collaborative Music Creation}, 
  year={2009},
  volume={},
  number={},
  pages={273-274},
  abstract={In recent years the popularity of music and rhythm-based games has experienced tremendous growth. However, almost all of these games require custom hardware to be used as input devices, and these devices control only one or two similar instruments. In this paper we describe One Man Band, a prototype video game for musical expression that uses novel 3D spatial interaction techniques using accelerometer-based motion controllers. One Man Band provides users with 3D gestural interfaces to control both the timing and sound of the music played, with both single and collaborative player modes. We further investigate the ability to detect different musical gestures without explicit selection of mode, giving the user the ability to seamlessly transition between instrument types with a single input device.},
  keywords={Collaboration;Instruments;Games;Prototypes;Rhythm;Accelerometers;Toy industry;Interactive systems;Hardware;Motion control;3D gestural interfaces;video games;music;Wii Remote;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Auditory (non-speech) feedback;K.8.0 [Personal Computing]: General-Games;I.5.5 [Pattern Recognition]: Implementation-Interactive Systems},
  doi={10.1109/VR.2009.4811051},
  ISSN={2375-5334},
  month={March},}

@INPROCEEDINGS{8446561,
  author={Karakottas, Antonis and Papachristou, Alexandros and Doumanoqlou, Alexandros and Zioulis, Nikolaos and Zarpalas, Dimitrios and Daras, Petros},
  booktitle={2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Augmented VR}, 
  year={2018},
  volume={},
  number={},
  pages={1-1},
  abstract={Traditional VR is mostly about headset experiences either in completely virtual environments or 360° videos. On the other hand AR has been mixing realities by inserting the virtual within the real. In this work we present the Augmented VR concept that lies at the middle right of the virtuality continuum, typically referred to as augmented virtuality. We offer another perspective by blending the real within the virtual focusing on capturing actual human performances in three dimensions and emplacing them within virtual environments [1]-[3]. By compressing and transmitting this new type of 3D media we can also achieve real-time interaction, communication and collaboration between users. Being in full 3D our media are compatible with a variety of applications be it either VR, AR, MR and open up new exciting opportunities like free viewpoint spectating while also increasing the feeling of immersion of all participating users. We demonstrate our technology via a prototype two player game that can support spectating in various devices like head mounted displays (VR) or tablet laptops (AR). Our system is easy to setup, requiring minimal non-technical human intervention, and relatively low cost taking one step ahead in making this technology available to the consumer public.},
  keywords={Three-dimensional displays;Real-time systems;Media;Virtual environments;Videos;Human-centered computing - Human computer interaction (HCI) - Interaction paradigms - Virtual reality;Human-centered computing - Human computer interaction (HCI) - Interaction paradigms - Mixed/augmented reality;Computing methodologies - Computer graphics - Graphics systems and interfaces - Mixed/augmented reality;Computing methodologies - Computer vision - Image and video acquisition - Camera calibration;Computing methodologies-Computer vision - Computer vision representations - Appearance and texture representations;Computing methodologies - Computer vision - Computer vision problems - Reconstruction;Multi-user Virtual Reality;Teleimmersion},
  doi={10.1109/VR.2018.8446561},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9757653,
  author={Borhani, Zahra},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={[DC] Annotation in Asynchronous Collaborative Immersive Analytic Environments using Augmented Reality}, 
  year={2022},
  volume={},
  number={},
  pages={950-951},
  abstract={Immersive Analytics (IA) and Augmented Reality (AR) head-mounted displays provide a different paradigm for people to analyze multidimensional data and externalize their thoughts by utilizing the stereoscopic nature of headsets. However, using annotation in IA-AR is challenging and not well-understood. In addition, IA collaborative environments add another complexity level for users operating on complex visualized datasets. Current AR systems focus mainly on synchronized collaboration, while asynchronous collaboration has remained unexplored. This project investigates annotation in IA for asynchronous collaborative environments. We present our research studies on virtual annotation types and introduce a new filtering annotation technique for IA.},
  keywords={Headphones;Three-dimensional displays;Annotations;Conferences;Stereo image processing;Collaboration;Data visualization;Annotation;Immersive Analytics;Collaborative Augmented Reality;Asynchronous Collaboration},
  doi={10.1109/VRW55335.2022.00326},
  ISSN={},
  month={March},}

@INPROCEEDINGS{9090418,
  author={Li, Yuan},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Spatial Referencing for Anywhere, Anytime Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={555-556},
  abstract={Augmented Reality (AR) systems can effectively enhance collaboration in the real world by providing useful spatial cues through virtual overlays. In particular, AR can be used to facilitate referencing a location in space as required in many collaborative activities. However, the goal of supporting spatial referencing anywhere at any time is hindered by issues with current AR technology, such as the lack of a precise environment model, unreliable tracking, and difficulty in synchronizing among different devices. This paper proposes a research agenda that innovates in the domain of interaction techniques for spatial referencing for everywhere AR.},
  keywords={Biological system modeling;Three-dimensional displays;Collaboration;Augmented reality;Visualization;Reliability;Computational modeling;Human-centered computing;Mixed / augmented reality;Human-centered computing;Collaborative interaction;Human-centered computing;Interaction techniques},
  doi={10.1109/VRW50115.2020.00129},
  ISSN={},
  month={March},}

@INPROCEEDINGS{7467242,
  author={Wu, Fuli and Ding, Weilong},
  booktitle={2015 International Conference on Virtual Reality and Visualization (ICVRV)}, 
  title={Research of Collaborative Interactive for Medical Imaging Based on Roles Change}, 
  year={2015},
  volume={},
  number={},
  pages={232-237},
  abstract={It is important to share the large amount of medical imaging data in different areas for a diagnosis and treatment collaboratively. Traditional medical visualization systems lack cross-platform access and collaborative mechanisms. This paper proposes a remote collaborative interactive systems for medical imaging based on users' role change. All master users can achieve collaborative interaction with one medical imaging, the spectators can receive the same visualization scene in real-time, and the spectators can change their roles into a master user. The experimental results show that: all remote users can share the medical image data, and they can get real-time collaborative interaction of slice rendering and volume rendering.},
  keywords={Data visualization;Collaboration;Biomedical imaging;Rendering (computer graphics);Browsers;Protocols;Servers;Medical imaging;Collaborative interaction;Role change},
  doi={10.1109/ICVRV.2015.60},
  ISSN={},
  month={Oct},}

@INPROCEEDINGS{9090677,
  author={Pan, Ye and Mitchell, Kenny},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={PoseMMR: A Collaborative Mixed Reality Authoring Tool for Character Animation}, 
  year={2020},
  volume={},
  number={},
  pages={758-759},
  abstract={Augmented reality devices enable new approaches for character animation, e.g., given that character posing is three dimensional in nature it follows that interfaces with higher degrees-of-freedom (DoF) should outperform 2D interfaces. We present PoseMMR, allowing Multiple users to animate characters in a Mixed Reality environment, like how a stop-motion animator would manipulate a physical puppet, frame-by-frame, to create the scene. We explore the potential advantages of the PoseMMR can facilitate immersive posing, animation editing, version control and collaboration, and provide a set of guidelines to foster the development of immersive technologies as tools for collaborative authoring of character animation.},
  keywords={Animation;Three-dimensional displays;Collaboration;Virtual reality;Tools;Two dimensional displays;Kinematics},
  doi={10.1109/VRW50115.2020.00230},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10169760,
  author={Haigang, Jiang and Rui, Ling and Linfeng, Tang},
  booktitle={2023 9th International Conference on Virtual Reality (ICVR)}, 
  title={Use of Mixed Reality in HVAC System Equipment Fault Detection and Diagnosis Method}, 
  year={2023},
  volume={},
  number={},
  pages={256-260},
  abstract={In the traditional building HVAC system equipment operation and maintenance process, some of the HVAC system equipment installation locations are hidden in the building space, so many HVAC equipment repair and maintenance operations have limited maintenance space and low visualization, and the building air conditioning system fault diagnosis process is often plagued by on-site work collaboration factors resulting in low fault repair efficiency. Based on the above problems, this paper proposes to superimpose Mixed Reality (MR) technology on top of the Building Information Model (BIM) to develop a BIM+MR-based building HVAC system equipment fault diagnosis system to improve the immersive and remote visualization interaction capability in the HVAC system equipment maintenance process through the digital twin technology. The BIM+MR fault diagnosis system is developed to improve the immersive and remote visualization interaction capability in the HVAC system equipment maintenance process using digital twin technology. The technical verification shows that the efficiency of HVAC’s air conditioning equipment fault diagnosis in the pilot project has increased by 1.88 times after applying MR technology.},
  keywords={Fault diagnosis;Air conditioning;Visualization;Solid modeling;HVAC;Buildings;Mixed reality;mixed reality;building information modeling;fault detection diagnosis;heating ventilation and air conditioning (HAVC);data-driven approach},
  doi={10.1109/ICVR57957.2023.10169760},
  ISSN={2331-9569},
  month={May},}

@INPROCEEDINGS{9090540,
  author={Weissker, Tim and Tornow, Philipp and Froehlich, Bernd},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Tracking Multiple Collocated HTC Vive Setups in a Common Coordinate System}, 
  year={2020},
  volume={},
  number={},
  pages={592-593},
  abstract={Multiple collocated HTC Vive setups sharing the same base stations and room calibration files still track their devices in different coordinate systems. We present a procedure for mapping the tracking data of multiple users to a common coordinate system and show that it enables spatially consistent interactions of collocated collaborators.},
  keywords={Target tracking;Conferences;Data visualization;Virtual environments;Collaboration;Base stations;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Collaborative interaction},
  doi={10.1109/VRW50115.2020.00147},
  ISSN={},
  month={March},}

@INPROCEEDINGS{10108717,
  author={Chang, Eunhee and Lee, Yongjae and Yoo, Byounghyun and Im, Hojeong},
  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Comparing Context-sharing Interfaces in XR Remote Collaboration}, 
  year={2023},
  volume={},
  number={},
  pages={719-720},
  abstract={eXtended Reality (XR) remote collaboration refers to working to- gether on virtual data or real objects through various types of reality. To achieve a higher immersive collaboration experience, it is critical to converge heterogeneous realities in one shared workspace, where environmental changes are continuously updated. In this study, We extend our previous work, the Webized XR system, in terms of sharing the local environmental context. This system can provide three types of context-sharing interface (2D video, 360 video, and 3D reconstruction), delivering spatial information about the workspace. We also performed a pilot user study to measure the quality of this system.},
  keywords={Solid modeling;Three-dimensional displays;Extended reality;Conferences;Collaboration;Immersive experience;User interfaces;Human-centered computing-Virtual Reality;Human-centered computing-Mixed augmented reality;Human- centered computing-User studies},
  doi={10.1109/VRW58643.2023.00202},
  ISSN={},
  month={March},}PROCEEDINGS{8797966,
  author={Norman, Mitchell and Lee, Gun and Smith, Ross T. and Billinqhurs, Mark},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Mixed Presence Collaborative Mixed Reality System}, 
  year={2019},
  volume={},
  number={},
  pages={1106-1107},
  abstract={Research has shown that Mixed Presence Groupware (MPG) systems are a valuable collaboration tool. However research into MPG systems is limited to a handful of tabletop and Virtual Reality (VR) systems with no exploration of Head-Mounted Display (HMD) based Augmented Reality (AR) solutions. We present a new system with two local users and one remote user using HMD based AR interfaces. Our system provides tools allowing users to layout a room with the help of a remote user. The remote user has access to a marker and pointer tools to assist in directing the local users. Feedback collected from several groups of users showed that our system is easy to learn but could have increased accuracy and consistency.},
  keywords={Collaboration;Resists;Prototypes;Tools;Augmented reality;Webcams;Augmented Reality;remote collaboration;mixed presence;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces;Collaborative computing;H.l.2 [Models and Principles]: User/Machine Systems;Human Factors},
  doi={10.1109/VR.2019.8797966},
  ISSN={2642-5254},
  month={March},}
