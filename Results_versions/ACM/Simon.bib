@inproceedings{10.1145/3613904.3642814,
	author = {Irlitti, Andrew and Latifoglu, Mesut and Hoang, Thuong and Syiem, Brandon Victor and Vetere, Frank},
	title = {Volumetric Hybrid Workspaces: Interactions with Objects in Remote and Co-located Telepresence},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642814},
	doi = {10.1145/3613904.3642814},
	abstract = {Volumetric telepresence aims to create a shared space, allowing people in local and remote settings to collaborate seamlessly. Prior telepresence examples typically have asymmetrical designs, with volumetric capture in one location and objects in one format. In this paper, we present a volumetric telepresence mixed reality system that supports real-time, symmetrical, multi-user, partially distributed interactions, using objects in multiple formats, across multiple locations. We align two volumetric environments around a common spatial feature to create a shared workspace for remote and co-located people using objects in three formats: physical, virtual, and volumetric. We conducted a study with 18 participants over 6 sessions, evaluating how telepresence workspaces support spatial coordination and hybrid communication for co-located and remote users undertaking collaborative tasks. Our findings demonstrate the successful integration of remote spaces, effective use of proxemics and deixis to support negotiation, and strategies to manage interactivity in hybrid workspaces.},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	articleno = {802},
	numpages = {16},
	keywords = {augmented reality, collaboration, mixed reality, partially distributed teams, telepresence, volumetric capture, workspace awareness},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '24}
}


@inproceedings{10.1145/3613904.3642502,
	author = {Wong, Emily and S\'{a}nchez Esquivel, Juan and Leiva, Germ\'{a}n and Gr\o{}nb\ae{}k, Jens Emil Sloth and Velloso, Eduardo},
	title = {Practice-informed Patterns for Organising Large Groups in Distributed Mixed Reality Collaboration},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642502},
	doi = {10.1145/3613904.3642502},
	abstract = {Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication. Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned. Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration. To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration. These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session. The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships. As a result, this work creates a blueprint for scaling collaboration across distributed spaces.},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	articleno = {1030},
	numpages = {18},
	keywords = {collaboration, f-formations, mixed reality, scale, space and place},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '24}
}


@inproceedings{10.1145/3025453.3025923,
	author = {Berry, Andrew B. L. and Lim, Catherine and Hartzler, Andrea L. and Hirsch, Tad and Wagner, Edward H. and Ludman, Evette and Ralston, James D.},
	title = {How Values Shape Collaboration Between Patients with Multiple Chronic Conditions and Spousal Caregivers},
	year = {2017},
	isbn = {9781450346559},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3025453.3025923},
	doi = {10.1145/3025453.3025923},
	abstract = {Individuals with multiple chronic conditions (MCC) collaborate with spousal caregivers daily to pursue what is most important to their health and well-being. Previous research in human-computer interaction has supported individuals with chronic conditions or their caregivers, but little has supported both as a unit. We conducted a field study with 12 patient-caregiver dyads, all married and living together, to identify partners' values and how they shape collaborative management of MCC. Partners' coinciding values motivated them to empathize with and support each other in the face of challenges related to health and well-being. When their values were asymmetric, they perceived tensions between individual autonomy and their ability to coordinate with their partner. Systems to support partners in this context could help them overcome asymmetric values, but should balance this with support for individual autonomy.},
	booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
	pages = {5257–5270},
	numpages = {14},
	keywords = {caregiver, collaboration, coordination, multiple chronic conditions, patient, self-care, self-management},
	location = {Denver, Colorado, USA},
	series = {CHI '17}
}


@inproceedings{10.1145/1518701.1518784,
	author = {Leshed, Gilly and Perez, Diego and Hancock, Jeffrey T. and Cosley, Dan and Birnholtz, Jeremy and Lee, Soyoung and McLeod, Poppy L. and Gay, Geri},
	title = {Visualizing real-time language-based feedback on teamwork behavior in computer-mediated groups},
	year = {2009},
	isbn = {9781605582467},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1518701.1518784},
	doi = {10.1145/1518701.1518784},
	abstract = {While most collaboration technologies are concerned with supporting particular tasks such as workflows or meetings, many work groups do not have the teamwork skills essential to effective collaboration. One way to improve teamwork is to provide dynamic feedback generated by automated analyses of behavior, such as language use. Such feedback can lead members to reflect on and subsequently improve their collaborative behavior, but might also distract from the task at hand. We have experimented with GroupMeter - a chat-based system that presents visual feedback on team members' language use. Feedback on proportion of agreement words and overall word count was presented using two different designs. When receiving feedback, teams in our study expressed more agreement in their conversations and reported greater focus on language use as compared to when not receiving feedback. This suggests that automated, real-time linguistic feedback can elicit behavioral changes, offering opportunities for future research.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {537–546},
	numpages = {10},
	keywords = {cmc, cscw, feedback visualization, linguistic analysis, peripheral displays, teamwork},
	location = {Boston, MA, USA},
	series = {CHI '09}
}


@inproceedings{10.1145/3173574.3173647,
	author = {Homaeian, Leila and Goyal, Nippun and Wallace, James R. and Scott, Stacey D.},
	title = {Group vs Individual: Impact of TOUCH and TILT Cross-Device Interactions on Mixed-Focus Collaboration},
	year = {2018},
	isbn = {9781450356206},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3173574.3173647},
	doi = {10.1145/3173574.3173647},
	abstract = {Cross-device environments (XDEs) have been developed to support a multitude of collaborative activities. Yet, little is known about how different cross-device interaction techniques impact group collaboration, including how their impact on independent and joint work that often occurs during group work. In this work, we explore the impact of two XDE data browsing techniques: TOUCH and TILT. Through a mixed-methods study of a collaborative sensemaking task, we show that TOUCH and TILT have distinct impacts on how groups accomplish, and shift between, independent and joint work. Finally, we reflect on these findings and how they can more generally inform the design of XDEs.},
	booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	pages = {1–13},
	numpages = {13},
	keywords = {cross-device, mixed-focus collaboration, tilt, touch},
	location = {<conf-loc>, <city>Montreal QC</city>, <country>Canada</country>, </conf-loc>},
	series = {CHI '18}
}


@inproceedings{10.1145/3290605.3300431,
	author = {Teo, Theophilus and Lawrence, Louise and Lee, Gun A. and Billinghurst, Mark and Adcock, Matt},
	title = {Mixed Reality Remote Collaboration Combining 360 Video and 3D Reconstruction},
	year = {2019},
	isbn = {9781450359702},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3290605.3300431},
	doi = {10.1145/3290605.3300431},
	abstract = {Remote Collaboration using Virtual Reality (VR) and Augmented Reality (AR) has recently become a popular way for people from different places to work together. Local workers can collaborate with remote helpers by sharing 360-degree live video or 3D virtual reconstruction of their surroundings. However, each of these techniques has benefits and drawbacks. In this paper we explore mixing 360 video and 3D reconstruction together for remote collaboration, by preserving benefits of both systems while reducing drawbacks of each. We developed a hybrid prototype and conducted user study to compare benefits and problems of using 360 or 3D alone to clarify the needs for mixing the two, and also to evaluate the prototype system. We found participants performed significantly better on collaborative search tasks in 360 and felt higher social presence, yet 3D also showed potential to complement. Participant feedback collected after trying our hybrid system provided directions for improvement.},
	booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
	pages = {1–14},
	numpages = {14},
	keywords = {360 panorama, 3d scene reconstruction, interaction methods, mixed reality, remote collaboration, virtual reality},
	location = {Glasgow, Scotland Uk},
	series = {CHI '19}
}


@inproceedings{10.1145/1357054.1357234,
	author = {M\"{u}ller-Tomfelde, Christian and Schremmer, Claudia},
	title = {Touchers and mousers: commonalities and differences in co-located collaboration with multiple input devices},
	year = {2008},
	isbn = {9781605580111},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1357054.1357234},
	doi = {10.1145/1357054.1357234},
	abstract = {We present new findings on commonalities and differences between touch and mouse input for co-located interaction between teams of two people who know each other. Twenty-two participants were instructed to work as co-located pairs on three sets of two concurrent digital jigsaw puzzles, displayed on a horizontal tabletop that allows for multiple concurrent input devices. They were advised to use their preference for, or any combination of, direct (touch) and indirect (mouse) input device to achieve the goal. We increased the task?s difficulty: In the second and third puzzle task, participants had to discover that pieces were mixed up between the two puzzle stacks. We used this 'hidden task' to trigger spontaneous transitions from individual to collaborative work. Based on a qualitative analysis of individual interaction trajectories of direct and indirect input devices, we discuss patterns of collaboration. This furthers scientific understanding of co-located collaboration with multiple input devices.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {1149–1152},
	numpages = {4},
	keywords = {co-located collaboration, direct and indirect input devices, human factors, multiple input devices, tabletop, trajectories},
	location = {Florence, Italy},
	series = {CHI '08}
}


@inproceedings{10.1145/3025453.3025593,
	author = {Lazem, Shaimaa and Jad, Hussein Aly},
	title = {We Play We Learn: Exploring the Value of Digital Educational Games in Rural Egypt},
	year = {2017},
	isbn = {9781450346559},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3025453.3025593},
	doi = {10.1145/3025453.3025593},
	abstract = {The Egyptian education system faces urgent challenges. Proposed governmental reforms tend to focus on increasing access to physical and digital resources. There is insufficient understanding as to how the provided resources are currently used in rural areas. We explored the extent to which digital technology could motivate primary students to collaboratively learn a challenging topic in the National Mathematics Curriculum. We designed and researched a digital game to support memorizing multiplication facts. We used an incentive structure that encouraged individual learning with rewarding teamwork. The game was tested with mixed ability and gender groups of students using the Teams-Game-Tournament collaboration technique. A key outcome was that the students with educationally disadvantaged backgrounds benefited from using the game format. They devised their own play and study strategies. We discuss implications on future designs of the game, and considerations for its integration in Egyptian schools.},
	booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
	pages = {2782–2791},
	numpages = {10},
	keywords = {Egypt, HCI4D, ICT4D, ICTD, collaborative learning, edutainment, game-based learning, games, play},
	location = {Denver, Colorado, USA},
	series = {CHI '17}
}


@inproceedings{10.1145/3025453.3026028,
	author = {Dey, Arindam and Piumsomboon, Thammathip and Lee, Youngho and Billinghurst, Mark},
	title = {Effects of Sharing Physiological States of Players in a Collaborative Virtual Reality Gameplay},
	year = {2017},
	isbn = {9781450346559},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3025453.3026028},
	doi = {10.1145/3025453.3026028},
	abstract = {Interfaces for collaborative tasks, such as multiplayer games can enable more effective and enjoyable collaboration. However, in these systems, the emotional states of the users are often not communicated properly due to their remoteness from one another. In this paper, we investigate the effects of showing emotional states of one collaborator to the other during an immersive Virtual Reality (VR) gameplay experience. We created two collaborative immersive VR games that display the real-time heart-rate of one player to the other. The two different games elicited different emotions, one joyous and the other scary. We tested the effects of visualizing heart-rate feedback in comparison with conditions where such a feedback was absent. The games had significant main effects on the overall emotional experience.},
	booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
	pages = {4045–4056},
	numpages = {12},
	keywords = {collaborative gameplay, emotions, empathic computing, physiological sensors, user study., virtual reality},
	location = {Denver, Colorado, USA},
	series = {CHI '17}
}


@inproceedings{10.1145/3544548.3581041,
	author = {Wu, Yudan and You, Shanhe and Guo, Zixuan and Li, Xiangyang and Zhou, Guyue and Gong, Jiangtao},
	title = {MR.Brick: Designing A Remote Mixed-reality Educational Game System for Promoting Children’s Social &amp; Collaborative Skills},
	year = {2023},
	isbn = {9781450394215},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3544548.3581041},
	doi = {10.1145/3544548.3581041},
	abstract = {Children are one of the groups most influenced by COVID-19-related social distancing, and a lack of contact with peers can limit their opportunities to develop social and collaborative skills. However, remote socialization and collaboration as an alternative approach is still a great challenge for children. This paper presents MR.Brick, a Mixed Reality (MR) educational game system that helps children adapt to remote collaboration. A controlled experimental study involving 24 children aged six to ten was conducted to compare MR.Brick with the traditional video game by measuring their social and collaborative skills and analyzing their multi-modal playing behaviours. The results showed that MR.Brick was more conducive to children’s remote collaboration experience than the traditional video game. Given the lack of training systems designed for children to collaborate remotely, this study may inspire interaction design and educational research in related fields.},
	booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
	articleno = {448},
	numpages = {18},
	keywords = {children, educational game, mixed reality, remote collaboration, social and collaborative skill, tangible user interface},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	series = {CHI '23}
}


@inproceedings{10.1145/3411764.3445041,
	author = {Sabet, Mehrnaz and Orand, Mania and W. McDonald, David},
	title = {Designing Telepresence Drones to Support Synchronous, Mid-air Remote Collaboration: An Exploratory Study},
	year = {2021},
	isbn = {9781450380966},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411764.3445041},
	doi = {10.1145/3411764.3445041},
	abstract = {Drones are increasingly used to support humanitarian crises and events that involve dangerous or costly tasks. While drones have great potential for remote collaborative work and aerial telepresence, existing drone technology is limited in its support for synchronous collaboration among multiple remote users. Through three design iterations and evaluations, we prototyped Squadrone, a novel aerial telepresence platform that supports synchronous mid-air collaboration among multiple remote users. We present our design and report results from evaluating our iterations with 13 participants in 3 different collaboration configurations. Our first design iteration validates the basic functionality of the platform. Then, we establish the effectiveness of collaboration using a 360-degree shared aerial display. Finally, we simulate a type of search task in an open environment to see if collaborative telepresence impacts members’ participation. The results validate some initial goals for Squadrone and are used to reflect back on a recent telepresence design framework.},
	booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {450},
	numpages = {17},
	keywords = {Collaborative remote control, Collaborative work, Drones, Quadcopters, Remote collaboration, Telepresence, UAV, User Interface},
	location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
	series = {CHI '21}
}


@inproceedings{10.1145/3613904.3642293,
	author = {Gr\o{}nb\ae{}k, Jens Emil Sloth and S\'{a}nchez Esquivel, Juan and Leiva, Germ\'{a}n and Velloso, Eduardo and Gellersen, Hans and Pfeuffer, Ken},
	title = {Blended Whiteboard: Physicality and Reconfigurability in Remote Mixed Reality Collaboration},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642293},
	doi = {10.1145/3613904.3642293},
	abstract = {The whiteboard is essential for collaborative work. To preserve its physicality in remote collaboration, Mixed Reality (MR) can blend real whiteboards across distributed spaces. Going beyond reality, MR can further enable interactions like panning and zooming in a virtually reconfigurable infinite whiteboard. However, this reconfigurability conflicts with the sense of physicality. To address this tension, we introduce Blended Whiteboard, a remote collaborative MR system enabling reconfigurable surface blending across distributed physical whiteboards. Blended Whiteboard supports a unique collaboration style, where users can sketch on their local whiteboards but also reconfigure the blended space to facilitate transitions between loosely and tightly coupled work. We describe design principles inspired by proxemics; supporting users in changing between facing each other and being side-by-side, and switching between navigating the whiteboard synchronously and independently. Our work shows exciting benefits and challenges of combining physicality and reconfigurability in the design of distributed MR whiteboards.},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	articleno = {798},
	numpages = {16},
	keywords = {3C collaboration model, avatars, f-formations, mixed reality, proxemics, remote collaboration},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '24}
}


@inproceedings{10.1145/3411764.3445576,
	author = {Gasques, Danilo and Johnson, Janet G. and Sharkey, Tommy and Feng, Yuanyuan and Wang, Ru and Xu, Zhuoqun Robin and Zavala, Enrique and Zhang, Yifei and Xie, Wanze and Zhang, Xinming and Davis, Konrad and Yip, Michael and Weibel, Nadir},
	title = {ARTEMIS: A Collaborative Mixed-Reality System for Immersive Surgical Telementoring},
	year = {2021},
	isbn = {9781450380966},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411764.3445576},
	doi = {10.1145/3411764.3445576},
	abstract = {Traumatic injuries require timely intervention, but medical expertise is not always available at the patient’s location. Despite recent advances in telecommunications, surgeons still have limited tools to remotely help inexperienced surgeons. Mixed Reality hints at a future where remote collaborators work side-by-side as if co-located; however, we still do not know how current technology can improve remote surgical collaboration. Through role-playing and iterative-prototyping, we identify collaboration practices used by expert surgeons to aid novice surgeons as well as technical requirements to facilitate these practices. We then introduce ARTEMIS, an AR-VR collaboration system that supports these key practices. Through an observational study with two expert surgeons and five novice surgeons operating on cadavers, we find that ARTEMIS supports remote surgical mentoring of novices through synchronous point, draw, and look affordances and asynchronous video clips. Most participants found that ARTEMIS facilitates collaboration despite existing technology limitations explored in this paper.},
	booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {662},
	numpages = {14},
	keywords = {Augmented Reality, Collaboration, Mixed Reality, Surgery, Telementoring, Virtual Reality},
	location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
	series = {CHI '21}
}


@inproceedings{10.1145/2470654.2481312,
	author = {Houben, Steven and Bardram, Jakob E. and Vermeulen, Jo and Luyten, Kris and Coninx, Karin},
	title = {Activity-centric support for ad hoc knowledge work: a case study of co-activity manager},
	year = {2013},
	isbn = {9781450318990},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2470654.2481312},
	doi = {10.1145/2470654.2481312},
	abstract = {Modern knowledge work consists of both individual and highly collaborative activities that are typically composed of a number of configuration, coordination and articulation processes. The desktop interface today, however, provides very little support for these processes and rather forces knowledge workers to adapt to the technology. We introduce co-Activity Manager, an activity-centric desktop system that (i) provides tools for ad hoc dynamic configuration of a desktop working context, (ii) supports both explicit and implicit articulation of ongoing work through a built-in collaboration manager and (iii) provides the means to coordinate and share working context with other users and devices. In this paper, we discuss the activity theory informed design of co-Activity Manager and report on a 14 day field deployment in a multi-disciplinary software development team. The study showed that the activity-centric workspace supports different individual and collaborative work configuration practices and that activity-centric collaboration is a two-phase process consisting of an activity sharing and per-activity coordination phase.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {2263–2272},
	numpages = {10},
	keywords = {activity theory, activity-centric computing, collaborative work, desktop interface},
	location = {Paris, France},
	series = {CHI '13}
}


@inproceedings{10.1145/3290605.3300458,
	author = {Piumsomboon, Thammathip and Lee, Gun A. and Irlitti, Andrew and Ens, Barrett and Thomas, Bruce H. and Billinghurst, Mark},
	title = {On the Shoulder of the Giant: A Multi-Scale Mixed Reality Collaboration with 360 Video Sharing and Tangible Interaction},
	year = {2019},
	isbn = {9781450359702},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3290605.3300458},
	doi = {10.1145/3290605.3300458},
	abstract = {We propose a multi-scale Mixed Reality (MR) collaboration between the Giant, a local Augmented Reality user, and the Miniature, a remote Virtual Reality user, in Giant-Miniature Collaboration (GMC). The Miniature is immersed in a 360-video shared by the Giant who can physically manipulate the Miniature through a tangible interface, a combined 360-camera with a 6 DOF tracker. We implemented a prototype system as a proof of concept and conducted a user study (n=24) comprising of four parts comparing: A) two types of virtual representations, B) three levels of Miniature control, C) three levels of 360-video view dependencies, and D) four 360-camera placement positions on the Giant. The results show users prefer a shoulder mounted camera view, while a view frustum with a complimentary avatar is a good visualization for the Miniature virtual representation. From the results, we give design recommendations and demonstrate an example Giant-Miniature Interaction.},
	booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
	pages = {1–17},
	numpages = {17},
	keywords = {live panorama sharing, mixed reality, multi-scale, remote collaboration, tangible user interface, wearable interface},
	location = {Glasgow, Scotland Uk},
	series = {CHI '19}
}


@inproceedings{10.1145/2207676.2208689,
	author = {Bardram, Jakob and Gueddana, Sofiane and Houben, Steven and Nielsen, S\o{}ren},
	title = {ReticularSpaces: activity-based computing support for physically distributed and collaborative smart spaces},
	year = {2012},
	isbn = {9781450310154},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2207676.2208689},
	doi = {10.1145/2207676.2208689},
	abstract = {Smart spaces research focuses on technology for multiple displays and devices for collocated participants. In most approaches, however, users have to cope with heterogeneous interfaces and information organization, as well as a lack of support for collaboration with mobile and remote users outside the smart space. In this paper, we present ReticularSpaces; a multi-display smart space system built on the principles of activity-based computing. The focus of ReticularSpaces is to support: (i) unified interaction with applications and documents through ReticularUI, a novel distributed user interfaces design; (ii) management of the complexity of tasks between users and displays; (iii) mobile users in a local, remote or 'nomadic' settings; and (iv) collaboration among local and remote users. We describe the motivation, design, and architecture of ReticularSpaces, and report from a preliminary feasibility study. The study shows that participants found ReticularSpaces useful and effective, but at the same time reveals new areas for research on smart environments.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {2845–2854},
	numpages = {10},
	keywords = {smart spaces, nomadic computing, multiple display environments, distributed user interfaces, collaboration},
	location = {Austin, Texas, USA},
	series = {CHI '12}
}


@inproceedings{10.1145/3411764.3445144,
	author = {Odili Uchidiuno, Judith and Hammer, Jessica and Koedinger, Ken and Ogan, Amy},
	title = {Fostering Equitable Help-Seeking for K-3 Students in Low Income and Rural Contexts},
	year = {2021},
	isbn = {9781450380966},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411764.3445144},
	doi = {10.1145/3411764.3445144},
	abstract = {Adaptive Collaborative Learning Support (ACLS) systems improve collaboration and learning for students over individual work or collaboration with non-adaptive support. However, many ACLS systems are ill-suited for rural contexts where students often need multiple kinds of support to complete tasks, may speak languages unsupported by the system, and require more than pre-assigned tutor-tutee student pairs for more equitable learning. We designed an intervention that fosters more equitable help-seeking by automatically detecting student struggles and prompts them to seek help from specific peers that can help. We conducted a mixed-methods experimental study with 98 K-3 students in a rural village in Tanzania over a one-month period, evaluating how the system affects student interactions, system engagement, and student learning. Our intervention increased student interactions by almost 4 times compared to the control condition, increased domain knowledge interactions, and propelled students to engage in more cognitively challenging activities.},
	booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {684},
	numpages = {14},
	location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
	series = {CHI '21}
}


@inproceedings{10.1145/3613904.3642435,
	author = {Lakhdhir, Sabrina and Nayar, Chehak and Anderson, Fraser and Fournier, Helene and Holsti, Liisa and Kondratova, Irina and Perin, Charles and Somanath, Sowmya},
	title = {GlucoMaker: Enabling Collaborative Customization of Glucose Monitors},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642435},
	doi = {10.1145/3613904.3642435},
	abstract = {Millions of individuals with diabetes use glucose monitors to track blood sugar levels. Research shows that such individuals seek to customize different aspects of their interactions with these devices, including how they engage with, decorate, and wear them. However, it remains challenging to tailor both device form and function to accommodate individual needs. To address this challenge, we introduce GlucoMaker, a system for collaboratively customizing physical design aspects of glucose monitors. Prior to designing GlucoMaker, we conducted a prototyping and focus group study to understand customization preferences and collaboration benefits. GlucoMaker provides individuals with the ability to a) select monitor form and function preferences, b) alter predefined and downloadable digital model files, c) receive feedback on monitor designs from stakeholders, and d) learn technical design aspects. We further demonstrate the versatility and design space of GlucoMaker with three examples of different form and function use cases.},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	articleno = {127},
	numpages = {21},
	keywords = {collaboration, customization, design, design for health, fabrication, glucose monitors},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '24}
}


@inproceedings{10.1145/2858036.2858224,
	author = {Zagermann, Johannes and Pfeil, Ulrike and R\"{a}dle, Roman and Jetter, Hans-Christian and Klokmose, Clemens and Reiterer, Harald},
	title = {When Tablets meet Tabletops: The Effect of Tabletop Size on Around-the-Table Collaboration with Personal Tablets},
	year = {2016},
	isbn = {9781450333627},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2858036.2858224},
	doi = {10.1145/2858036.2858224},
	abstract = {Cross-device collaboration with tablets is an increasingly popular topic in HCI. Previous work has shown that tablet-only collaboration can be improved by an additional shared workspace on an interactive tabletop. However, large tabletops are costly and need space, raising the question to what extent the physical size of shared horizontal surfaces really pays off. In order to analyse the suitability of smaller-than-tabletop devices (e.g. tablets) as a low-cost alternative, we studied the effect of the size of a shared horizontal interactive workspace on users' attention, awareness, and efficiency during cross-device collaboration. In our study, 15 groups of two users executed a sensemaking task with two personal tablets (9.7") and a horizontal shared display of varying sizes (10.6", 27", and 55"). Our findings show that different sizes lead to differences in participants' interaction with the tabletop and in the groups' communication styles. To our own surprise we found that larger tabletops do not necessarily improve collaboration or sensemaking results, because they can divert users' attention away from their collaborators and towards the shared display.},
	booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
	pages = {5470–5481},
	numpages = {12},
	keywords = {tablets, tabletops, group work, display size, cross-device interaction},
	location = {San Jose, California, USA},
	series = {CHI '16}
}


@inproceedings{10.1145/3173574.3173632,
	author = {Toxtli, Carlos and Monroy-Hern\'{a}ndez, Andr\'{e}s and Cranshaw, Justin},
	title = {Understanding Chatbot-mediated Task Management},
	year = {2018},
	isbn = {9781450356206},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3173574.3173632},
	doi = {10.1145/3173574.3173632},
	abstract = {Effective task management is essential to successful team collaboration. While the past decade has seen considerable innovation in systems that track and manage group tasks, these innovations have typically been outside of the principal communication channels: email, instant messenger, and group chat. Teams formulate, discuss, refine, assign, and track the progress of their collaborative tasks over electronic communication channels, yet they must leave these channels to update their task-tracking tools, creating a source of friction and inefficiency. To address this problem, we explore how bots might be used to mediate task management for individuals and teams. We deploy a prototype bot to eight different teams of information workers to help them create, assign, and keep track of tasks, all within their main communication channel. We derived seven insights for the design of future bots for coordinating work.},
	booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	pages = {1–6},
	numpages = {6},
	keywords = {task management, mediated communication, chatbot, bot},
	location = {<conf-loc>, <city>Montreal QC</city>, <country>Canada</country>, </conf-loc>},
	series = {CHI '18}
}


@inproceedings{10.1145/3544548.3581285,
	author = {Ivanyi, Balazs Andras and Tjemsland, Truls Bendik and Tsalidis de Zabala, Christian Vasileios and Toth, Lilla Julia and Dyrholm, Marcus Alexander and Naylor, Scott James and Paradiso, Ann and Lamb, Dwayne and Chudge, Jarnail and Adjorlu, Ali and Serafin, Stefania},
	title = {DuoRhythmo: Design and remote user experience evaluation (UXE) of a collaborative accessible digital musical interface (CADMI) for people with ALS (PALS)},
	year = {2023},
	isbn = {9781450394215},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3544548.3581285},
	doi = {10.1145/3544548.3581285},
	abstract = {We present DuoRhythmo, a collaborative accessible digital musical interface (CADMI) that gives people living with Amyotrophic Lateral Sclerosis (PALS) the experience of remotely and collaboratively creating music in real-time. We designed DuoRhythmo specifically to be utilized for eye tracking and optimized it for head- and computer mouse interaction, as well using a user-centered design approach. Together with five PALS, we completed a mixed-methods evaluation to assess the accessibility of DuoRhythmo. Participants described the CADMI using the Microsoft Desirability Toolkit (MDT) as fun, empowering, accessible, easy to use, engaging, and stimulating and gave an average System Usability Scale (SUS) score of 79.5. We suggest further research on remote collaboration within the field of accessible digital musical instruments (ADMIs) using the term CADMI to explore the positive effects of collaborative music-making on the quality of life of PALS.},
	booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
	articleno = {56},
	numpages = {13},
	keywords = {accessibility, eye tracking, musical co-creation, people living with ALS, remote user experience evaluation, user-centered design},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	series = {CHI '23}
}


@inproceedings{10.1145/3411764.3445272,
	author = {Alharthi, Sultan A. and LaLone, Nicolas James and Sharma, Hitesh Nidhi and Dolgov, Igor and Toups Dugas, Phoebe O.},
	title = {An Activity Theory Analysis of Search &amp; Rescue Collective Sensemaking and Planning Practices},
	year = {2021},
	isbn = {9781450380966},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411764.3445272},
	doi = {10.1145/3411764.3445272},
	abstract = {Search and rescue (SAR), a disaster response activity performed to locate and save victims, primarily involves collective sensemaking and planning. SAR responders learn to search and navigate the environment, process information about buildings, and collaboratively plan with maps. We synthesize data from five sources, including field observations and interviews, to understand the informational components of SAR and how information is recorded and communicated. We apply activity theory, uncovering unforeseen factors that are relevant to the design of collaboration systems and training solutions. Through our analysis, we derive design implications to support collaborative information technology and training systems: mixing physical and digital mapping; mixing individual and collective mapping; building for different levels and sources of information; and building for different rules, roles, and activities.},
	booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {146},
	numpages = {20},
	keywords = {training, sensemaking, search and rescue, practice, planning, maps, activity theory.},
	location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
	series = {CHI '21}
}


@inproceedings{10.1145/1978942.1978983,
	author = {Vihavainen, Sami and Mate, Sujeet and Sepp\"{a}l\"{a}, Lassi and Cricri, Francesco and Curcio, Igor D.D.},
	title = {We want more: human-computer collaboration in mobile social video remixing of music concerts},
	year = {2011},
	isbn = {9781450302289},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1978942.1978983},
	doi = {10.1145/1978942.1978983},
	abstract = {Recording and publishing mobile video clips from music concerts is popular. There is a high potential to increase the concert's perceived value when producing video remixes from individual video clips and using them socially. A digital production of a video remix is an interactive process between human and computer. However, it is not clear what the collaboration implications between human and computer are.We present a case study where we compare the processes and products of manual and automatic mobile video remixing. We provide results from the first systematic real world study of the subject. We draw our observations from a user trial where fans recorded mobile video clips during a rock concert.The results reveal issues on heterogeneous interests of the stakeholders, unexpected uses of the raw material, the burden of editing, diverse quality requirements, motivations for remixing, the effect of understanding the logic of automation, and the collaborative use of manual and automatic remixing.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {287–296},
	numpages = {10},
	keywords = {video, social, music, mobile, human factors, automation},
	location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
	series = {CHI '11}
}


@inproceedings{10.1145/3411764.3445246,
	author = {Johnson, Janet G and Gasques, Danilo and Sharkey, Tommy and Schmitz, Evan and Weibel, Nadir},
	title = {Do You Really Need to Know Where “That” Is? Enhancing Support for Referencing in Collaborative Mixed Reality Environments},
	year = {2021},
	isbn = {9781450380966},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411764.3445246},
	doi = {10.1145/3411764.3445246},
	abstract = {Mixed Reality has been shown to enhance remote guidance and is especially well-suited for physical tasks. Conversations during these tasks are heavily anchored around task objects and their spatial relationships in the real world, making referencing - the ability to refer to an object in a way that is understood by others - a crucial process that warrants explicit support in collaborative Mixed Reality systems. This paper presents a 2x2 mixed factorial experiment that explores the effects of providing spatial information and system-generated guidance to task objects. It also investigates the effects of such guidance on the remote collaborator’s need for spatial information. Our results show that guidance increases performance and communication efficiency while reducing the need for spatial information, especially in unfamiliar environments. Our results also demonstrate a reduced need for remote experts to be in immersive environments, making guidance more scalable, and expertise more accessible.},
	booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {514},
	numpages = {14},
	keywords = {Remote Guidance, Referencing, Mixed Reality, Collaboration},
	location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
	series = {CHI '21}
}


@inproceedings{10.1145/3544548.3581444,
	author = {Johnson, Janet G and Sharkey, Tommy and Butarbutar, Iramuali Cynthia and Xiong, Danica and Huang, Ruijie and Sy, Lauren and Weibel, Nadir},
	title = {UnMapped: Leveraging Experts’ Situated Experiences to Ease Remote Guidance in Collaborative Mixed Reality},
	year = {2023},
	isbn = {9781450394215},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3544548.3581444},
	doi = {10.1145/3544548.3581444},
	abstract = {Collaborative Mixed Reality (MR) systems that help extend expertise for physical tasks to remote environments often situate experts in an immersive view of the task environment to bring the collaboration closer to collocated settings. In this paper, we design UnMapped, an alternative interface for remote experts that combines a live 3D view of the active space within the novice’s environment with a static 3D recreation of the expert’s own workspace to leverage their existing spatial memories within it. We evaluate the impact of this approach on single and repeated use of collaborative MR systems for remote guidance through a comparative study. Our results indicate that despite having a limited understanding of the novice’s environment, using an UnMapped interface increased performance and communication efficiency while reducing experts’ task load. We also outline the various affordances of providing remote experts with a familiar and spatially-stable environment to assist novices.},
	booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
	articleno = {878},
	numpages = {20},
	keywords = {Augmented Reality, Mixed Reality, Physical Tasks, Remote Collaboration, Virtual Reality},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	series = {CHI '23}
}


@inproceedings{10.1145/3313831.3376541,
	author = {Wells, Thomas and Houben, Steven},
	title = {CollabAR  Investigating the Mediating Role of Mobile AR Interfaces on Co-Located Group Collaboration},
	year = {2020},
	isbn = {9781450367080},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3313831.3376541},
	doi = {10.1145/3313831.3376541},
	abstract = {Mobile Augmented Reality (AR) technology is enabling new applications for different domains including architecture, education or medical work. As AR interfaces project digital data, information and models into the real world, it allows for new forms of collaborative work. However, despite the wide availability of AR applications, very little is known about how AR interfaces mediate and shape collaborative practices. This paper presents a study which examines how a mobile AR (M-AR) interface for inspecting and discovering AR models of varying complexity impacts co-located group practices. We contribute new insights into how current mobile AR interfaces impact co-located collaboration. Our results show that M-AR interfaces induce high mental load and frustration, cause a high number of context switches between devices and group discussion, and overall leads to a reduction in group interaction. We present design recommendations for future work focusing on collaborative AR interfaces.},
	booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
	pages = {1–13},
	numpages = {13},
	keywords = {co-located collaboration, mobile augmented reality},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '20}
}


@inproceedings{10.1145/3613904.3642864,
	author = {Rasch, Julian and Perzl, Florian and Weiss, Yannick and M\"{u}ller, Florian},
	title = {Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642864},
	doi = {10.1145/3613904.3642864},
	abstract = {With the proliferation of VR and a metaverse on the horizon, many multi-user activities are migrating to the VR world, calling for effective collaboration support. As one key feature, traditional collaborative systems provide users with undo mechanics to reverse errors and other unwanted changes. While undo has been extensively researched in this domain and is now considered industry standard, it is strikingly absent for VR systems in research and industry. This work addresses this research gap by exploring different undo techniques for basic object manipulation in different collaboration modes in VR. We conducted a study involving 32 participants organized in teams of two. Here, we studied users’ performance and preferences in a tower stacking task, varying the available undo techniques and their mode of collaboration. The results suggest that users desire and use undo in VR and that the choice of the undo technique impacts users’ performance and social connection.},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	articleno = {952},
	numpages = {14},
	keywords = {CSCW, Connectedness, Multi-User, SocialVR, Undo, Virtual Reality},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '24}
}


@inproceedings{10.1145/642611.642705,
	author = {Ballagas, Rafael and Ringel, Meredith and Stone, Maureen and Borchers, Jan},
	title = {iStuff: a physical user interface toolkit for ubiquitous computing environments},
	year = {2003},
	isbn = {1581136307},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/642611.642705},
	doi = {10.1145/642611.642705},
	abstract = {The iStuff toolkit of physical devices, and the flexible software infrastructure to support it, were designed to simplify the exploration of novel interaction techniques in the post-desktop era of multiple users, devices, systems and applications collaborating in an interactive environment. The toolkit leverages an existing interactive workspace in-frastructure, making it lightweight and platform independent. The supporting software framework includes a dynamically configurable intermediary to simplify the mapping of devices to applications. We describe the iStuff architecture and provide several examples of iStuff, organized into a design space of ubiquitous computing interaction components. The main contribution is a physical toolkit for distributed, heterogeneous environments with run-time retargetable device data flow. We conclude with some insights and experiences derived from using this toolkit and framework to prototype experimental interaction techniques for ubiquitous computing environments.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {537–544},
	numpages = {8},
	keywords = {wireless devices, user interface toolkits, ubiquitous computing, tangible user interfaces, programming environments, intermediation, input and interaction technologies, development tools},
	location = {Ft. Lauderdale, Florida, USA},
	series = {CHI '03}
}


@inproceedings{10.1145/642611.642711,
	author = {Brown, Barry and MacColl, Ian and Chalmers, Matthew and Galani, Areti and Randell, Cliff and Steed, Anthony},
	title = {Lessons from the lighthouse: collaboration in a shared mixed reality system},
	year = {2003},
	isbn = {1581136307},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/642611.642711},
	doi = {10.1145/642611.642711},
	abstract = {Museums attract increasing numbers of online visitors along with their conventional physical visitors. This paper presents a study of a mixed reality system that allows web, virtual reality and physical visitors to share a museum visit together in real time. Our system allows visitors to share their location and orientation, communicate over a voice channel, and jointly navigate around a shared information space. Results from a study of 34 users of the system show that visiting with the system was highly interactive and retained many of the attractions of a traditional shared exhibition visit. Specifically, users could navigate together, collaborate around objects and discuss exhibits. These findings have implications for non-museum settings, in particular how location awareness is a powerful resource for collaboration, and how 'hybrid objects' can support collaboration at-a-distance.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {577–584},
	numpages = {8},
	keywords = {virtual reality, museum visiting, mixed reality, location-awareness, context-awareness, WWW},
	location = {Ft. Lauderdale, Florida, USA},
	series = {CHI '03}
}


@inproceedings{10.1145/2858036.2858213,
	author = {Martinez-Maldonado, Roberto and Goodyear, Peter and Kay, Judy and Thompson, Kate and Carvalho, Lucila},
	title = {An Actionable Approach to Understand Group Experience in Complex, Multi-surface Spaces},
	year = {2016},
	isbn = {9781450333627},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2858036.2858213},
	doi = {10.1145/2858036.2858213},
	abstract = {There is a steadily growing interest in the design of spaces in which multiple interactive surfaces are present and, in turn, in understanding their role in group activity. However, authentic activities in these multi-surface spaces can be complex. Groups commonly use digital and non-digital artefacts, tools and resources, in varied ways depending on their specific social and epistemic goals. Thus, designing for collaboration in such spaces can be very challenging. Importantly, there is still a lack of agreement on how to approach the analysis of groups' experiences in these heterogeneous spaces. This paper presents an actionable approach that aims to address the complexity of understanding multi-user multi-surface systems. We provide a structure for applying different analytical tools in terms of four closely related dimensions of user activity: the setting, the tasks, the people and the runtime co-configuration. The applicability of our approach is illustrated with six types of analysis of group activity in a multi-surface design studio.},
	booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
	pages = {2062–2074},
	numpages = {13},
	keywords = {ubicomp ecologies, shared-display, multi-surface, horizontal display, groupware, collocated collaboration},
	location = {San Jose, California, USA},
	series = {CHI '16}
}


@inproceedings{10.1145/1240624.1240787,
	author = {Adamczyk, Piotr D. and Twidale, Michael B.},
	title = {Supporting multidisciplinary collaboration: requirements from novel HCI education},
	year = {2007},
	isbn = {9781595935939},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1240624.1240787},
	doi = {10.1145/1240624.1240787},
	abstract = {Many collaborative design tools may suffer from being too generic to address the specific complexities inherent in multidisciplinary collaboration. We provide accounts of several multidisciplinary HCI courses at our institution, elaborating on the challenges student teams face when integrating design practice from a wide variety of disciplines. Of particular interest are the distinct approaches that these multidisciplinary teams adopt that differ from more common forms of collaborative design. We suggest reasons for the poor rate of adoption of existing collaborative support tools and outline specific suggestions for directions in both ethnographic studies of multidisciplinary collaboration and collaborative systems design.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {1073–1076},
	numpages = {4},
	keywords = {social bookmarking, multidisciplinary collaboration, low fidelity prototyping, design tools, design education},
	location = {<conf-loc>, <city>San Jose</city>, <state>California</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '07}
}


@inproceedings{10.1145/3313831.3376550,
	author = {Bai, Huidong and Sasikumar, Prasanth and Yang, Jing and Billinghurst, Mark},
	title = {A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing},
	year = {2020},
	isbn = {9781450367080},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3313831.3376550},
	doi = {10.1145/3313831.3376550},
	abstract = {Supporting natural communication cues is critical for people to work together remotely and face-to-face. In this paper we present a Mixed Reality (MR) remote collaboration system that enables a local worker to share a live 3D panorama of his/her surroundings with a remote expert. The remote expert can also share task instructions back to the local worker using visual cues in addition to verbal communication. We conducted a user study to investigate how sharing augmented gaze and gesture cues from the remote expert to the local worker could affect the overall collaboration performance and user experience. We found that by combing gaze and gesture cues, our remote collaboration system could provide a significantly stronger sense of co-presence for both the local and remote users than using the gaze cue alone. The combined cues were also rated significantly higher than the gaze in terms of ease of conveying spatial actions.},
	booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
	pages = {1–13},
	numpages = {13},
	keywords = {3d panorama, augmented reality, eye gaze, hand gesture, mixed reality, remote collaboration, scene reconstruction, virtual reality},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '20}
}


@inproceedings{10.1145/2207676.2207685,
	author = {Kreitmayer, Stefan and Rogers, Yvonne and Laney, Robin and Peake, Stephen},
	title = {From participatory to contributory simulations: changing the game in the classroom},
	year = {2012},
	isbn = {9781450310154},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2207676.2207685},
	doi = {10.1145/2207676.2207685},
	abstract = {There is much potential for supporting collaborative learning with interactive computer simulations in formal education and professional training. A number have been developed for single user and remote interaction. In contrast, our research is concerned with how such learning activities can be designed to fit into co-located large group settings, such as whole classrooms. This paper reports on the iterative design process and two in-the-wild evaluations of the 4Decades game, which was developed for a whole classroom of students to engage with a climate simulation. The system allows students to play and change the rules of the simulation, thereby enabling them to be actively engaged at different levels. The notion of Contributory Simulations is proposed as an instructional model that empowers groups to make informed, critical changes to the underlying scientific model. We discuss how large-group collaboration was supported through constraining an ecology of shared devices and public displays.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {49–58},
	numpages = {10},
	keywords = {ubiquitous technologies, tablets, serious games, participatory simulations, contributory simulations, collaborative learning, ambient displays},
	location = {Austin, Texas, USA},
	series = {CHI '12}
}


@inproceedings{10.1145/3613904.3641924,
	author = {Szymanski, Annalisa and Wimer, Brianna L and Anuyah, Oghenemaro and Eicher-Miller, Heather A and Metoyer, Ronald A},
	title = {Integrating Expertise in LLMs: Crafting a Customized Nutrition Assistant with Refined Template Instructions},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3641924},
	doi = {10.1145/3613904.3641924},
	abstract = {Large Language Models (LLMs) have the potential to contribute to the fields of nutrition and dietetics in generating food product explanations that facilitate informed food selections. However, the extent to which these models offer effective and accurate information remains unverified. In collaboration with registered dietitians (RDs), we evaluate the strengths and weaknesses of LLMs in providing accurate and personalized nutrition information. Through a mixed-methods approach, RDs validated GPT-4 outputs at various levels of prompt specificity, which led to the development of design guidelines used to prompt LLMs for nutrition information. We tested these guidelines by creating a GPT prototype, The Food Product Nutrition Assistant, tailored for food product explanations. This prototype was refined and evaluated in focus groups with RDs. We find that the implementation of these dietitian-reviewed template instructions enhance the generation of detailed food product descriptions and tailored nutrition information.},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	articleno = {992},
	numpages = {22},
	keywords = {Artificial Intelligence, Food Recommendations, Large Language Models},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '24}
}


@inproceedings{10.1145/3173574.3173758,
	author = {Park, Seonwook and Gebhardt, Christoph and R\"{a}dle, Roman and Feit, Anna Maria and Vrzakova, Hana and Dayama, Niraj Ramesh and Yeo, Hui-Shyong and Klokmose, Clemens N. and Quigley, Aaron and Oulasvirta, Antti and Hilliges, Otmar},
	title = {AdaM: Adapting Multi-User Interfaces for Collaborative Environments in Real-Time},
	year = {2018},
	isbn = {9781450356206},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3173574.3173758},
	doi = {10.1145/3173574.3173758},
	abstract = {Developing cross-device multi-user interfaces (UIs) is a challenging problem. There are numerous ways in which content and interactivity can be distributed. However, good solutions must consider multiple users, their roles, their preferences and access rights, as well as device capabilities. Manual and rule-based solutions are tedious to create and do not scale to larger problems nor do they adapt to dynamic changes, such as users leaving or joining an activity. In this paper, we cast the problem of UI distribution as an assignment problem and propose to solve it using combinatorial optimization. We present a mixed integer programming formulation which allows real-time applications in dynamically changing collaborative settings. It optimizes the allocation of UI elements based on device capabilities, user roles, preferences, and access rights. We present a proof-of-concept designer-in-the-loop tool, allowing for quick solution exploration. Finally, we compare our approach to traditional paper prototyping in a lab study.},
	booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	pages = {1–14},
	numpages = {14},
	keywords = {ui adaptation, optimization, distributed user interface, cross-device interaction},
	location = {<conf-loc>, <city>Montreal QC</city>, <country>Canada</country>, </conf-loc>},
	series = {CHI '18}
}


@inproceedings{10.1145/1357054.1357102,
	author = {Danis, Catalina M. and Viegas, Fernanda B. and Wattenberg, Martin and Kriss, Jesse},
	title = {Your place or mine? visualization as a community component},
	year = {2008},
	isbn = {9781605580111},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1357054.1357102},
	doi = {10.1145/1357054.1357102},
	abstract = {Many Eyes is a web site that provides collaborative visualization services, allowing users to upload data sets, visualize them, and comment on each other's visualizations. This paper describes a first interview-based study of Many Eyes users, which sheds light on user motivation for creating public visualizations. Users talked about data for many reasons, from scientific research to political advocacy to hobbies. One consistent theme across these different scenarios is the use of visualizations in communication and collaborative practices. Collaboration and conversation, however, often took place outside the site, leaving no traces on Many Eyes itself. In other words, despite spurring significant social activity, Many Eyes is not so much an online community as a "community component" which users insert into pre-existing online social systems.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {275–284},
	numpages = {10},
	keywords = {visualization, social data analysis, distributed community, communication},
	location = {Florence, Italy},
	series = {CHI '08}
}


@inproceedings{10.1145/3025453.3025793,
	author = {Evans, Abigail C. and Davis, Katie and Fogarty, James and Wobbrock, Jacob O.},
	title = {Group Touch: Distinguishing Tabletop Users in Group Settings via Statistical Modeling of Touch Pairs},
	year = {2017},
	isbn = {9781450346559},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3025453.3025793},
	doi = {10.1145/3025453.3025793},
	abstract = {We present Group Touch, a method for distinguishing among multiple users simultaneously interacting with a tabletop computer using only the touch information supplied by the device. Rather than tracking individual users for the duration of an activity, Group Touch distinguishes users from each other by modeling whether an interaction with the tabletop corresponds to either: (1) a new user, or (2) a change in users currently interacting with the tabletop. This reframing of the challenge as distinguishing users rather than tracking and identifying them allows Group Touch to support multi-user collaboration in real-world settings without custom instrumentation. Specifically, Group Touch examines pairs of touches and uses the difference in orientation, distance, and time between two touches to determine whether the same person performed both touches in the pair. Validated with field data from high-school students in a classroom setting, Group Touch distinguishes among users "in the wild" with a mean accuracy of 92.92% (SD=3.94%). Group Touch can imbue collaborative touch applications in real-world settings with the ability to distinguish among multiple users.},
	booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
	pages = {35–47},
	numpages = {13},
	keywords = {tabletop, modeling, distinguishing users, "in the wild"},
	location = {Denver, Colorado, USA},
	series = {CHI '17}
}


@inproceedings{10.1145/3613904.3642857,
	author = {Patnaik, Biswaksen and Peng, Huaishu and Elmqvist, Niklas},
	title = {VisTorch: Interacting with Situated Visualizations using Handheld Projectors},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642857},
	doi = {10.1145/3613904.3642857},
	abstract = {Spatial data is best analyzed in situ, but existing mixed reality technologies can be bulky, expensive, or unsuitable for collaboration. We present VisTorch: a handheld device for projected situated analytics consisting of a pico-projector, a multi-spectrum camera, and a touch surface. VisTorch enables viewing charts situated in physical space by simply pointing the device at a surface to reveal visualizations in that location. We evaluated the approach using both a user study and an expert review. In the former, we asked 20 participants to first organize charts in space and then refer to these charts to answer questions. We observed three spatial and one temporal pattern in participant analyses. In the latter, four experts—a museum designer, a statistical software developer, a theater stage designer, and an environmental educator—utilized VisTorch to derive practical usage scenarios. Results from our study showcase the utility of situated visualizations for memory and recall.},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	articleno = {208},
	numpages = {13},
	keywords = {Ubiquitous analytics, augmented reality, immersive analytics, situated visualization.},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '24}
}


@inproceedings{10.1145/3544548.3581274,
	author = {Bala, Paulo and Sanches, Pedro and Ces\'{a}rio, Vanessa and Le\~{a}o, Sarah and Rodrigues, Catarina and Nunes, Nuno Jardim and Nisi, Valentina},
	title = {Towards Critical Heritage in the wild: Analysing Discomfort through Collaborative Autoethnography},
	year = {2023},
	isbn = {9781450394215},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3544548.3581274},
	doi = {10.1145/3544548.3581274},
	abstract = {As we engaged in designing digital interventions for intercultural dialogues around public cultural heritage sites, we saw an opportunity to surface multiple interpretations and points of view of history and shine a critical lens on current societal issues. To do so, we present the results of a collaborative auto-ethnography of alternative tours accompanied by intercultural guides, to explore sensory and embodied engagements with cultural heritage sites in a southern European capital. By focusing on the differences in how we experienced the heritage sites, we analyse the duality of discomfort, a common concept in HCI, in that it can both be deployed as a resource for designing systems that can transform people’s understanding of history or it can be a hindrance for engagement, having an unequal effect on individuals.},
	booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
	articleno = {771},
	numpages = {19},
	keywords = {cultural heritage, ethnography, intercultural dialogues},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	series = {CHI '23}
}


@inproceedings{10.1145/3613904.3642945,
	author = {Sakashita, Mose and Thoravi Kumaravel, Balasaravanan and Marquardt, Nicolai and Wilson, Andrew David},
	title = {SharedNeRF: Leveraging Photorealistic and View-dependent Rendering for Real-time and Remote Collaboration},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642945},
	doi = {10.1145/3613904.3642945},
	abstract = {Collaborating around physical objects necessitates examining different aspects of design or hardware in detail when reviewing or inspecting physical artifacts or prototypes. When collaborators are remote, coordinating the sharing of views of their physical environment becomes challenging. Video-conferencing tools often do not provide the desired viewpoints for a remote viewer. While RGB-D cameras offer 3D views, they lack the necessary fidelity. We introduce SharedNeRF, designed to enhance synchronous remote collaboration by leveraging the photorealistic and view-dependent nature of Neural Radiance Field (NeRF). The system complements the higher visual quality of the NeRF rendering with the instantaneity of a point cloud and combines them through carefully accommodating the dynamic elements within the shared space, such as hand gestures and moving objects. The system employs a head-mounted camera for data collection, creating a volumetric task space on the fly and updating it as the task space changes. In our preliminary study, participants successfully completed a flower arrangement task, benefiting from SharedNeRF’s ability to render the space in high fidelity from various viewpoints.},
	booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
	articleno = {675},
	numpages = {14},
	keywords = {Collaboration, NeRF, Spatial Interfaces;},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '24}
}


@inproceedings{10.1145/3491102.3501936,
	author = {Liao, Mengqi and Sundar, S. Shyam and B. Walther, Joseph},
	title = {User Trust in Recommendation Systems: A comparison of Content-Based, Collaborative and Demographic Filtering},
	year = {2022},
	isbn = {9781450391573},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3491102.3501936},
	doi = {10.1145/3491102.3501936},
	abstract = {Three of the most common approaches used in recommender systems are content-based filtering (matching users’ preferences with products’ characteristics), collaborative filtering (matching users with similar preferences), and demographic filtering (catering to users based on demographic characteristics). Do users’ intuitions lead them to trust one of these approaches over others, independent of the actual operations of these different systems? Does their faith in one type or another depend on the quality of the recommendation, rather than how the recommendation appears to have been derived? We conducted an empirical study with a prototype of a movie recommender system to find out. A 3 (Ostensible Recommender Type: Content vs. Collaborative vs. Demographic Filtering) x 2 (Recommendation Quality: Good vs. Bad) experiment (N=226) investigated how users evaluate systems and attribute responsibility for the recommendations they receive. We found that users trust systems that use collaborative filtering more, regardless of the system's performance. They think that they themselves are responsible for good recommendations but that the system is responsible for bad recommendations (reflecting a self-serving bias). Theoretical insights, design implications and practical solutions for the cold start problem are discussed.},
	booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
	articleno = {486},
	numpages = {14},
	keywords = {User Experience Design, Personalization, Empirical study that tells us about how people use a system},
	location = {<conf-loc>, <city>New Orleans</city>, <state>LA</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '22}
}


@inproceedings{10.1145/3173574.3173973,
	author = {Nouwens, Midas and Klokmose, Clemens Nylandsted},
	title = {The Application and Its Consequences for Non-Standard Knowledge Work},
	year = {2018},
	isbn = {9781450356206},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3173574.3173973},
	doi = {10.1145/3173574.3173973},
	abstract = {Application-centric computing dominates human-computer interactions, yet the concept of an application is ambiguous and the impact of its ubiquity underexplored. We unpack "the application" through the lens of non-standard knowledge work: freelance, self-employed, and fixed-term contract workers who create knowledge in collaboration with a wide variety of stakeholders on a per-project basis. Based on interviews with fourteen participants we describe how: i) their economic value is intertwined with data and skills related to specific applications; ii) their access to this value is systematically jeopardised in collaboration due to the different application practices, preferences, and proficiencies of other stakeholders; and iii) they mitigate the costs of this compromise through cross-application collaboration strategies. We trace these experiences to common characteristics of applications, such as update processes, interface symmetries, application-document relationships, and operating system and hardware dependencies. By empirically and analytically focusing on "the application", we reveal the implications of the current application-centric computing paradigm and discuss how variations within this model create qualitatively different human-computer interactions.},
	booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	pages = {1–12},
	numpages = {12},
	keywords = {non-standard work, knowledge work, application-centric computing, application paradigm},
	location = {<conf-loc>, <city>Montreal QC</city>, <country>Canada</country>, </conf-loc>},
	series = {CHI '18}
}


@inproceedings{10.1145/3491102.3517526,
	author = {Belghith, Yasmine and Venkatagiri, Sukrit and Luther, Kurt},
	title = {Compete, Collaborate, Investigate: Exploring the Social Structures of Open Source Intelligence Investigations},
	year = {2022},
	isbn = {9781450391573},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3491102.3517526},
	doi = {10.1145/3491102.3517526},
	abstract = {Online investigations are increasingly conducted by individuals with diverse skill levels and experiences, with mixed results. Novice investigations often result in vigilantism or doxxing, while expert investigations have greater success rates and fewer mishaps. Many of these experts are involved in a community of practice known as Open Source Intelligence (OSINT), with an ethos and set of techniques for conducting investigations using only publicly available data. Through semi-structured interviews with 14 expert OSINT investigators from nine different organizations, we examine the social dynamics of this community, including the collaboration and competition patterns that underlie their investigations. We also describe investigators’ use of and challenges with existing OSINT tools, and implications for the design of social computing systems to better support crowdsourced investigations.},
	booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
	articleno = {129},
	numpages = {18},
	keywords = {open source intelligence, investigation, experts, crowdsourcing, competition, collaboration, OSINT},
	location = {<conf-loc>, <city>New Orleans</city>, <state>LA</state>, <country>USA</country>, </conf-loc>},
	series = {CHI '22}
}


@inproceedings{10.1145/1978942.1979201,
	author = {Lucero, Andr\'{e}s and Holopainen, Jussi and Jokela, Tero},
	title = {Pass-them-around: collaborative use of mobile phones for photo sharing},
	year = {2011},
	isbn = {9781450302289},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1978942.1979201},
	doi = {10.1145/1978942.1979201},
	abstract = {In this paper we explore shared collocated interactions with mobile phones. We introduce a phone-based application that allows a small group of collocated people to share photos using the metaphor of passing paper photos around. The prototype encourages people to share their devices and use them interchangeably while discussing photos face-to-face. The prototype supports ad-hoc photo sharing in different contexts by taking into account the spatial arrangement of users around a table, measured with sensors embedded in their mobile phones. Our evaluations show that people are willing to share and connect their mobile phones to engage in collaborative interactions. Participants were able to easily share their collections of photos using our proposed interaction techniques.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {1787–1796},
	numpages = {10},
	keywords = {photo sharing., handheld devices, collocated interaction},
	location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
	series = {CHI '11}
}


@inproceedings{10.1145/642611.642714,
	author = {McDonald, David W.},
	title = {Recommending collaboration with social networks: a comparative evaluation},
	year = {2003},
	isbn = {1581136307},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/642611.642714},
	doi = {10.1145/642611.642714},
	abstract = {Studies of information seeking and workplace collaboration often find that social relationships are a strong factor in determining who collaborates with whom. Social networks provide one means of visualizing existing and potential interaction in organizational settings. Groupware designers are using social networks to make systems more sensitive to social situations and guide users toward effective collaborations. Yet, the implications of embedding social networks in systems have not been systematically studied. This paper details an evaluation of two different social networks used in a system to recommend individuals for possible collaboration. The system matches people looking for expertise with individuals likely to have expertise. The effectiveness of social networks for matching individuals is evaluated and compared. One finding is that social networks embedded into systems do not match individuals' perceptions of their personal social network. This finding and others raise issues for the use of social networks in groupware. Based on the evaluation results, several design considerations are discussed.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {593–600},
	numpages = {8},
	keywords = {social networks, recommender systems, information seeking, expertise locating},
	location = {Ft. Lauderdale, Florida, USA},
	series = {CHI '03}
}


@inproceedings{10.1145/1753326.1753516,
	author = {Forlizzi, Jodi and Barley, William C. and Seder, Thomas},
	title = {Where should i turn: moving from individual to collaborative navigation strategies to inform the interaction design of future navigation systems},
	year = {2010},
	isbn = {9781605589299},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1753326.1753516},
	doi = {10.1145/1753326.1753516},
	abstract = {The design of in-vehicle navigation systems fails to take into account the social nature of driving and automobile navigation. In this paper, we consider navigation as a social activity among drivers and navigators to improve design of such systems. We explore the implications of moving from a map-centered, individually-focused design paradigm to one based upon collaborative human interaction during the navigation task. We conducted a qualitative interaction design study of navigation among three types of teams: parents and their teenage children, couples, and unacquainted individuals. We found that collaboration varied among these different teams, and was influenced by social role, as well as the task role of driver or navigator. We also found that patterns of prompts, maneuvers, and confirmations varied among the three teams. We identify overarching practices that differ greatly from the literature on individual navigation. From these discoveries, we present design implications that can be used to inform future navigation systems.},
	booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	pages = {1261–1270},
	numpages = {10},
	keywords = {interaction design, in-car navigation, gps systems},
	location = {Atlanta, Georgia, USA},
	series = {CHI '10}
}
