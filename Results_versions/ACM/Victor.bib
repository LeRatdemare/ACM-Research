@inproceedings{10.1145/3341215.3358246,
	author = {Smilovitch, Michael and Lachman, Richard},
	title = {BirdQuestVR: A Cross-Platform Asymmetric Communication Game},
	year = {2019},
	isbn = {9781450368711},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3341215.3358246},
	doi = {10.1145/3341215.3358246},
	abstract = {BirdQuestVR is a cross-platform asymmetric communication game between one player in Virtual Reality and another on a mobile device. The game explores asymmetric co-operative gaming in a shared physical space, taking the physical surroundings of the VR user into account in its design. Asymmetric games feature different rules, abilities, or objectives for different players, generating unique and nuanced game experiences. Multiplayer asymmetric games in particular have been shown to increase teamwork and a collaborative mindset even after a play session has ended. Asymmetric design is commonplace in both digital and analog games but has yet to see widespread adoption in the emerging Virtual Reality (VR) gaming space. BirdQuestVR seeks to leverage the affordances of current consumer-grade VR headsets to build asymmetric gameplay around communication, embodied performance, and physical humour.},
	booktitle = {Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts},
	pages = {307–313},
	numpages = {7},
	keywords = {asymmetric, avatar embodiment, cross-platform, social play, virtual reality},
	location = {Barcelona, Spain},
	series = {CHI PLAY '19 Extended Abstracts}
}


@inproceedings{10.1145/3613905.3650824,
	author = {Kitson, Alexandra and Ahn, Sun Joo (Grace) and Gonzalez, Eric J and Panda, Payod and Isbister, Katherine and Gonzalez-Franco, Mar},
	title = {Virtual Games, Real Interactions: A Look at Cross-reality Asymmetrical Co-located Social Games},
	year = {2024},
	isbn = {9798400703317},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613905.3650824},
	doi = {10.1145/3613905.3650824},
	abstract = {Multiuser, multi-device environments in extended realities (XR) enable synchronous social interactions. With the freedom and flexibility to choose the most suitable device, we allow for inclusive environments where even spectators can be involved. However, existing research has mostly been conducted in controlled laboratory settings, which limits the applicability of the findings to naturalistic scenarios. We conducted a mixed methods study with social XR experts to explore situated and asymmetrical modalities in the context of XR gaming for enabling social interactions in naturalistic social settings, focusing on two games. We considered variations in available devices, spatial constraints, and users’ motivations and expertise. Our research suggests that asymmetrical interfaces may reduce barriers to entry for XR, support social connection, and promote cross-platform communication and collaboration. Together, our findings provoke critical discussions for future work on the effective deployment of asymmetrical interfaces in naturalistic scenarios and address potential technical, spatial, and social challenges.},
	booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
	articleno = {365},
	numpages = {9},
	keywords = {Asymmetric XR, Co-located XR, Hybrid Gaming, Multi-device, Multiplayer VR, Social XR, Virtual Reality},
	location = {<conf-loc>
<city>Honolulu</city>
<state>HI</state>
<country>USA</country>
</conf-loc>},
	series = {CHI EA '24}
}


@inproceedings{10.1145/2046396.2046410,
	author = {Goldman, Max},
	title = {Role-based interfaces for collaborative software development},
	year = {2011},
	isbn = {9781450310147},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2046396.2046410},
	doi = {10.1145/2046396.2046410},
	abstract = {Real-time collaboration between multiple simultaneous contributors to a shared document is full of both opportunities and pitfalls, as evidenced by decades of research and industry work in computer-supported cooperative work. In the domain of software engineering, collaboration is still generally achieved either via shared use of a single computer (e.g. pair programming) or with version control (and manual pushing and pulling of changes). By examining and designing for the different roles collaborating programmers play when working synchronously together, we can build real-time collaborative programming systems that make their collaboration more effective. And beyond simple shared editing, we can provide asymmetric, role-specific interfaces on their shared task. Collabode is a web-based IDE for collaborative programming with simultaneous editors that, along with several novel models for closely-collaborative software development, explores the potential of real-time cooperative programming.},
	booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
	pages = {23–26},
	numpages = {4},
	keywords = {collaboration, crowdsourcing, pair programming, software development},
	location = {Santa Barbara, California, USA},
	series = {UIST '11 Adjunct}
}


@inproceedings{10.1145/3379337.3415827,
	author = {Thoravi Kumaravel, Balasaravanan and Nguyen, Cuong and DiVerdi, Stephen and Hartmann, Bjoern},
	title = {TransceiVR: Bridging Asymmetrical Communication Between VR Users and External Collaborators},
	year = {2020},
	isbn = {9781450375146},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3379337.3415827},
	doi = {10.1145/3379337.3415827},
	abstract = {Virtual Reality (VR) users often need to work with other users, who observe them outside of VR using an external display. Communication between them is difficult; the VR user cannot see the external user's gestures, and the external user cannot see VR scene elements outside of the VR user's view. We carried out formative interviews with experts to understand these asymmetrical interactions and identify their goals and challenges. From this, we identify high-level system design goals to facilitate asymmetrical interactions and a corresponding space of implementation approaches based on the level of programmatic access to a VR application. We present TransceiVR, a system that utilizes VR platform APIs to enable asymmetric communication interfaces for third-party applications without requiring source code access. TransceiVR allows external users to explore the VR scene spatially or temporally, to annotate elements in the VR scene at correct depths, and to discuss via a shared static virtual display. An initial co-located user evaluation with 10 pairs shows that our system makes asymmetric collaborations in VR more effective and successful in terms of task time, error rate, and task load index. An informal evaluation with a remote expert gives additional insight on utility of features for real world tasks.},
	booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
	pages = {182–195},
	numpages = {14},
	keywords = {asymmetric interactions, collaboration, virtual reality},
	location = {Virtual Event, USA},
	series = {UIST '20}
}


@inproceedings{10.1145/3491101.3519816,
	author = {Lyu, Haohua and Vachha, Cyrus and Chen, Qianyi and Pyrinis, Odysseus and Liou, Avery and Thoravi Kumaravel, Balasaravanan and Hartmann, Bjoern},
	title = {WebTransceiVR: Asymmetrical Communication Between Multiple VR and Non-VR Users Online},
	year = {2022},
	isbn = {9781450391566},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3491101.3519816},
	doi = {10.1145/3491101.3519816},
	abstract = {Increasing adoption of Virtual Reality (VR) systems in various fields has created the need for collaborative work and communication. Today, asymmetric communication between a VR user and other non-VR users remains a challenge. The VR user cannot see the external non-VR users, and the non-VR users are restricted to the VR user’s first-person view. To address this, we propose WebTransceiVR, an asymmetric collaboration toolkit which when integrated into a VR application, allows multiple non-VR users to share the virtual space of the VR user. It allows external users to enter and be part of the VR application’s space through standard web browsers on mobile and computers. These external users can explore and interact with the other users, the VR scene as well as the VR user. WebTransceiVR also includes a cloud-based streaming solution that enables many passive spectators to view the scene through any of the active cameras. We conduct informal user testing to gain additional insights for future work.},
	booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
	articleno = {313},
	numpages = {7},
	keywords = {Asymmetric Communication, Virtual Reality},
	location = {New Orleans, LA, USA},
	series = {CHI EA '22}
}


@inproceedings{10.1145/3170427.3188598,
	author = {Kangas, Jari and Sand, Antti and Jokela, Tero and Piippo, Petri and Eskolin, Peter and Salmimaa, Marja and Raisamo, Roope},
	title = {Remote Expert for Assistance in a Physical Operational Task},
	year = {2018},
	isbn = {9781450356213},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3170427.3188598},
	doi = {10.1145/3170427.3188598},
	abstract = {We created a mobile asymmetric collaboration system that allows a remote expert to assist a nomadic operative in a maintenance task. We explored the use of a virtual reality headset and several cameras and controls that (1) are easy to setup by the operative on the site, (2) enable the expert to better understand the situation on the site (be more immersed) and (3) offer the expert various flexible interface options to study the environment and to act on the devices on the site independent of the operative. The preliminary results indicate that the expert appreciates the opportunity for 360 degreeview and the flexibility of using detailed controls on a close-up camera view.},
	booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
	pages = {1–6},
	numpages = {6},
	keywords = {remote collaboration, user interaction, virtual reality},
	location = {<conf-loc>, <city>Montreal QC</city>, <country>Canada</country>, </conf-loc>},
	series = {CHI EA '18}
}


@inproceedings{10.1145/3334480.3382869,
	author = {Li, Zhengqing and Chan, Liwei and Teo, Theophilus and Koike, Hideki},
	title = {OmniGlobeVR: A Collaborative 360° Communication System for VR},
	year = {2020},
	isbn = {9781450368193},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3334480.3382869},
	doi = {10.1145/3334480.3382869},
	abstract = {In this paper, we propose OmniGlobeVR, a novel collaboration tool based on an asymmetric cooperation system that supports communication and cooperation between a VR user (occupant) and multiple non-VR users (designers) across the virtual and physical platform. The OmniGlobeVR allows designer(s) to access the content of a VR space from any point of view using two view modes: 360° first-person mode and third-person mode. Furthermore, a proper interface of a shared gaze awareness cue is designed to enhance communication between the occupant and the designer(s). The system also has a face window feature that allows designer(s) to share their facial expressions and upper body gesture with the occupant in order to exchange and express information in a nonverbal context. Combined together, the OmniGlobeVR allows collaborators between the VR and non-VR platforms to cooperate while allowing designer(s) to easily access physical assets while working synchronously with the occupant in the VR space.},
	booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
	pages = {1–8},
	numpages = {8},
	keywords = {360-degree panorama, collaboration, mixed reality, spherical display, virtual reality},
	location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
	series = {CHI EA '20}
}


@inproceedings{10.1145/3379337.3415843,
	author = {Jansen, Pascal and Fischbach, Fabian and Gugenheimer, Jan and Stemasov, Evgeny and Frommel, Julian and Rukzio, Enrico},
	title = {ShARe: Enabling Co-Located Asymmetric Multi-User Interaction for Augmented Reality Head-Mounted Displays},
	year = {2020},
	isbn = {9781450375146},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3379337.3415843},
	doi = {10.1145/3379337.3415843},
	abstract = {Head-Mounted Displays (HMDs) are the dominant form of enabling Virtual Reality (VR) and Augmented Reality (AR) for personal use. One of the biggest challenges of HMDs is the exclusion of people in the vicinity, such as friends or family. While recent research on asymmetric interaction for VR HMDs has contributed to solving this problem in the VR domain, AR HMDs come with similar but also different problems, such as conflicting information in visualization through the HMD and projection. In this work, we propose ShARe, a modified AR HMD combined with a projector that can display augmented content onto planar surfaces to include the outside users (non-HMD users). To combat the challenge of conflicting visualization between augmented and projected content, ShARe visually aligns the content presented through the AR HMD with the projected content using an internal calibration procedure and a servo motor. Using marker tracking, non-HMD users are able to interact with the projected content using touch and gestures. To further explore the arising design space, we implemented three types of applications (collaborative game, competitive game, and external visualization). ShARe is a proof-of-concept system that showcases how AR HMDs can facilitate interaction with outside users to combat exclusion and instead foster rich, enjoyable social interactions.},
	booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
	pages = {459–471},
	numpages = {13},
	keywords = {asymmetric interaction, augmented reality, co-located, head-mounted displays, mixed reality},
	location = {Virtual Event, USA},
	series = {UIST '20}
}


@inproceedings{10.1145/3266037.3271643,
	author = {Elvezio, Carmine and Ling, Frank and Liu, Jen-Shuo and Feiner, Steven},
	title = {Collaborative Virtual Reality for Low-Latency Interaction},
	year = {2018},
	isbn = {9781450359498},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3266037.3271643},
	doi = {10.1145/3266037.3271643},
	abstract = {In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.},
	booktitle = {Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
	pages = {179–181},
	numpages = {3},
	keywords = {collaboration, games, rehabilitation, virtual reality},
	location = {Berlin, Germany},
	series = {UIST '18 Adjunct}
}


@inproceedings{10.1145/3411763.3451844,
	author = {Jing, Allison and May, Kieran William and Naeem, Mahnoor and Lee, Gun and Billinghurst, Mark},
	title = {eyemR-Vis: Using Bi-Directional Gaze Behavioural Cues to Improve Mixed Reality Remote Collaboration},
	year = {2021},
	isbn = {9781450380959},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411763.3451844},
	doi = {10.1145/3411763.3451844},
	abstract = {Gaze is one of the most important communication cues in face-to-face collaboration. However, in remote collaboration, sharing dynamic gaze information is more difficult. In this research, we investigate how sharing gaze behavioural cues can improve remote collaboration in a Mixed Reality (MR) environment. To do this, we developed eyemR-Vis, a 360 panoramic Mixed Reality remote collaboration system that shows gaze behavioural cues as bi-directional spatial virtual visualisations shared between a local host and a remote collaborator. Preliminary results from an exploratory study indicate that using virtual cues to visualise gaze behaviour has the potential to increase co-presence, improve gaze awareness, encourage collaboration, and is inclined to be less physically demanding or mentally distracting.},
	booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {283},
	numpages = {7},
	keywords = {CSCW, Gaze Visualisation, Human-Computer Interaction, Mixed Reality Remote Collaboration},
	location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
	series = {CHI EA '21}
}


@inproceedings{10.1145/3411763.3451545,
	author = {Jing, Allison and May, Kieran William and Naeem, Mahnoor and Lee, Gun and Billinghurst, Mark},
	title = {eyemR-Vis: A Mixed Reality System to Visualise Bi-Directional Gaze Behavioural Cues Between Remote Collaborators},
	year = {2021},
	isbn = {9781450380959},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411763.3451545},
	doi = {10.1145/3411763.3451545},
	abstract = {This demonstration shows eyemR-Vis, a 360 panoramic Mixed Reality collaboration system that translates gaze behavioural cues to bi-directional visualisations between a local host (AR) and a remote collaborator (VR). The system is designed to share dynamic gaze behavioural cues as bi-directional spatial virtual visualisations between a local host and a remote collaborator. This enables richer communication of gaze through four visualisation techniques: browse, focus, mutual-gaze, and fixated circle-map. Additionally, our system supports simple bi-directional avatar interaction as well as panoramic video zoom. This makes interaction in the normally constrained remote task space more flexible and relatively natural. By showing visual communication cues that are physically inaccessible in the remote task space through reallocating and visualising the existing ones, our system aims to provide a more engaging and effective remote collaboration experience.},
	booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {188},
	numpages = {4},
	keywords = {CSCW, Gaze Visualisation, Human-Computer Interaction, Mixed Reality Remote Collaboration},
	location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
	series = {CHI EA '21}
}


@inproceedings{10.1145/3474349.3480211,
	author = {Chen, Bo-Han and Wong, Sai-Keung and Chang, Wei-Che and Ping-Hao Fan, Roy},
	title = {Towards Social Interaction between 1st and 2nd Person Perspectives on Bodily Play},
	year = {2021},
	isbn = {9781450386555},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3474349.3480211},
	doi = {10.1145/3474349.3480211},
	abstract = {Bodily play, which is a productive social interaction for bonding social relationships, has positive impacts on self-efficacy, acute cognitive benefit, and emotion. However, most bodily play encourages players to enjoy their own experiences. There are limited researches on sharing players' perspectives to enhance players' empathy for understanding others. Thus, we propose an asymmetric two-person game in an immersive environment. This bodily play, which supports perspective-taking via the integration with the first- and second-perspectives, has a collaborative interface that allows users to share their physiological and emotional perspectives. Initial testing of the system shows that players can not only understand well the feeling and problems encountered by each other through sharing perspectives and information but also increases the social closeness of players and stimulates empathy after the interplay.},
	booktitle = {Adjunct Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology},
	pages = {1–3},
	numpages = {3},
	keywords = {Bodily play, collaborative exertion game, empathy, person perspectives, social interaction, virtual reality},
	location = {Virtual Event, USA},
	series = {UIST '21 Adjunct}
}


@inproceedings{10.1145/1866029.1866050,
	author = {Reilly, Derek F. and Rouzati, Hafez and Wu, Andy and Hwang, Jee Yeon and Brudvik, Jeremy and Edwards, W. Keith},
	title = {TwinSpace: an infrastructure for cross-reality team spaces},
	year = {2010},
	isbn = {9781450302715},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1866029.1866050},
	doi = {10.1145/1866029.1866050},
	abstract = {We introduce TwinSpace, a flexible software infrastructure for combining interactive workspaces and collaborative virtual worlds. Its design is grounded in the need to support deep connectivity and flexible mappings between virtual and real spaces to effectively support collaboration. This is achieved through a robust connectivity layer linking heterogeneous collections of physical and virtual devices and services, and a centralized service to manage and control mappings between physical and virtual. In this paper we motivate and present the architecture of TwinSpace, discuss our experiences and lessons learned in building a generic framework for collaborative cross-reality, and illustrate the architecture using two implemented examples that highlight its flexibility and range, and its support for rapid prototyping.},
	booktitle = {Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
	pages = {119–128},
	numpages = {10},
	keywords = {collaborative virtual environment, cross-reality, interactive room, ontology, rdf, smart room, tuplespace, virtual world},
	location = {New York, New York, USA},
	series = {UIST '10}
}


@inproceedings{10.1145/3311350.3347152,
	author = {Zhou, Zhuoming and M\'{a}rquez Segura, Elena and Duval, Jared and John, Michael and Isbister, Katherine},
	title = {Astaire: A Collaborative Mixed Reality Dance Game for Collocated Players},
	year = {2019},
	isbn = {9781450366885},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3311350.3347152},
	doi = {10.1145/3311350.3347152},
	abstract = {Despite the growth of Virtual Reality (VR), the design space of collocated social play in VR remains narrow. Here we present Astaire, a collaborative hybrid VR dance game for two players sharing an HTC Vive VR system. The game resulted from a design research process using embodied design methods, and drawing upon concepts in HCI and Play Design, including social affordances, and asymmetric and interdependent play. Here we present insights from a study playtesting Astaire alongside two VR games that inspired ours: Keep Talking and Nobody Explodes (KTNE), and Audioshield. We examined players' and spectators' enjoyment, and interpersonal relationships, which were self-reported higher for Astaire. Using data from semi-structured interviews, we foreground design elements that impacted our participants' play experience, grouped under the themes of balance of players' roles, the physicality afforded by the game, and the social experience enabled. Our work contributes to opening the design space of hybrid collocated VR--through our game, we surface inspirational design concepts in HCI, and share design knowledge gained during our design process.},
	booktitle = {Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
	pages = {5–18},
	numpages = {14},
	keywords = {alternative controllers, asymmetrical play, collocated play, embodied sketching, gestural excess, hybrid vr, mr, physicality, social play, social touch, spectatorship, vr},
	location = {<conf-loc>, <city>Barcelona</city>, <country>Spain</country>, </conf-loc>},
	series = {CHI PLAY '19}
}


@inproceedings{10.1145/985921.985926,
	author = {Brownholtz, Beth and Geyer, Werner and Muller, Michael J. and Wilcox, Eric and Millen, David R.},
	title = {Explorations in an activity-centric collaboration environment},
	year = {2004},
	isbn = {1581137036},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/985921.985926},
	doi = {10.1145/985921.985926},
	abstract = {This demonstration presents a new hybrid collaboration technology that partakes of selected qualities of informal, ad hoc, easy-to-initiate collaborative tools, and more formal, structured, and disciplined collaborative applications. Our approach focuses on the support of lightweight, informally structured, opportunistic activities featuring heterogeneous threads of shared objects with dynamic membership as well as blended synchronous and asynchronous collaboration. We will introduce the system, and then invite audience members to use it in several exercises.},
	booktitle = {CHI '04 Extended Abstracts on Human Factors in Computing Systems},
	pages = {767–768},
	numpages = {2},
	keywords = {CSCW, activity-centric collaboration, computer-mediated communication},
	location = {Vienna, Austria},
	series = {CHI EA '04}
}


@inproceedings{10.1145/571985.572016,
	author = {Booth, Kellogg S. and Fisher, Brian D. and Lin, Chi Jui Raymond and Argue, Ritchie},
	title = {The "mighty mouse" multi-screen collaboration tool},
	year = {2002},
	isbn = {1581134886},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/571985.572016},
	doi = {10.1145/571985.572016},
	abstract = {Many computer operating systems provide seamless support for multiple display screens, but there are few cross-platform tools for collaborative use of multiple computers in a shared display environment. Mighty Mouse is a novel groupware tool built on the public domain VNC protocol. It is tailored specifically for face-to-face collaboration where multiple heterogeneous computers (usually laptops) are viewed simultaneously (usually via projectors) by people working together on a variety of applications under various operating systems. Mighty Mouse uses only the remote input capability of VNC, but enhances this with various features to support flexible movement between the various platforms, "floor control" to facilitate smooth collaboration, and customization features to accommodate different user, platform, and application preferences in a relatively seamless manner. The design rationale arises from specific observations about how people collaborate in meetings, which allows certain simplifying assumptions to be made in the implementation.},
	booktitle = {Proceedings of the 15th Annual ACM Symposium on User Interface Software and Technology},
	pages = {209–212},
	numpages = {4},
	keywords = {collaboration, cut-and-paste, keyboard mappings, low-fidelity prototyping, single display groupware, virtual network computing},
	location = {Paris, France},
	series = {UIST '02}
}


@inproceedings{10.1145/3332165.3347938,
	author = {Wu, Te-Yen and Gong, Jun and Seyed, Teddy and Yang, Xing-Dong},
	title = {Proxino: Enabling Prototyping of Virtual Circuits with Physical Proxies},
	year = {2019},
	isbn = {9781450368162},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3332165.3347938},
	doi = {10.1145/3332165.3347938},
	abstract = {We propose blending the virtual and physical worlds for prototyping circuits using physical proxies. With physical proxies, real-world components (e.g. a motor, or light sensor) can be used with a virtual counterpart for a circuit designed in software. We demonstrate this concept in Proxino, and elucidate the new scenarios it enables for makers, such as remote collaboration with physically distributed electronics components. We compared our hybrid system and its output with designs of real circuits to determine the difference through a system evaluation and observed minimal differences. We then present the results of an informal study with 9 users, where we gathered feedback on the effectiveness of our system in different working conditions (with a desktop, using a mobile, and with a remote collaborator). We conclude by sharing our lessons learned from our system and discuss directions for future research that blend physical and virtual prototyping for electronic circuits.},
	booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
	pages = {121–132},
	numpages = {12},
	keywords = {breadboard, circuit construction, remote collaboration},
	location = {New Orleans, LA, USA},
	series = {UIST '19}
}


@inproceedings{10.1145/3332165.3347948,
	author = {Makiguchi, Motohiro and Sakamoto, Daisuke and Takada, Hideaki and Honda, Kengo and Ono, Tetsuo},
	title = {Interactive 360-Degree Glasses-Free Tabletop 3D Display},
	year = {2019},
	isbn = {9781450368162},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3332165.3347948},
	doi = {10.1145/3332165.3347948},
	abstract = {We present an interactive 360-degree tabletop display system for collaborative work around a round table. Users are able to see 3D objects on the tabletop display anywhere around the table without 3D glasses. The system uses a visual perceptual mechanism for smooth motion parallax in the horizontal direction with fewer projectors than previous works. A 360-degree camera mounted above the table and image recognition software detects users' positions around the table and the heights of their faces (eyes) as they move around the table in real-time. Those mechanics help display correct vertical and horizontal direction motion parallax for different users simultaneously. Our system also has a user interaction function with a tablet device that manipulates 3D objects displayed on the table. These functions support collaborative work and communication between users. We implemented a prototype system and demonstrated the collaborative features of the 360-degree tabletop display system.},
	booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
	pages = {625–637},
	numpages = {13},
	keywords = {autostereoscopic 3d, linear blending, smooth motion parallax, tabletop display},
	location = {New Orleans, LA, USA},
	series = {UIST '19}
}


@inproceedings{10.1145/1358628.1358636,
	author = {Tan, Amy and Kondoz, Ahmet M.},
	title = {Barriers to virtual collaboration},
	year = {2008},
	isbn = {9781605580128},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1358628.1358636},
	doi = {10.1145/1358628.1358636},
	abstract = {This paper reports on the implementation and use of a virtual collaboration system - a virtual collaborative desk (VCD) that has been introduced to a software design team in an organizational context. Virtual collaboration systems are complex and can be considered as social-technical systems, oftentimes encompassing several layers of both technical and social issues. If this multi-layered social-technical system is to work effectively and provide a dependable service, then all the layers must be well understood and structured accordingly. Otherwise, these layers can become barriers to virtual collaboration if they impede the collaborating users of a virtual team from attaining their goals. An amalgamation of principles from life-cycle and ethnomethodologically informed ethnography approaches in the evaluation of a virtual collaborative system is demonstrated in a case-study to enable researchers to understand what these issues are and how the different types of issues can prevent effective virtual collaboration.},
	booktitle = {CHI '08 Extended Abstracts on Human Factors in Computing Systems},
	pages = {2045–2052},
	numpages = {8},
	keywords = {cscw, ethnography, life-cyle, longitudinal case study, usability, virtual collaboration, virtual teams.},
	location = {Florence, Italy},
	series = {CHI EA '08}
}


@inproceedings{10.1145/2468356.2479494,
	author = {He, Li},
	title = {Couple collaboration: a design research exploration},
	year = {2013},
	isbn = {9781450319522},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2468356.2479494},
	doi = {10.1145/2468356.2479494},
	abstract = {In the past few years, technologies designed to mediate intimacy have been growing, especially devices that support intimate acts and connectivity between geographically separated couples. However, romantic intimacy has several components, and the majority of couples are co-located, suggesting a design space to be explored. The current study aims to investigate how local intimate couples collaborate with each other to accomplish collective tasks in their daily life, and how couple collaboration may differ from teamwork within the workplace. The research process and findings will be discussed, and design implications for intimate technology will be provided. These insights could be used to explore novel design opportunities to mediate cognitive intimacy and mutuality within a couple.},
	booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
	pages = {2689–2694},
	numpages = {6},
	keywords = {collaboration, couples, intimacy},
	location = {Paris, France},
	series = {CHI EA '13}
}


@inproceedings{10.1145/2047196.2047207,
	author = {Hincapi\'{e}-Ramos, Juan David and Voida, Stephen and Mark, Gloria},
	title = {A design space analysis of availability-sharing systems},
	year = {2011},
	isbn = {9781450307161},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2047196.2047207},
	doi = {10.1145/2047196.2047207},
	abstract = {Workplace collaboration often requires interruptions, which can happen at inopportune times. Designing a successful availability-sharing system requires finding the right balance to optimize the benefits and reduce costs for both the interrupter and interruptee. In this paper, we examine the design space of availability-sharing systems and identify six relevant design dimensions: abstraction, presentation, information delivery, symmetry, obtrusiveness and temporal gradient. We describe these dimensions in terms of the tensions between interrupters and interruptees revealed in previous studies of workplace collaboration and deployments of awareness systems. As a demonstration of the utility of our design space, we introduce InterruptMe, a novel availability-sharing system that represents a previously unexplored point in the design space and that balances the tensions between interrupters and interruptees. InterruptMe differs from previous systems in that it displays availability information only when needed by monitoring implicit inputs from the system's users, implements a traceable asymmetry structure, and introduces the notion of per-communications channel availability.},
	booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
	pages = {85–96},
	numpages = {12},
	keywords = {availability, interruptibility, workplace awareness},
	location = {Santa Barbara, California, USA},
	series = {UIST '11}
}


@inproceedings{10.1145/3266037.3266086,
	author = {Cha, Yoonjeong and Nam, Sungu and Yi, Mun Yong and Jeong, Jaeseung and Woo, Woontack},
	title = {Augmented Collaboration in Shared Space Design with Shared Attention and Manipulation},
	year = {2018},
	isbn = {9781450359498},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3266037.3266086},
	doi = {10.1145/3266037.3266086},
	abstract = {Augmented collaboration in a shared house design scenario has been studied widely with various approaches. However, those studies did not consider human perception. Our goal is to lower the user's perceptual load for augmented collaboration in shared space design scenarios. Applying attention theories, we implemented shared head gaze, shared selected object, and collaborative manipulation features in our system in two different versions with HoloLens. To investigate whether user perceptions of the two different versions differ, we conducted an experiment with 18 participants (9 pairs) and conducted a survey and semi-structured interviews. The results did not show significant differences between the two versions, but produced interesting insights. Based on the findings, we provide design guidelines for collaborative AR systems.},
	booktitle = {Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
	pages = {13–15},
	numpages = {3},
	keywords = {collaborative ar, human perception, shared space design},
	location = {Berlin, Germany},
	series = {UIST '18 Adjunct}
}


@inproceedings{10.5555/850976.854976,
	author = {Fiorentino, Michele and de Amicis, Raffaele and Monno, Giuseppe and Stork, Andre},
	title = {Spacedesign: A Mixed Reality Workspace for Aesthetic Industrial Design},
	year = {2002},
	isbn = {0769517811},
	publisher = {IEEE Computer Society},
	address = {USA},
	abstract = {Spacedesign is an innovative Mixed Reality (MR) application addressed to aesthetic design of free form curves and surfaces. It is a unique and comprehensive approach which uses task-specific configurations to support the design workflow from concept to mock-up evaluation and review. The first-phase conceptual design benefits from a workbench-like 3-D display for free hand sketching, surfacing and engineering visualization. Semi-transparentstereo glasses augment the pre-production physical prototype by additional shapes, textures and annotations. Both workspaces share a common interface and allow collaboration and cooperation between different experts, who can configure the system for the specific task. A faster design workflow and CAD data consistency can be thus naturally achieved. Tests andcollaborations with designers, mainly from automotive industry, are providing systematic feedback for this on-going research. As far as the authors are concerned, there is no known similar approach that integrates the creation and editing phase of 3D curves and surfaces in Virtual and Augmented Reality (VR/AR). Herein we see the major contribution of our new application.},
	booktitle = {Proceedings of the 1st International Symposium on Mixed and Augmented Reality},
	pages = {86},
	series = {ISMAR '02}
}


@inproceedings{10.1145/2380116.2380151,
	author = {Karnik, Abhijit and Martinez Plasencia, Diego and Mayol-Cuevas, Walterio and Subramanian, Sriram},
	title = {PiVOT: personalized view-overlays for tabletops},
	year = {2012},
	isbn = {9781450315807},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2380116.2380151},
	doi = {10.1145/2380116.2380151},
	abstract = {We present PiVOT, a tabletop system aimed at supporting mixed-focus collaborative tasks. Through two view-zones, PiVOT provides personalized views to individual users while presenting an unaffected and unobstructed shared view to all users. The system supports multiple personalized views which can be present at the same spatial location and yet be only visible to the users it belongs to. The system also allows the creation of personal views that can be either 2D or (auto-stereoscopic) 3D images. We first discuss the motivation and the different implementation principles required for realizing such a system, before exploring different designs able to address the seemingly opposing challenges of shared and personalized views. We then implement and evaluate a sample prototype to validate our design ideas and present a set of sample applications to demonstrate the utility of the system.},
	booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
	pages = {271–280},
	numpages = {10},
	keywords = {tabletop, multi-view, multi-user, lumisty},
	location = {Cambridge, Massachusetts, USA},
	series = {UIST '12}
}


@inproceedings{10.1145/3131785.3131841,
	author = {Lee, Sang Won},
	title = {Hybrid Use of Asynchronous and Synchronous Interaction for Collaborative Creation},
	year = {2017},
	isbn = {9781450354196},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3131785.3131841},
	doi = {10.1145/3131785.3131841},
	abstract = {My dissertation is aimed at enabling people to collaborate to create complex artifacts: for example, to develop software, sketch GUI prototypes, play music together, or write a novel. Such creative processes are not well defined and can evolve dynamically. We introduce interactive systems that help users collaborate and communicate in the open-ended process. In particular, we investigate the benefits of both integrating asynchronous interactions into real-time collaborations and of having real time components in asynchronous collaborative settings. The systems provide tools that combine the two different types of interaction techniques, and we validate them via user study, participatory performing arts, and the online deployments of systems and crowdsourced tasks. The hybrid methods are designed to help users recover collaborative context, make the process approachable to nonexperts, collaborate online crowds on demand in real-time, and sustain liveness during collaboration. The dissertation will result in cross-domain knowledge in designing collaborative systems and it will help us create a framework for future intelligent systems that will help people solve more complex tasks effectively.},
	booktitle = {Adjunct Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
	pages = {95–98},
	numpages = {4},
	keywords = {groupware, crowdsourcing, creativity, collaboration},
	location = {Qu\'{e}bec City, QC, Canada},
	series = {UIST '17 Adjunct}
}


@inproceedings{10.1145/2851581.2892300,
	author = {K\"{u}hn, Romina and Korzetz, Mandy and B\"{u}schel, Lukas and Korger, Christina and Manja, Philip and Schlegel, Thomas},
	title = {Natural Voting Interactions for Collaborative Work with Mobile Devices},
	year = {2016},
	isbn = {9781450340823},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2851581.2892300},
	doi = {10.1145/2851581.2892300},
	abstract = {In collocated collaborative creativity work, such as mind mapping or other brainstorming sessions, a group of persons has to solve different tasks as well as discuss and decide on one of the various proposed solutions at the same place. Since mobile devices are increasingly used in collaborative work, these tasks also can be performed digitally. This work addresses the process of voting for one solution out of several proposals. We provide intuitive multi-device interactions for a seamless ranking and voting using multiple mobile devices. They each represent one solution to a given task. Our proposed voting interactions include turning devices and rearranging their order according to their ratings. We demonstrate the feasibility of our interactions by implementing an application prototype. They facilitate the involvement of mobile devices in collaborative work as well as the support of users during voting and ranking processes.},
	booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
	pages = {2570–2575},
	numpages = {6},
	keywords = {user experience, multi-device, interaction design, collaboration},
	location = {San Jose, California, USA},
	series = {CHI EA '16}
}


@inproceedings{10.1145/3266037.3266102,
	author = {Feick, Martin and Tang, Anthony and Bateman, Scott},
	title = {Mixed-Reality for Object-Focused Remote Collaboration},
	year = {2018},
	isbn = {9781450359498},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3266037.3266102},
	doi = {10.1145/3266037.3266102},
	abstract = {In this paper we outline the design of a mixed-reality system to support object-focused remote collaboration. Here, being able to adjust collaborators' perspectives on the object as well as understand one another's perspective is essential to support effective collaboration over distance. We propose a low-cost mixed-reality system that allows users to: (1) quickly align and understand each other's perspective; (2) explore objects independently from one another, and (3) render gestures in the remote's workspace. In this work, we focus on the expert's role and we introduce an interaction technique allowing users to quickly manipulation 3D virtual objects in space.},
	booktitle = {Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
	pages = {63–65},
	numpages = {3},
	keywords = {vr, object-focused remote collaboration, mixed reality, ar},
	location = {Berlin, Germany},
	series = {UIST '18 Adjunct}
}


@inproceedings{10.1145/1056808.1056880,
	author = {Blevis, Eli and Lim, Youn-Kyung and Ozakca, Muzaffer and Aneja, Shweta},
	title = {Designing interactivity for the specific context of designerly collaborations},
	year = {2005},
	isbn = {1595930027},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1056808.1056880},
	doi = {10.1145/1056808.1056880},
	abstract = {We report on one of several exploratory, formulative studies that we conducted to help inform the thoughtful use of mixed physical and digital interactivity in a wiki-based system targeted at design collaborations. This study had two parts, both involving bar-coded cards, a bar-code scanner, and a projector. One part emphasized a creative, synthesis-oriented design activity. The other part emphasized a decision-making design activity.We learned that our method of designing the physical cards and the variance in the types of information we included on the cards significantly affected the collaborative behaviors. We also learned that the extension of interactivity from the digital to the physical world and back again successfully scaffolded both creative and decision-making activities in our context, although with some very notable differences in interactive behaviors between the specific activities. This latter point notwithstanding, we learned that allowing high-resolution, small size physical cards to be arrayed and manipulated on a shared surface matters much more for the purposes of scaffolding the collaborative activities than the ability to scan and project large-size, low-resolution facsimiles of the same information, in specific contexts of collaborative story-creation and decision making.},
	booktitle = {CHI '05 Extended Abstracts on Human Factors in Computing Systems},
	pages = {1216–1219},
	numpages = {4},
	keywords = {presentation technologies, naturalistic studies, mixed physical and digital interactivity, exploratory studies, design studies, design, collocated (co-located) collaborative design},
	location = {Portland, OR, USA},
	series = {CHI EA '05}
}


@inproceedings{10.1145/2468356.2468832,
	author = {Jennex, Matthew and Louraine, Stephanie and Miller, Stephen and Rosenzweig Castillo, Ang\'{e}lica},
	title = {Let's chalk! strengthening communities through play},
	year = {2013},
	isbn = {9781450319522},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2468356.2468832},
	doi = {10.1145/2468356.2468832},
	abstract = {Greenways (public outdoor walking and biking paths) are unique communities ripe for collaboration. We propose Let's Chalk: a system for collaborative distance sidewalk chalk play that connects greenways in different locations to create an aesthetic experience that changes perspectives via reflection and strengthened community.},
	booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
	pages = {2573–2578},
	numpages = {6},
	keywords = {play, greenways, distance collaboration, community, chalk, aesthetic experience},
	location = {Paris, France},
	series = {CHI EA '13}
}


@inproceedings{10.1145/3544549.3585589,
	author = {El Ali, Abdallah and Stepanova, Ekaterina R. and Palande, Shalvi and Mader, Angelika and Cesar, Pablo and Jansen, Kaspar},
	title = {BreatheWithMe: Exploring Visual and Vibrotactile Displays for Social Breath Awareness during Colocated, Collaborative Tasks},
	year = {2023},
	isbn = {9781450394222},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3544549.3585589},
	doi = {10.1145/3544549.3585589},
	abstract = {Sharing breathing signals has the capacity to provide insights into hidden experiences and enhance interpersonal communication. However, it remains unclear how the modality of breath signals (visual, haptic) is socially interpreted during collaborative tasks. In this mixed-methods study, we design and evaluate BreatheWithMe, a prototype for real-time sharing and receiving of breathing signals through visual, vibrotactile, or visual-vibrotactile modalities. In a within-subjects study (15 pairs), we investigated the effects of modality on breathing synchrony, social presence, and overall user experience. Key findings showed: (a) there were no significant effects of visualization modality on breathing synchrony, only on deliberate music-driven synchronization; (b) visual modality was preferred over vibrotactile feedback, despite no differences across social presence dimensions; (c) BreatheWithMe was perceived to be an insightful window into others, however included data exposure and social acceptability concerns. We contribute insights into the design of multi-modal real-time breathing visualization systems for colocated, collaborative tasks.},
	booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
	articleno = {58},
	numpages = {8},
	keywords = {Breathing, LED, awareness, collaborative, dyadic, haptics, mutltimodal, respiration, social interactions, visual},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	series = {CHI EA '23}
}


@inproceedings{10.1145/3526113.3545637,
	author = {Peng, Yi-Hao and Wu, Jason and Bigham, Jeffrey and Pavel, Amy},
	title = {Diffscriber: Describing Visual Design Changes to Support Mixed-Ability Collaborative Presentation Authoring},
	year = {2022},
	isbn = {9781450393201},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3526113.3545637},
	doi = {10.1145/3526113.3545637},
	abstract = {Visual slide-based presentations are ubiquitous, yet slide authoring tools are largely inaccessible to people who are blind or visually impaired (BVI). When authoring presentations, the 9 BVI presenters in our formative study usually work with sighted collaborators to produce visual slides based on the text content they produce. While BVI presenters valued collaborators’ visual design skill, the collaborators often felt they could not fully review and provide feedback on the visual changes that were made. We present Diffscriber, a system that identifies and describes changes to a slide’s content, layout, and style for presentation authoring. Using our system, BVI presentation authors can efficiently review changes to their presentation by navigating either a summary of high-level changes or individual slide elements. To learn more about changes of interest, presenters can use a generated change hierarchy to navigate to lower-level change details and element styles. BVI presenters using Diffscriber were able to identify slide design changes and provide feedback more easily as compared to using only the slides alone. More broadly, Diffscriber illustrates how advances in detecting and describing visual differences can improve mixed-ability collaboration.},
	booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
	articleno = {35},
	numpages = {13},
	keywords = {Visual design, Slides, Presentation, Multimedia creation, Change descriptions and captioning, Authoring tools, Accessibility},
	location = {Bend, OR, USA},
	series = {UIST '22}
}


@inproceedings{10.1145/3544549.3583891,
	author = {B\"{u}schel, Wolfgang and Krug, Katja and Klamka, Konstantin and Dachselt, Raimund},
	title = {Demonstrating CleAR Sight: Transparent Interaction Panels for Augmented Reality},
	year = {2023},
	isbn = {9781450394222},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3544549.3583891},
	doi = {10.1145/3544549.3583891},
	abstract = {In this work, we demonstrate our concepts for transparent interaction panels in augmented-reality environments. Mobile devices can support interaction with head-mounted displays by providing additional input channels, such as touch &amp; pen input and spatial device input, and also an additional, personal display. However, occlusion of the physical context, other people, or the virtual content can be problematic. To address this, we previously introduced CleAR Sight, a concept and research platform for transparent interaction panels to support interaction in HMD-based mixed reality. Here, we will demonstrate the different interaction and visualization techniques supported in CleAR Sight that facilitate basic manipulation, data exploration, and sketching &amp; annotation for various use cases such as 3D volume visualization, collaborative data analysis, and smart home control.},
	booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
	articleno = {432},
	numpages = {5},
	keywords = {augmented reality, human-computer interaction, transparent displays, visualization},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	series = {CHI EA '23}
}


@inproceedings{10.1145/3411763.3451561,
	author = {Radu, Iulian and Joy, Tugce and Schneider, Bertrand},
	title = {Virtual Makerspaces: Merging AR/VR/MR to Enable Remote Collaborations in Physical Maker Activities},
	year = {2021},
	isbn = {9781450380959},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411763.3451561},
	doi = {10.1145/3411763.3451561},
	abstract = {We present a mixed-reality system for remote collaborations, where collaborators can discuss, explore, create and learn about 3D physical objects. The system combines Hololens augmented reality, 3D Kinect cameras, PC and virtual reality interfaces, into a virtual space that hosts remote collaborators, and physical &amp; virtual objects.When talented people have access to fabrication tools and expertise, incredible inventions can be manifested to solve local and global problems. Because of this, the “maker movement” is a growing phenomenon, manifested through the increased number of maker spaces in many affluent communities. However, many talented individuals lack access to resources in their local communities, and collaboration opportunities with remote experts are wasted due to the limitations of current teleconferencing systems. We present a mixed-reality (MR) system for enabling remote collaborations in the context of maker activities, that allows groups of students and instructors to discuss, explore, create and learn about physical objects. The system combines augmented reality (AR) headsets, 3D cameras, PC and virtual reality (VR) interfaces, into a virtual space that contains multiple remote students, instructors, physical and virtual objects. Remote students can see a real-time 3D scan of the on-site user's physical environment, and the virtual avatars of other students. The system can support learning and exploration by showing virtual overlays on real objects (ex: showing a physical robot's sensor data or internal circuitry) while responding to real-time manipulation of physical objects by the on-site user; and can support design activities by allowing remote and local participants to annotate physical objects with virtual drawings and virtual models. This platform is being developed as an open-source project, and we are currently building applications with the intention to deploy in hybrid makerspace classrooms involving on-site and remote students.},
	booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {202},
	numpages = {5},
	keywords = {remote education, mixed reality, maker spaces, collaborative learning},
	location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
	series = {CHI EA '21}
}


@inproceedings{10.1145/3613905.3638174,
	author = {Seo, Jwawon},
	title = {Motives and Role of Psychological Ownership in AR Workspaces for Remote Collaboration},
	year = {2024},
	isbn = {9798400703317},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613905.3638174},
	doi = {10.1145/3613905.3638174},
	abstract = {Augmented Reality (AR) systems have been proposed as solutions to reinforce remote collaboration over performing physical tasks by integrating digital information into the physical environment. However, there is a fundamental gap that arises from the difference in the degree to which remote and local workers interact with virtual and physical objects, which is attributed to differing senses of ownership over these objects. Addressing this gap, my thesis investigates how collaborative functionality affects psychological ownership in shared AR workspaces. Specifically, the study examines how the ability to modify annotations shapes Individual and Collective Psychological Ownership (IPO and CPO). Furthermore, I explore how IPO and CPO mediate the effect of collaborative functionality on the outcomes of remote collaboration. My overall goal is to extend the theoretical understanding of psychological ownership, offering insights into the dynamics of shared AR environments by focusing on the interplay between physical and virtual elements.},
	booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
	articleno = {438},
	numpages = {5},
	keywords = {augmented reality, psychological ownership, remote collaboration, shared information, social interaction},
	location = {<conf-loc>
<city>Honolulu</city>
<state>HI</state>
<country>USA</country>
</conf-loc>},
	series = {CHI EA '24}
}


@inproceedings{10.1145/3027063.3052966,
	author = {Tunnell, Harry and Faiola, Anthony and Bolchini, Davide},
	title = {Guidelines to Incorporate a Clinician User Experience (UX) into the Design of Patient-Operated mHealth},
	year = {2017},
	isbn = {9781450346566},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3027063.3052966},
	doi = {10.1145/3027063.3052966},
	abstract = {This interactivity demonstration paper highlights how a patient-operated mHealth solution can be designed to improve clinician understanding of a patient's health status during a first face-to-face encounter. Patients can use smartphones to retrieve difficult-to-recall-from memory personal health information. This provides an opportunity to improve patient-clinician collaboration. To explore this idea, a mixed method study with 12 clinicians in a simulated encounter was conducted. A smartphone personal health record was prototyped and used for an experimental study. Communication, efficiency, and effectiveness was improved for clinicians who experienced the prototype. Study outcomes included a validated set of design guidelines for mHealth tools to support better patient-clinician communication.},
	booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
	pages = {385–388},
	numpages = {4},
	keywords = {secondary user experience (ux), personal health record (phr), mhealth, health information technology (hit), design guidelines},
	location = {<conf-loc>, <city>Denver</city>, <state>Colorado</state>, <country>USA</country>, </conf-loc>},
	series = {CHI EA '17}
}


@inproceedings{10.1145/634067.634107,
	author = {Salcedo, Manuel Romero},
	title = {Supporting group awareness in alliance},
	year = {2001},
	isbn = {1581133405},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/634067.634107},
	doi = {10.1145/634067.634107},
	abstract = {An important problem in several scientific and technical fields, and in general in the whole human knowledge, is the development and maintenance of common documents. Alliance is a collaborative writing system which allow co-authors, spread out across different locations, to work together sharing common documents. In order for collaboration to succeed and to be efficient, co-authors need to be aware of each other's activities (actions, intentions, presence, etc.). This position paper focus on describing the protocol developed for supporting group awareness in Alliance. A discussion of how well this protocol works in a real operational context is included.},
	booktitle = {CHI '01 Extended Abstracts on Human Factors in Computing Systems},
	pages = {61–62},
	numpages = {2},
	keywords = {groupware system, group awareness, collaborative writing system, CSCW},
	location = {Seattle, Washington},
	series = {CHI EA '01}
}


@inproceedings{10.1145/3383668.3419953,
	author = {Daronnat, Sylvain},
	title = {Human-Agent Trust Relationships in a Real-Time Collaborative Game},
	year = {2020},
	isbn = {9781450375870},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3383668.3419953},
	doi = {10.1145/3383668.3419953},
	abstract = {Collaborative virtual agents are often deployed to help users make decisions in real-time. For this collaboration to work, users must adequately trust the agents that they are interacting with. In my research, I use a game where human-agent interactions are recorded via a logging system and survey instruments in order to explore this trust relationship. I then study the impact that different agents have on reliance, performance, cognitive load and trust. I seek to understand which aspects of an agent influence the development of trust the most. With this research, I hope to pave the way for trust-aware agents, capable of adapting their behaviours to users in real-time.},
	booktitle = {Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play},
	pages = {18–20},
	numpages = {3},
	keywords = {trust, task difficulty, reliance, performance, hci, collaborative game, cognitive load, agents},
	location = {Virtual Event, Canada},
	series = {CHI PLAY '20}
}


@inproceedings{10.1109/ISMAR.2009.5336522,
	author = {Nilsson, Susanna and Johansson, Bjorn and Jonsson, Arne},
	title = {Using AR to support cross-organisational collaboration in dynamic tasks},
	year = {2009},
	isbn = {9781424453900},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://doi.org/10.1109/ISMAR.2009.5336522},
	doi = {10.1109/ISMAR.2009.5336522},
	abstract = {This paper presents a study where Augmented Reality (AR) technology has been used as a tool for supporting collaboration between the rescue services, the police and military personnel in a crisis management scenario. There are few studies on how AR systems should be designed to improve cooperation between actors from different organizations while at the same time support individual needs. In the present study an AR system was utilized for supporting joint planning tasks by providing organisation-specific views of a shared working. The study involved a simulated emergency event conducted in close to real settings with representatives from the organisations for which the system is developed. As a baseline, a series of trials without the AR system was carried out. Results show that the users were positive towards the AR system, and would like to use it in real work. They also experience some performance benefits of using the AR system compared to their traditional tools. Finally, the problem of designing for collaborative work as well as the benefits of using an iterative design processes is discussed.},
	booktitle = {Proceedings of the 2009 8th IEEE International Symposium on Mixed and Augmented Reality},
	pages = {3–12},
	numpages = {10},
	series = {ISMAR '09}
}


@inproceedings{10.1109/ISMAR.2009.5336478,
	author = {Ai, Zhuming and Livingston, Mark A.},
	title = {Integration of georegistered information on a virtual globe},
	year = {2009},
	isbn = {9781424453900},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://doi.org/10.1109/ISMAR.2009.5336478},
	doi = {10.1109/ISMAR.2009.5336478},
	abstract = {In collaborative augmented reality (AR) missions, much georegistered information is collected and sent to a command and control center. This paper describes the concept and prototypical implementation of a mixed reality (MR) based system that integrates georegistered information from AR systems and other sources on a virtual globe. The application can be used for a command and control center to monitor the field operation where multiple AR users are engaging in a collaborative mission. Google Earth is used to demonstrate the system, which integrates georegistered icons, live video streams from field operators or surveillance cameras, 3D models, and satellite or aerial photos into one MR environment.},
	booktitle = {Proceedings of the 2009 8th IEEE International Symposium on Mixed and Augmented Reality},
	pages = {169–170},
	numpages = {2},
	series = {ISMAR '09}
}


@inproceedings{10.1145/3613905.3643978,
	author = {Xu, Jiangnan and Papangelis, Konstantinos and Tigwell, Garreth W. and Lalone, Nicolas and Zhou, Pengyuan and Saker, Michael and Chamberlain, Alan and Dunham, John and Luna, Sanzida Mojib and Schwartz, David},
	title = {Spatial Computing: Defining the Vision for the Future},
	year = {2024},
	isbn = {9798400703317},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613905.3643978},
	doi = {10.1145/3613905.3643978},
	abstract = {Spatial Computing integrates technologies like Mixed Reality, Artificial Intelligence, and the Global Positioning System, enabling immersive, natural, and intelligent multi-modal interactions in physical and virtual spaces. With the huge potential to benefit users in multiple scenarios (e.g., gaming, education, design, and healthcare), Spatial Computing is growing at an incredible rate, with different attempts to define and capitalize on the growth in both industry and academia. Beyond the location, shape, and relationship of geographic objects, Spatial Computing delves into the social, emotional, and cognitive dimensions of shared spaces. However, the human-computer interaction research on Spatial Computing faces a notable gap, particularly in user experience areas like collaboration, trust, ethics, and accessibility. Addressing this gap, this Special Interest Group (SIG) seeks to unite experts from academia and industry to explore current and future trends in Spatial Computing.},
	booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
	articleno = {583},
	numpages = {4},
	keywords = {Artificial Intelligent, Augmented Reality, Human-Computer Interaction, Location-based Media, Mixed Reality, Spatial Computing, Special Interests Group},
	location = {<conf-loc>
<city>Honolulu</city>
<state>HI</state>
<country>USA</country>
</conf-loc>},
	series = {CHI EA '24}
}


@inproceedings{10.1145/2559206.2581333,
	author = {Szigeti, Steve James and Stevens, Anne and Tu, Robert and Jofre, Ana and Gebhardt, Alex and Chevalier, Fanny and Lee, Jonathan and Diamond, Sara L.},
	title = {Output to input: concepts for physical data representations and tactile user interfaces},
	year = {2014},
	isbn = {9781450324748},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2559206.2581333},
	doi = {10.1145/2559206.2581333},
	abstract = {Tangible user interfaces and physical representations of data are both promising approaches to improving insights derived from large data sets. Interactive tangible representations of data, which seamlessly combine those two approaches, potentially take advantage of cognitive processes, data representations, and interactions not supported by current approaches and may enhance collaboration. This paper describes user evaluations of two sets of prototypes comprised of physical blocks to represent data. One set uses six blocks of identical dimensions and another set uses six blocks with different dimensions. The objectives of this pilot study include (i) making general observations on how users interact with the two prototypes, (ii) making observations on the role these tangible interfaces play in collaboration, and (iii) comparing the two sets of tangible interfaces. We report on the results of the study and discuss future work. make general observations on how users interacted with the tangible interfaces; two, to make observations on the role the tangible interfaces play in collaboration; and three, to compare the two sets of tangible interfaces with one another. We report on the results of the study and discuss future work.},
	booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
	pages = {1813–1818},
	numpages = {6},
	keywords = {tangible computing, physical visualization, nui, human factors, design},
	location = {Toronto, Ontario, Canada},
	series = {CHI EA '14}
}


@inproceedings{10.1145/1240866.1241001,
	author = {Cherian, Sajeev P. and Olson, Judith S.},
	title = {Extending a theory of remote scientific collaboration to corporate contexts},
	year = {2007},
	isbn = {9781595936424},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1240866.1241001},
	doi = {10.1145/1240866.1241001},
	abstract = {In this paper we present preliminary results of a research project aimed at understanding the theoretical basis for successful remote collaboration in corporations. We evaluate some corporate distributed teams with respect to a theory of remote scientific collaboration to identify similarities and differences in corporate contexts. Preliminary results indicate that distance collaboration in corporations differs from distance collaboration in scientific research in three key ways: (1) the importance of functional (as opposed to geographic) distance, (2) new collaboration paradigms (i.e. offshoring) with varying degrees of.otherness. and (3) different incentives. We additionally discuss future research plans based upon our initial findings.},
	booktitle = {CHI '07 Extended Abstracts on Human Factors in Computing Systems},
	pages = {2321–2326},
	numpages = {6},
	keywords = {distributed teams, distance collaboration, collaboration, CSCW},
	location = {San Jose, CA, USA},
	series = {CHI EA '07}
}

@inproceedings{10.5555/946248.946803,
	author = {MacWilliams, Asa and Sandor, Christian and Wagner, Martin and Bauer, Martin and Klinker, Gudrun and Bruegge, Bernd},
	title = {Herding Sheep: Live System Development for Distributed Augmented Reality},
	year = {2003},
	isbn = {0769520065},
	publisher = {IEEE Computer Society},
	address = {USA},
	abstract = {In the past, architectures of Augmented Reality systemshave been widely different and taylored to specific tasks. Inthis paper, we use the example of the SHEEP game to showhow the structural flexibility of DWARF, our component-basedDistributed Wearable Augmented Reality Framework,facilitates a rapid prototyping and online developmentprocess for building, debugging and altering a complex,distributed, highly interactive AR system.The SHEEP system was designed to test and demonstratethe potential of tangible user interfaces which dynamicallyvisualize, manipulate and control complex operations ofmany inter-dependent processes. SHEEP allows the usersmore freedom of action and forms of interaction and collaboration,following the tool metaphor that bundles softwarewith hardware in units that are easily understandable to theuser. We describe how we developed SHEEP, showing thecombined evolution of framework and application, as wellas the progress from rapid prototype to final demonstrationsystem. The dynamic aspects of DWARF facilitated testingand allowed us to rapidly evaluate new technologies.SHEEP has been shown successfully at various occasions.We describe our experiences with these demos.},
	booktitle = {Proceedings of the 2nd IEEE/ACM International Symposium on Mixed and Augmented Reality},
	pages = {123},
	series = {ISMAR '03}
}

@inproceedings{10.1145/3170427.3174358,
	author = {Barros, Sergio and Jansen, Jack and Cesar, Pablo and R\"{o}ggla, Thomas and Shamma, David A.},
	title = {Designing the Club of the Future with Data: A Case Study on Collaboration of Creative Industries},
	year = {2018},
	isbn = {9781450356213},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3170427.3174358},
	doi = {10.1145/3170427.3174358},
	abstract = {This paper reflects on the development of a multi-sensory clubbing experience which was deployed during a two-day event within the context of the Amsterdam Dance Event in October 2016 in Amsterdam. We present how the entire experience was developed end-to-end and deployed at the event through the collaboration of several project partners from industries such as art and design, music, food, technology and research. Central to the system are smart textiles, namely wristbands equipped with Bluetooth LE sensors which were used to sense people attending the dance event. We describe the components of the system, the development process, the collaboration between the involved entities and the event itself. To conclude the paper, we highlight insights gained from conducting a real world research deployment across many collaborators and stakeholders with different backgrounds.},
	booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
	pages = {1–9},
	numpages = {9},
	keywords = {wear- ables, system design, smart textiles, sensors, datasets, artifacts, activity recognition},
	location = {<conf-loc>, <city>Montreal QC</city>, <country>Canada</country>, </conf-loc>},
	series = {CHI EA '18}
}


@inproceedings{10.1145/3544549.3585630,
	author = {Hayatpur, Devamardeep and Helfenbaum, Tehilla and Xia, Haijun and Stuerzlinger, Wolfgang and Gries, Paul},
	title = {Structuring Collaboration in Programming Through Personal-Spaces},
	year = {2023},
	isbn = {9781450394222},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3544549.3585630},
	doi = {10.1145/3544549.3585630},
	abstract = {The effectiveness of pair programming in pedagogy depends on the frequency and quality of communication of the driver. We explore an alternative collaboration paradigm that tackles this imbalance through Parsons problems: students are given fragments of code out of order and tasked with re-organizing them into the correct order. We then create an interdependence between students by assigning each to a different sub-problem in their own space, termed Personal-spaces – they must engage in dialog to negotiate, exchange, and share fragments. In an exploratory study with nine pairs of undergraduate students, we find evidence pointing to affordances of different coordination conditions: Personal-spaces promoted ownership and engagement, while Turn-taking (akin to pair programming) helped maintain a consistent train of thought. Our results provide considerations for design of appropriate problem sets and interfaces to structure collaborative learning.},
	booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
	articleno = {263},
	numpages = {7},
	keywords = {Parsons problems, collaborative learning, pair programming},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	series = {CHI EA '23}
}


@inproceedings{10.1145/3544549.3585806,
	author = {Le, Khanh-Duy and Ly, Duy-Nam and Nguyen, Hoang-Long and Le, Quang-Tri and Fjeld, Morten and Tran, Minh-Triet},
	title = {HybridMingler: Towards Mixed-Reality Support for Mingling at Hybrid Conferences},
	year = {2023},
	isbn = {9781450394222},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3544549.3585806},
	doi = {10.1145/3544549.3585806},
	abstract = {Mingling, the activity of ad-hoc, private, opportunistic conversations ahead of, during, or after breaks, is an important socializing activity for attendees at scheduled events, such as in-person conferences. The Covid-19 pandemic had a dramatic impact on the way conferences are organized, so that most of them now take place in a hybrid mode where people can either attend on-site or remotely. While on-site attendees can resume in-person mingling, hybrid modes make it challenging for remote attendees to mingle with on-site peers. In addressing this problem, we propose a collaborative mixed-reality (MR) concept, including a prototype, called HybridMingler. This is a distributed MR system supporting ambient awareness and allowing both on-site and remote conference attendees to virtually mingle. HybridMingler aims to provide both on-site and remote attendees with a spatial sense of co-location in the very same venue location, thus ultimately improving perceived presence.},
	booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
	articleno = {163},
	numpages = {7},
	keywords = {AR, MR, VR, distributed systems, hybrid conferences, mingling, mobile devices},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	series = {CHI EA '23}
}


@inproceedings{10.1145/985921.985971,
	author = {Birnholtz, Jeremy},
	title = {Factors affecting the utility of technology-mediated collaboration in science and engineering},
	year = {2004},
	isbn = {1581137036},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/985921.985971},
	doi = {10.1145/985921.985971},
	abstract = {There is significant interest in scientific and industrial collaboration and the CSCW technologies to support it. What is missing, however, is a means for systematic assessment of prospective users, in order to determine what tools are needed and how organizational factors encourage or discourage collaboration. This study proposes development and deployment of a survey instrument measuring social and organizational factors that will likely affect the utility of collaboration for scientists in 3 fields. Results will drive the design of other types of tools for assessing group needs, and specific recommendations about tools likely to be useful under different conditions.},
	booktitle = {CHI '04 Extended Abstracts on Human Factors in Computing Systems},
	pages = {1045–1046},
	numpages = {2},
	keywords = {science, collaboratories, collaboration tools, CSCW},
	location = {Vienna, Austria},
	series = {CHI EA '04}
}
